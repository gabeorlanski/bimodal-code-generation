defaults:
  - base_config
  - task: ???
  - objective: ???
  - _self_
  - objective: seq2seq

# Required Arguments
model: ???
model_path: null
is_checkpoint: false
project: so-code-gen


# General Arguments
data_path: data
device: cpu
seed: 1
numpy_seed: 2
pytorch_seed: 3
save_best_model: False

# Prompt Arguments
prompts: null

# Evaluation Arguments
batch_size: 5
remove_input_ids: False
seq_per_sample: 200
generation:
  do_sample: true
  max_length: 512
  temperature: 0.5
  top_p: 0.95
  top_k: 50

training:
  batch_size: 1
  dataloader_num_workers: 0
  ddp_find_unused_parameters: false
  disable_tqdm: true
  eval_accumulation_steps: 1
  eval_steps: 100
  evaluation_strategy: steps
  fp16: true
  fp16_backend: apex
  fp16_opt_level: O2
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  greater_is_better: false
  group_by_length: false
  half_precision_backend: apex
  learning_rate: 5.0e-05
  load_best_model_at_end: false
  logging_first_step: true
  logging_steps: 25
  lr_scheduler_type: linear
  max_grad_norm: 1.0
  max_steps: 1000
  metric_for_best_model: eval_loss
  num_train_epochs: 1
  output_dir: models
  remove_unused_columns: false
  save_steps: 500
  save_strategy: steps
  save_total_limit: 10
  use_8bit_adam: false
  warmup_ratio: 0.01
  warmup_steps: 100
  weight_decay: 0.1
  xpu_backend: ccl

data_args:
  seq_length: 512

metrics:
  - exact-match
  - bleu
preprocessors: [ ]