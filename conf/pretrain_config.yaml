defaults:
  - train_config@_global_
  - _self_


training:
  batch_size: 1
  learning_rate: 1e-4
  fp16: true
  num_train_epochs: 1
  max_steps: 10000
  save_strategy: steps
  evaluation_strategy: steps
  save_steps: 500
  eval_steps: 500
  logging_steps: 100
  warmup_ratio: 0.01
  load_best_model_at_end: False
  use_8bit_adam: True
generation:
  num_return_sequences: 25
device: 0
preprocessors: [ ]
postprocessors: [ ]
tracking:
  log_model: true
  entity: nyu-code-research

data_args:
  seq_length: 1024
