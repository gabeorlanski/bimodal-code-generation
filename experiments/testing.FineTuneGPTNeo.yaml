debug: false
project: adversarial-code
name: FineTuneGPTNeo
group: testing
tracking: false
task:
  name: mbpp
model: EleutherAI/gpt-neo-125M
model_path: best_models/SO.TestSOData
is_checkpoint: true
data_path: data
device: 0
seed: 1
numpy_seed: 2
pytorch_seed: 3
generation:
  max_length: 1024
  do_sample: true
  temperature: 0.5
  num_return_sequences: 2
  min_length: 75
  top_p: 0.95
  top_k: 50
training:
  batch_size: 1
  output_dir: models
  learning_rate: 5.0e-06
  logging_steps: 10
  warmup_ratio: 0.05
  weight_decay: 0.1
  gradient_accumulation_steps: 2
  lr_scheduler_type: polynomial
  eval_accumulation_steps: 1
  metric_for_best_model: eval_loss
  load_best_model_at_end: true
  save_total_limit: 2
  num_train_epochs: 10
  greater_is_better: false
  evaluation_strategy: epoch
  save_strategy: epoch
  group_by_length: true
  logging_first_step: true
  xpu_backend: ccl
  fp16: true
  disable_tqdm: true
  max_grad_norm: 1.0
  ddp_find_unused_parameters: false
metrics:
- exact-match
- bleu
preprocessors:
- add-prefix:
    prefix: 'You are an expert Python programmer, and here is your task: '
- add-suffix:
    suffix: '#Solution:

      '
    key: input_sequence
objective: lm
data_args:
  seq_length: 512
disable_cache: true
