{"line": 2, "body": "What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?\n", "type": 1, "id": "1", "date": "2016-08-02T15:39:14.947", "score": 10, "comment_count": 0, "tags": ["neural-networks", "backpropagation", "terminology", "definitions"], "title": "What is \"backprop\"?", "answer_count": 5, "views": 625, "accepted_answer": "3", "answers": {"3": {"line": 4, "body": "\"Backprop\" is the same as \"backpropagation\": it's just a shorter way to say it. It is sometimes abbreviated as \"BP\".\n", "type": 2, "id": "3", "date": "2016-08-02T15:40:24.820", "score": 15, "comment_count": 0, "parent_id": "1"}, "222": {"line": 198, "body": "'Backprop' is short for 'backpropagation of error' in order to avoid confusion when using backpropagation term.\nBasically backpropagation refers to the method for computing the gradient of the case-wise error function with respect to the weights for a feedforward networkWerbos. And backprop refers to a training method that uses backpropagation to compute the gradient.\nSo we can say that a backprop network is a feedforward network trained by backpropagation.\nThe 'standard backprop' term is a euphemism for the generalized delta rule which is most widely used supervised training method.\nSource: What is backprop? at FAQ of Usenet newsgroup comp.ai.neural-nets\nReferences:\n\nWerbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University.\nWerbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting,Wiley Interscience.\nBertsekas, D. P. (1995), Nonlinear Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-14-0.\nBertsekas, D. P. and Tsitsiklis, J. N. (1996), Neuro-Dynamic Programming, Belmont, MA: Athena Scientific, ISBN 1-886529-10-8.\nPolyak, B.T. (1964), \"Some methods of speeding up the convergence of iteration methods,\" Z. Vycisl. Mat. i Mat. Fiz., 4, 1-17.\nPolyak, B.T. (1987), Introduction to Optimization, NY: Optimization Software, Inc.\nReed, R.D., and Marks, R.J, II (1999), Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge, MA: The MIT Press, ISBN 0-262-18190-8.\nRumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986), \"Learning internal representations by error propagation\", in Rumelhart, D.E. and McClelland, J. L., eds. (1986), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1, 318-362, Cambridge, MA: The MIT Press.\nWerbos, P.J. (1974/1994), The Roots of Backpropagation, NY: John Wiley & Sons. Includes Werbos's 1974 Harvard Ph.D. thesis, Beyond Regression.\n\n", "type": 2, "id": "222", "date": "2016-08-03T14:39:02.827", "score": 9, "comment_count": 1, "parent_id": "1"}, "83": {"line": 77, "body": "Yes, as Franck has rightly put, \"backprop\" means backpropogation, which is frequently used in the domain of neural networks for error optimization.\nFor a detailed explanation, I would point out this tutorial on the concept of backpropogation by a very good book of Michael Nielsen. \n", "type": 2, "id": "83", "date": "2016-08-02T16:54:40.380", "score": 3, "comment_count": 0, "parent_id": "1"}, "20534": {"line": 13466, "body": "It's a fancy name for the multivariable chain rule.\n", "type": 2, "id": "20534", "date": "2020-04-22T00:49:46.747", "score": 1, "comment_count": 2, "parent_id": "1"}, "28584": {"line": 19775, "body": "We need to compute the gradients in-order to train the deep neural networks. Deep neural network consists of many layers. Weight parameters are present between the layers. Since we need to compute the gradients of loss function for each weight, we use an algorithm called backprop. It is an abbreviation for backpropagation, which is also called as error backpropagation or reverse differentiation.\nIt can be understood well from the following paragraph taken from Neural Networks and Neural Language Models\n\nFor deep networks, computing the gradients for each weight is much\nmore complex,since we are computing the derivative with respect to\nweight parameters that appear all the way back in the very early\nlayers of the network, even though the loss is computed only at the\nvery end of the network.The solution to computing this gradient is an\nalgorithm called error backpropagation or backprop. While backprop was\ninvented for neural networks, it turns out to be the same as a more\ngeneral procedure called backward  differentiation, which depends on\nthe  notion  of computation graphs.\n\n", "type": 2, "id": "28584", "date": "2021-07-08T10:32:25.017", "score": 0, "comment_count": 0, "parent_id": "1"}}}
{"line": 39, "body": "Why would anybody want to use \"hidden layers\"? How do they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?\n", "type": 1, "id": "42", "date": "2016-08-02T16:09:25.427", "score": 6, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "deep-neural-networks", "hidden-layers"], "title": "What is the purpose of the hidden layers?", "answer_count": 4, "views": 3508, "accepted_answer": "51", "answers": {"48": {"line": 45, "body": "Hidden layers by themselves aren't useful. If you had hidden layers that were linear, the end result would still be a linear function of the inputs, and so you could collapse an arbitrary number of linear layers down to a single layer.\nThis is why we use nonlinear activation functions, like RELU. This allows us to add a level of nonlinear complexity with each hidden layer, and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions.\nBecause we can (at least in theory) capture any degree of complexity, we think of neural networks as \"universal learners,\" in that a large enough network could mimic any function.\n", "type": 2, "id": "48", "date": "2016-08-02T16:15:49.970", "score": 3, "comment_count": 0, "parent_id": "42"}, "51": {"line": 48, "body": "\"Hidden\" layers really aren't all that special... a hidden layer is really no more than any layer that isn't input or output. So even a very simple 3 layer NN has 1 hidden layer. So I think the question isn't really \"How do hidden layers help?\" as much as \"Why are deeper networks better?\".  \nAnd the answer to that latter question is an area of active research. Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we don't really understand why deep neural networks work. That is, we don't understand them in complete detail anyway.\nThat said, the theory, as I understand it goes something like this...  successive layers of the network learn successively more sophisticated features, which build on the features from preceding layers. So, for example, an NN used for facial recognition might work like this: the first layer detects edges and nothing else. The next layer up recognizes geometric shapes (boxes, circles, etc.). The next layer up recognizes primitive features of a face, like eyes, noses, jaw, etc. The next layer up then recognizes composites based on combinations of \"eye\" features, \"nose\" features, and so on.  \nSo, in theory, deeper networks (more hidden layers) are better in that they develop a more granular/detailed representation of a \"thing\" being recognized.  \n", "type": 2, "id": "51", "date": "2016-08-02T16:16:59.330", "score": 5, "comment_count": 1, "parent_id": "42"}, "25691": {"line": 17443, "body": "One aspect that I'd like to add to the previous answers is the so-called Curse of dimensionality. This concept refers to the problem that many algorithms have a time complexity that grows exponentially with the dimension of the data.\nAs a simple example, let us consider a set $\\{0,1\\}^{D}$ that has only two values per dimension. For example, $\\{0,1\\}^{2} = \\{(0,0),(0,1),(1,0),(1,1)\\}$ and $(0,1,0) \\in \\{0,1\\}^{3}$. Now imagine that you are given a function $f: \\{0,1\\}^{\\times D} \\rightarrow \\{TRUE, FALSE\\}$ that outputs TRUE exactly for one particular input. The goal is to determine that input.\nIn the example, if nothing else is known about f, the best thing one can do is to try the inputs one after another. However, $\\{0,1\\}^{D}$ has $2^D$ elements. So the number of inputs one has to try out will in general be roughly $2^D$ as well.\nHowever, there exist examples suffering from a curse of dimensionality that can be solved with deep learning, i.e. using neural networks with many hidden layers.\nOne example of great practical importance is given by high-dimensional partial differential equations, see e.g this report:\nhttp://www.sam.math.ethz.ch/sam_reports/reports_final/reports2017/2017-44_fp.pdf\nor this example for heat equations:\nhttps://arxiv.org/abs/1901.10854\nI also found this review on using deep learning to overcome the curse of dimensionality:\nhttps://cbmm.mit.edu/sites/default/files/publications/02_761-774_00966_Bpast.No_.66-6_28.12.18_K1.pdf\n", "type": 2, "id": "25691", "date": "2021-01-10T22:56:08.420", "score": 0, "comment_count": 1, "parent_id": "42"}, "25679": {"line": 17431, "body": "Actually, the hierarchical learning explanation given by mindcrime is not that acceptable anymore (This was also indicated by Ian Goodfellow). Since there are neural networks with 150 layers or more, and this explanation does not make sense for such neural networks.  However, we can think of it as solving the knots of high dimensional manifolds, i.e. we transform the input into high dimensional space, and this helps us to find a better representation of the data.\nA geometric interpretation was explained as such in the book Deep Learning with Python by Francois Chollet:\n\n...you can interpret a neural network as a very complex geometric transformation in a high-dimensional space, implemented via a long series of simple steps...\n\n\nImagine two sheets of colored paper: one red and one blue. Put one on top of the other. Now crumple them together into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time. Uncrumpling paper balls is what machine learning is about: finding neat representations for complex, highly folded data manifolds. At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. Each layer in a deep network applies a transformation that disentangles the data a little--and a deep stack of layers makes tractable an extremely complicated disentanglement process.\n\nI suggest you to read this brilliant blog post to learn about the topological interpretation of deep learning.\nAlso, this toy interactive code may help you.\nIn the context of machine learning, the concept of a manifold can be illustrated as in the following figure.\n\nIn the first part, data are 3-dimensional. However, we can find a transformation to get the second image, which shows that data is actually artificially high dimensional, i.e. it is a 2-dimensional manifold in 3-D space. This example may be thought of as a classification problem, and colors may represent classes, and we can find a trivial representation of the data for classification.\nAnother example could be following figures from the blog I mentioned. In here, this classification problem cannot be solved without having a layer that has 3 or more hidden units, regardless of depth. So the notion of high dimensional transformation is important.\n\nWe can map this data to 3-D, and find a plane to separate them.\n\n", "type": 2, "id": "25679", "date": "2021-01-10T13:29:49.417", "score": 2, "comment_count": 0, "parent_id": "42"}}}
{"line": 49, "body": "I've implemented the reinforcement learning algorithm for an agent to play snappy bird (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.\nIs it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously, storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the NN on an existing q-table would work, but I would like to not use a q-table at all if possible.\n", "type": 1, "id": "52", "date": "2016-08-02T16:19:30.337", "score": 7, "comment_count": 0, "tags": ["neural-networks", "reinforcement-learning", "deep-rl", "function-approximation"], "title": "Is it possible to implement reinforcement learning using a neural network?", "answer_count": 2, "views": 598, "accepted_answer": "1437", "answers": {"1437": {"line": 390, "body": "Andrej Karpathy's blog has a tutorial on getting a neural network to learn pong with reinforcement learning. His commentary on the current state of the field is interesting.\nHe also provides a whole bunch of links (David Silver's course catches my eye). Here is a working link to the lecture videos.\nHere are demos of DeepMinds game playing.\nGet links to the papers at Andrej Karpathy's blog above\n- rat fps\n\nnice demos at 19 minutes into this\n\n", "type": 2, "id": "1437", "date": "2016-08-07T06:30:33.053", "score": 2, "comment_count": 0, "parent_id": "52"}, "11928": {"line": 8075, "body": "Yes, it is possible. The field of deep reinforcement learning is all about using deep neural networks (that is, neural networks with at least one hidden layer) to approximate value functions (such as the $Q$ function) or policies.\nHave a look at the paper A Brief Survey of Deep Reinforcement Learning that gives a brief survey of the field.\n", "type": 2, "id": "11928", "date": "2019-04-21T19:15:49.277", "score": 0, "comment_count": 0, "parent_id": "52"}}}
{"line": 434, "body": "For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.\nGiven I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?\n", "type": 1, "id": "1485", "date": "2016-08-09T09:01:52.373", "score": 3, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "gaming"], "title": "How to separate image recognition from logic?", "answer_count": 2, "views": 111, "accepted_answer": "1523", "answers": {"1523": {"line": 469, "body": "The underlying abstraction (which is essentially what you'd be using the first network for) is that of reducing the state-space of the raw input via feature extraction/synthesis and/or dimensionality reduction.\nAt present, there are few definite rules for doing this: practice is more a question of 'informed trial and error'. \nIf you add some information to your question regarding what has been previously attempted in this area (e.g. on the \nALE platform), this it might be possible to offer some more specific advice.\n", "type": 2, "id": "1523", "date": "2016-08-10T11:24:13.367", "score": 4, "comment_count": 0, "parent_id": "1485"}, "6988": {"line": 4610, "body": "That depends on what type of network you want to use for your second network, instead of feeding the outputs of the first layer, it would be much better if you jointly train both the networks. But that depends on the architecture of the second network ('logic' network).\n", "type": 2, "id": "6988", "date": "2018-07-02T16:08:01.350", "score": 0, "comment_count": 0, "parent_id": "1485"}}}
{"line": 878, "body": "I have been messing around in tensorflow playground. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?\n", "type": 1, "id": "1987", "date": "2016-09-18T11:20:23.830", "score": 12, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "tensorflow", "regression"], "title": "How to classify data which is spiral in shape?", "answer_count": 8, "views": 16513, "accepted_answer": "1990", "answers": {"1990": {"line": 881, "body": "There are many approaches to this kind of problem. The most obvious one is to create new features. The best features I can come up with is to transform the coordinates to spherical coordinates. \nI have not found a way to do it in playground, so I just created a few features that should help with this (sin features). After 500 iterations it will saturate and will fluctuate at 0.1 score. This suggest that no further improvement will be done and most probably I should make the hidden layer wider or add another layer.\nNot a surprise that after adding just one neuron to the hidden layer you easily get 0.013 after 300 iterations. Similar thing happens by adding a new layer (0.017, but after significantly longer 500 iterations. Also no surprise as it is harder to propagate the errors). Most probably you can play with a learning rate or do an adaptive learning to make it faster, but this is not the point here.\n\n", "type": 2, "id": "1990", "date": "2016-09-18T23:27:57.760", "score": 13, "comment_count": 0, "parent_id": "1987"}, "6193": {"line": 4029, "body": "Ideally neural networks should be able to find out the function out on it's own without us providing the spherical features. After some experimentation I was able to reach a configuration where we do not need anything except $X_1$ and $X_2$. This net converged after about 1500 epochs which is quite long. So the best way might still be to add additional features but I am just trying to say that it is still possible to converge without them.\n\n", "type": 2, "id": "6193", "date": "2018-04-27T12:51:09.063", "score": 9, "comment_count": 0, "parent_id": "1987"}, "10000": {"line": 6655, "body": "By cheating... theta is $\\arctan(y,x)$, $r$ is $\\sqrt{(x^2 + y^2)}$.\nIn theory, $x^2$ and $y^2$ should work, but, in practice, they somehow failed, even though, occasionally, it works.\n\n", "type": 2, "id": "10000", "date": "2019-01-15T07:59:04.600", "score": 4, "comment_count": 2, "parent_id": "1987"}, "13115": {"line": 9003, "body": "\nThis is an example of vanilla Tensorflow playground with no added features and no modifications.\nThe run for Spiral was between 187 to ~300 Epoch, depending. \nI used Lasso Regularization L1 so I could eliminate coefficients. \nI decreased the batch size by 1 to keep the output from over fitting. \nIn my second example I added some noise to the data set then upped the L1 to compensate. \n\n", "type": 2, "id": "13115", "date": "2019-06-28T12:21:43.583", "score": 0, "comment_count": 0, "parent_id": "1987"}, "18442": {"line": 12577, "body": "The solution I reached after an hour of trial usually converges in just 100 epochs. \nYeah, I know it does not have the smoothest decision boundary out there, but it converges pretty fast.\n\nI learned a few things from this spiral experiment:-\n\nThe output layer should be greater than or equal to the input layer. At least that's what I noticed in the case of this spiral problem.\nKeep the initial learning rate high, like 0.1 in this case, then as you approach a low test error like 3-5% or less, decrease the learning rate by a notch(0.03) or two. This helps in converging faster and avoids jumping around the global minima.\nYou can see the effects of keeping the learning rate high by checking the error graph at the top right.\nFor smaller batch sizes like 1, 0.1 is too high a learning rate as the model fails to converge as it jumps around the global minima.\nSo, if you would like to keep a high learning rate(0.1), keep the batch size high(10) as well. This usually gives a slow yet smoother convergence.\n\nCoincidentally the solution I came up with is very similar to the one provided by Salvador Dali.\nKindly add a comment, if you find any more intuitions or reasonings.\n", "type": 2, "id": "18442", "date": "2020-03-05T15:56:34.683", "score": 0, "comment_count": 0, "parent_id": "1987"}, "22585": {"line": 15052, "body": "You can increase no of hidden layers. Following is an example (But not very efficient)\n\n", "type": 2, "id": "22585", "date": "2020-07-20T03:31:27.763", "score": 0, "comment_count": 0, "parent_id": "1987"}, "27955": {"line": 19234, "body": "\nThis is the architecture proposed and tested on the playground tensor flow for the Spiral Dataset. Two Hidden Layers with 8 neurons each is proposed with Tanh activation function.\n", "type": 2, "id": "27955", "date": "2021-05-25T12:43:50.003", "score": 0, "comment_count": 0, "parent_id": "1987"}, "28107": {"line": 19361, "body": "May be you need reset all settings, and select x squared and y squared, only 1 hidden layer with 5 neurons.\n\n", "type": 2, "id": "28107", "date": "2021-06-05T07:18:07.313", "score": 0, "comment_count": 0, "parent_id": "1987"}}}
{"line": 886, "body": "The question is about the architecture of Deep Residual Networks (ResNets). The model that won the 1-st places at \"Large Scale Visual Recognition Challenge 2015\" (ILSVRC2015) in all five main tracks:\n\n\nImageNet Classification: \"Ultra-deep\" (quote Yann) 152-layer nets \nImageNet Detection: 16% better than 2nd\nImageNet Localization: 27% better than 2nd\nCOCO Detection: 11% better than 2nd\nCOCO Segmentation: 12% better than 2nd\nSource: MSRA @ ILSVRC & COCO 2015 competitions (presentation, 2-nd slide)\n\n\nThis work is described in the following article:\n\nDeep Residual Learning for Image Recognition (2015, PDF)\n\n\nMicrosoft Research team (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:\n\n\"Identity Mappings in Deep Residual Networks (2016)\"\n\nstate that depth plays a key role:\n\n\"We obtain these results via a simple but essential concept -- going deeper. These results demonstrate the potential of pushing the limits of depth.\"\n\nIt is emphasized in their presentation also (deeper - better): \n\n- \"A deeper model should not have higher training error.\" \n  - \"Deeper ResNets have lower training error, and also lower test error.\" \n  - \"Deeper ResNets have lower error.\"\n  - \"All benefit more from deeper features - cumulative gains!\"\n  - \"Deeper is still better.\"\n\nHere is the sctructure of 34-layer residual (for reference):\n\n\nBut recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:\n\nResidual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)\n\nDeep Resnets are described as many shallow networks whose outputs are pooled at various depths. \nThere is a picture in the article. I attach it with explanation:\n\n Residual Networks are\n  conventionally shown as (a), which is a natural representation of\n  Equation (1). When we expand this formulation to Equation (6), we\n  obtain an unraveled view of a 3-block residual network (b). From this\n  view, it is apparent that residual networks have O(2^n) implicit paths\n  connecting input and output and that adding a block doubles the number\n  of paths.\n\nIn conclusion of the article it is stated:\n\nIt is not depth, but the ensemble that makes residual networks strong.\n  Residual networks push the limits of network multiplicity, not network\n  depth. Our proposed unraveled view and the lesion study show that\n  residual networks are an implicit ensemble of exponentially many\n  networks. If most of the paths that contribute gradient are very short\n  compared to the overall depth of the network, increased depth\n  alone can't be the key characteristic of residual networks. We now\n  believe that multiplicity, the network's expressability in the\n  terms of the number of paths, plays a key role.\n\nBut it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.\n\nShould we think of deep ResNets as an ensemble after all? Ensemble or depth makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represents and what is the key concept in it?\n", "type": 1, "id": "1997", "date": "2016-09-20T10:54:14.493", "score": 14, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "deep-neural-networks", "residual-networks"], "title": "Should deep residual networks be viewed as an ensemble of networks?", "answer_count": 2, "views": 721, "accepted_answer": "1999", "answers": {"1999": {"line": 888, "body": "Imagine a genie grants you three wishes. Because you are an ambitious deep learning researcher your first wish is a perfect solution for a 1000-layer NN for Image Net, which promptly appears on your laptop.\nNow a genie induced solution doesn't give you any intuition how it might be interpreted as an ensemble, but do you really believe that you need 1000 layers of abstraction to distinguish a cat from a dog? As the authors of the \"ensemble paper\" mention themselves, this is definitely not true for biological systems.\nOf course you could waste your second wish on a decomposition of the solution into an ensemble of networks, and I'm pretty sure the genie would be able to oblige. The reason being that part of the power of a deep network will always come from the ensemble effect.\nSo it is not surprising that two very successful tricks to train deep networks, dropout and residual networks, have an immediate interpretation as implicit ensemble. Therefore \"it's not depth, but the ensemble\" strikes me as a false dichotomy. You would really only say that if you honestly believed that you need hundreds or thousands of levels of abstraction to classify images with human accuracy. \nI suggest you use the last wish for something else, maybe a pinacolada. \n", "type": 2, "id": "1999", "date": "2016-09-20T14:40:30.907", "score": 5, "comment_count": 0, "parent_id": "1997"}, "15906": {"line": 10397, "body": "Random residual networks for many non-linearities such as tanh live on the edge of chaos,\nin that the cosine distance of two input vectors will converge to a fixed point at a polynomial rate, rather than an exponential rate, as with vanilla tanh networks. Thus a typical residual network will slowly cross the stable-chaotic boundary with depth, hovering around this boundary for many layers. Basically it does not \"forget\" the geometry of the input space \"very quickly\". So even if we make them considerably deep, they work better the vanilla networks. \nFor more information on the propagation of information in residual networks - Mean Field Residual Networks: On the Edge of Chaos\n", "type": 2, "id": "15906", "date": "2019-10-14T14:04:44.453", "score": 0, "comment_count": 0, "parent_id": "1997"}}}
{"line": 972, "body": "I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.\nI was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.\nThe concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me, but I don't understand where this would be applied with respect to gaming.\nFor example, if I had some simple enemy AI that I want to have adapted to the player's play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?\nWith these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on-off signals?)?\n", "type": 1, "id": "2117", "date": "2016-10-11T01:09:20.780", "score": 1, "comment_count": 0, "tags": ["neural-networks", "game-ai", "genetic-algorithms", "gaming"], "title": "What sort of game problems can neural networks trained/evolved with evolutionary algorithms solve, and how are they typically implemented?", "answer_count": 3, "views": 2186, "accepted_answer": "2181", "answers": {"2182": {"line": 1022, "body": "Without going in too much detail on how exactly Neural Networks and Generic Algorithms work, I can tell you that both the algorithms are not good candidates for computer games.  They work well in scientific environments where the system is \"trained\" on a huge data set to adjust the \"weights\" (variables) for a given problem.  This \"training\" process requires a lot of processing power, time and a large data set.\nComputer games, however either needs to run in real-time (no time for training) or turn-based (not enough data for training).\nAnother problem is that computer games need to free up as much as possible system resources for physics, graphics, sounds and the user interface to improve the player's experience so game developers usually use other lighter techniques (like a rule-based system) to create the illusion of an AI player.\n", "type": 2, "id": "2182", "date": "2016-10-19T07:03:35.853", "score": 1, "comment_count": 0, "parent_id": "2117"}, "2186": {"line": 1025, "body": "For your question there's a brilliant playground emerging!\nGo to https://gym.openai.com/ and explore!\nYou'll get interfaces to games if you want to try applying your machine learning skills and compare the performances of your trained AIs with others. And you can let yourself be inspired by the ideas discussed in the community.\nIf you're especially into Genetic Algorithms you'll find dicussions there too but I'd suggest digging deeper into Reinforcement Learning.\nIf you look at what Google Deep Mind accomplished playing\n\nBreakout\nMontezumas Revenge\nvarious other Atari Games ..\n\nand obviously !\n\nthe sensational victory at Go\n\nyou can say that Reinforcement Learning with (Deep) Neural Networks can be a very promising approach when it comes to training an AI to master games!\n", "type": 2, "id": "2186", "date": "2016-10-19T14:34:23.680", "score": 0, "comment_count": 1, "parent_id": "2117"}, "2181": {"line": 1021, "body": "\n\"if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network\"\n\nSure. Just provide a quality measure for the GA that's related in some manner to the effect of the player's actions on the game state/opponent(s). \nFor example, if defining an opponent's intelligence, one of the conceptually simplest things would be to give a GA population member a fitness that's inversely proportional to the increase in the player's score over some period of time.\n\nare Neural Networks always designed as on off signals?)?\n\nNo. In general, they can be considered to perform nonlinear regression, i.e. a mapping from a vector of real numbers of length n to another of length m. Classification (i.e. 0/1 outputs can be seen as a restricted case of this).\nAs per my answer to this AI SE question, there is a large body of literature (and mature software libraries) for using evolutionary computation to encode neural nets.\nMore generally, some early work in 'online adaptivity using GA-encoded NNs' appeared in the Creatures http://mrl.snu.ac.kr/courses/CourseSyntheticCharacter/grand96creatures.pdf series of games by Steve Grand (details).\n", "type": 2, "id": "2181", "date": "2016-10-19T06:56:10.607", "score": -1, "comment_count": 0, "parent_id": "2117"}}}
{"line": 1322, "body": "I am attempting to create a fully decoupled feed-forward neural network by using decoupled neural interfaces (DNIs) as explained in the paper Decoupled Neural Interfaces using Synthetic Gradients (2017) by Max Jaderberg et al. As in the paper, the DNI is able to produce a synthetic error gradient that reflects the error with respect the output:\n$$\n\\frac{\\partial L}{\\partial h_{i}}\n$$\nI can then use this to update the current layer's parameters by multiplying by the parameters to get the loss with respect to the parameters:\n$$\n\\frac{\\partial L}{\\partial \\theta}=\\frac{\\partial L}{\\partial h_{i}} * \\frac{\\partial h_{i}}{\\partial \\theta}\n$$\nIn the paper, the layer's model is then updated based on the next layer sending the true error backwards.\nMy question is, given that I am able to calculate the error with respect to the current output, how do I use this to calculate the loss with respect to the previous layer's output?\n", "type": 1, "id": "2563", "date": "2016-12-30T01:42:39.703", "score": 3, "comment_count": 0, "tags": ["neural-networks", "backpropagation", "papers", "loss"], "title": "How to perform back-propagation in Decoupled Neural Interfaces?", "answer_count": 1, "views": 129, "accepted_answer": null, "answers": {"8233": {"line": 5530, "body": "I kid you not when I tell you I just started with decoupled networks this morning. But the issue is terribly worded by most computer scientists. The way a decoupled neural interface works is not by sending the error backwards, but by storing it in a form of learned error inside a separate network. So not only do we have our nodes and weights, but also the Synths. The Synths modify the weight tables using past error. So instead of passing error back like the article suggests, it actually is passing it forward in a kind of \"hey don't do this.\" What's really fun is that to you never have to mix up your thinking to reverse a network for backprop if you're using a decoupled network. The amazing part is that it is actually easier to code this decoupled network, than it is to write a \"simple\" ANN, even better still is that it functions a lot like an LSTM network without all the programming mumbo-jumbo. You can see for yourself two different forms of the same network from here. \nThe code from the demo in the repository is from https://towardsdatascience.com/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af it was a fundamental resource for me, and it may be for you. It also helps you with loss, or cost.\n", "type": 2, "id": "8233", "date": "2018-10-03T04:15:39.663", "score": 0, "comment_count": 1, "parent_id": "2563"}}}
{"line": 1060, "body": "There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is \"embodied\". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's \"not even false\". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false).\nThen, since arguably at least chronologically in our evolution, most of our higher-level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question: what is it about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body?\nThis is my first question. And what I'm hoping for are answers that go beyond the a fortiori reply \"Our brain is so powerful and dynamic, it's great for any task, and so also for processing sensorimotor information\".\nMy second question is basically the same, but, instead of the human brain, I want to ask for neural networks. What are the properties of neural networks that make them particularly suitable for processing the kind of information that is produced by a body?\nHere are some of the reasons why people think neural networks are powerful:\n\nThe universal approximation theorem (of FFNNs)\nTheir ability to learn and self-organise\nRobustness to local degrading of information\nTheir ability to abstract/coarse-grain/convolute features, etc.\n\nWhile I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So, they don't provide a satisfactory answer to my question.\nWhat makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation?\nFor instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?\n", "type": 1, "id": "2234", "date": "2016-10-30T21:20:13.010", "score": 5, "comment_count": 0, "tags": ["neural-networks", "human-like", "embodied-cognition"], "title": "Why would neural networks be a particularly good framework for \"embodied AI\"?", "answer_count": 3, "views": 411, "accepted_answer": "2240", "answers": {"2240": {"line": 1066, "body": "To my mind the essential reason why neural networks and the brain are powerful is that they create a hierarchical model of data or of the world. If you ask why that makes them powerful, well, that's just the structure of the world. If you are stalked by a wolf, it's not like its upper jaw will attack you frontally, while his lower jaw will attack you from behind. If you want to respond to the threat with a feasible computational effort, you'll have to treat the wolf as one entity. Providing these kinds of entities or concepts from the raw bits and bytes of input is what a hierarchical representation does. \nNow, this is quite intuitive for sensory information: lashes, iris, eyebrow make up an eye, eyes, nose and mouth make up a face and so on. What is less obvious, is the fact that motor control works exactly the same way! Only in reverse. If you want to lift your arm, you'll just lift it. But for your brain to actually realise this move, the high level command has to be broken down into precise signals for every muscle involved. And this is done by propagating the command down the hierarchy. \nIn the brain these two functions are strongly intertwined. You use constant sensory feedback to adapt your motor control and in many cases you'd be incapable of integrating your stream of sensory data into a coherent representation if you didn't have the additional information of what your body is doing to change that stream of data. Saccades are a good example for that. \nOf course this doesn't mean that our cognitive functions are dependent on the processing of sensorimotor information. I would be surprised if a pure thinking machine wouldn't be possible. There is however a specific version of this \"embodied intelligence hypothesis\" that sounds plausible to me: \nCreating high level cognitive concepts with unsupervised learning is a really difficult problem. Creating high level motor representation might be significantly easier. The reason is that there is more immediate useful feedback. I have been thinking about how to provide a scaffolding for the learning of a hierarchy of cognitive concepts and one thing I could imagine is that high level cognitive concepts basically hitch a ride with the motor concepts. Just think of what a pantomime can express with movement alone. \n", "type": 2, "id": "2240", "date": "2016-10-31T13:47:58.610", "score": 3, "comment_count": 3, "parent_id": "2234"}, "2243": {"line": 1068, "body": "BlindKungFuMaster's answer deals with the hierarchical nature of perception and bodily control, so I'll set that aside and try instead to answer why evolution would use neural networks for animal embodied cognition, and then try to answer if robots of other artificial animals would use the same system.\nIt's important to focus on animals as a whole, not just humans, because that's how evolution works--like the famous John Gall quote:\n\nA complex system that works is invariably found to have evolved from a simple system that worked.\n\nIf you could build a system with five moving parts that does sensorimotor control, but it needs all five parts working in order to function at all, evolution could not build that system except in the rarest of circumstances. \nWhat evolution instead does is slowly extend functional systems. If having one light-sensitive cell connected to one muscle cell makes an organism more likely to survive, then you have the building blocks to add a second layer without inventing any new sorts of cells, because you already have the information-processing connector.\nNeural networks are convenient for evolution because their organization matches the hierarchical nature of the problem and the same kind of cell is used everywhere. All you need is dendrites to receive signals, a way to compute the threshold and trigger if the received signal is higher, axons that can make it to other cells, and then branches at the end of the axon to serve as multipliers. You can arbitrarily extend the depth and breadth of the network just by adding more cells.\nNeural networks are convenient for artificial sensorimotor control because they give you, in memory, access to lots of intermediate values. They're also convenient for the same reasons evolution found them convenient--we can just say what we expect the structure of the robotic control will look like, provide training data, and then eventually have a robot that works.\nBut there's lots of robotics where the control system is designed instead of learned. To take a very simple example, one could use machine learning on the thermostat problem, to learn what temperatures require the heater to be turned on and what temperatures require the air conditioner to be turned on. But this would be extra work and a less robust system than just designing the optimal control system ahead of time.\nIn control theory, there's a concept called adaptive control, where one of the state space parameters for the control system is a property of the system. For example, imagine a satellite; typically we think of the state space of the system as the position and velocity of the satellite in three dimensions, so six total coordinates. There's then a set of differential equations that describe how the satellite will move over time, and what would happen if we used the actuators on the satellite to change its velocity.\nBut part of those differential equations is the inertia of the satellite. That is, how much fuel we need to expend and how it'll affect the rotation and translation of the satellite depends on where the weight of the satellite is located. And this can change over time, as fuel is consumed or if it wasn't correctly measured to begin with. Adaptive control adds new states to the system to track the inertia, and then simultaneously updates its estimate of the inertia and uses that estimate to plan what controls are necessary to move to a desired position.\nYou could imagine solving this problem with neural networks, but we can fairly easily calculate the optimal solution from first principles. In that case, we don't need neural network-based control, but the end result will look something like it from the outside.\n", "type": 2, "id": "2243", "date": "2016-10-31T17:32:53.907", "score": 3, "comment_count": 8, "parent_id": "2234"}, "2244": {"line": 1069, "body": "\nwhat it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information?\n\nThey are an extension of sensory-motor receptors, function could mean any of the hundreds of specific calculations the brain makes, but each one is basically a circuit made out of variations of a basic cell type, with a basic computation, that is a neuron.\n\nWhat makes our brains' architecture particularly suitable for being an information processing unit inside a body?\n\nI don't think it is helpful to think about inside and outside processing, but rather processing along tracts and nodes,( closer to the receptor, available to consciousness,etc)but leaving aside this distinction, the brain architecture is suitable for processing information ( again what facet of information processing you are referring to is unclear), due to the number of specialized computations that derive from it's evolution.\n\nWhat are the properties of neural networks that makes them particularly suitable for processing the kind of information that is produced by a body?\n\nA neural network resembles certain parts/circuits of a brain, mainly how information is integrated based on a set of inputs and their frequency, there is variety and nuance in their types, but they all have inputs which in the case of a body are sensory/interneurons cells and outputs; neuron afferents and motor neurons.\n", "type": 2, "id": "2244", "date": "2016-10-31T20:48:12.107", "score": 0, "comment_count": 0, "parent_id": "2234"}}}
{"line": 588, "body": "We can read on Wikipedia page that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.\nSince ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.\nSo how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?\n", "type": 1, "id": "1655", "date": "2016-08-17T02:02:41.510", "score": 10, "comment_count": 3, "tags": ["neural-networks", "machine-learning", "hardware", "neuromorphic-engineering"], "title": "How does using ASIC for the acceleration of AI work?", "answer_count": 4, "views": 3460, "accepted_answer": "1734", "answers": {"1730": {"line": 656, "body": "I think the algorithm has changed minimally, but the necessary hardware has been trimmed to the bone.\nThe number of gate transitions are reduced (perhaps float ops and precision too), as are the number of data move operations, thus saving both power and runtime.  Google suggests their TPU achieves a 10X cost saving to get the same work done.\nhttps://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html\n", "type": 2, "id": "1730", "date": "2016-08-23T19:35:43.730", "score": 3, "comment_count": 0, "parent_id": "1655"}, "1734": {"line": 659, "body": "Tensor operations\nThe major work in most ML applications is simply a set of (very large) tensor operations e.g. matrix multiplication. You can do that easily in an ASIC, and all the other algorithms can just run on top of that.\n", "type": 2, "id": "1734", "date": "2016-08-24T10:22:19.957", "score": 5, "comment_count": 1, "parent_id": "1655"}, "7731": {"line": 5170, "body": "ASIC - It stands for Application-specific integrated circuit. Basically, you write programs to design a chip in HDL. I'll take cases of how modern computers work to explain my point:\n\nCPU's - CPU's are basically a microprocessor with many helper IC's performing specific tasks. In a microprocessor, there is only a single Arithmetic Processing unit (made up term) called Accumulator in which a value has to be stored, as computations are performed only and only the values stored in the accumulator. Thus every instruction, every operation, every R/W operation has to be done through the accumulator (that is why older computers used to freeze when you wrote from a file to some device, although nowadays the process has been refined and may not require accumulator to come in-between specifically DMA).\nNow in ML algorithms, you need to perform matrix multiplications which can be easily parallelized, but we have in our has a single processing unit only and so came the GPU's.\nGPU's - GPU's have 100's processing units but they lack the multipurpose facilities of a CPU. So they are good for parallelizable calculations. Since there is no memory overlapping (same part of the memory being manipulated by 2 processes)  in matrix multiplication, GPU's will work very well. Though since GPU is not multi-functional it will work only as fast as a CPU feeds data into its memory.\nASIC - ASIC can be anything a GPU, CPU or a processor of your design, with any amount of memory you want to give to it. Let' say you want to design your own specialized ML processor, design a processor on ASIC. Do you want a 256-bit FP number? Create a 256-bit processor. You want your summing to be fast? Implement a parallel adder up to a higher number of bits than conventional processors? You want n number of cores? No problem. you want to define the data-flow from different processing units to different places? You can do it. Also with careful planning, you can get a trade-off between ASIC area vs power vs speed. The only problem is that for all of this you need to create your own standards. Generally, some well-defined standards are followed in designing processors, like a number of pins and their functionality, IEEE 754 standard for floating-point representation, etc which have been come up after lots of trial and errors. So if you can overcome all of these you can easily create your own ASIC.\n\nI do not know what Google is doing with their TPU's but apparently, they designed some sort of Integer and FP standard for their 8-bit cores depending on the requirements at hand. They probably are implementing it on ASIC for power, area and speed considerations.\n", "type": 2, "id": "7731", "date": "2018-08-27T13:24:52.440", "score": 2, "comment_count": 0, "parent_id": "1655"}, "15545": {"line": 10137, "body": "Low precision enables high parallelism computation in Convo and FC layers.\nCPU & GPU fixed architecture, but ASIC/FPGA can be designed based on neural network architecture \n", "type": 2, "id": "15545", "date": "2019-09-19T19:11:19.570", "score": 0, "comment_count": 0, "parent_id": "1655"}}}
{"line": 1037, "body": "If I have a set of sensory nodes taking in information and a set of \"action nodes\" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?\n(This is in the context of evolving neural network)\n", "type": 1, "id": "2201", "date": "2016-10-23T17:09:24.797", "score": 6, "comment_count": 1, "tags": ["neural-networks", "convolutional-neural-networks", "evolutionary-algorithms"], "title": "What is the purpose of hidden nodes in neural network?", "answer_count": 4, "views": 4059, "accepted_answer": null, "answers": {"2202": {"line": 1038, "body": "Normally one node/layer applies linear fitting of the the input to the hypothesis, in other words uses linear function ($y = ax + b$). Adding layers chains liner functions, potentially allowing fitting higher order functions. A great explanation can be found here.\n", "type": 2, "id": "2202", "date": "2016-10-23T18:49:12.407", "score": 1, "comment_count": 0, "parent_id": "2201"}, "2206": {"line": 1042, "body": "A feed forward neural network without hidden nodes can only find linear decision boundaries. However, most of the time you need non-linear decision boundaries. Hence you need hidden nodes with a non-linear activation function. The more hidden nodes you have, the more data you need to find good parameters, but the more complex decision boundaries you can find.\n", "type": 2, "id": "2206", "date": "2016-10-23T22:35:38.523", "score": 5, "comment_count": 6, "parent_id": "2201"}, "5133": {"line": 3221, "body": "Neural Networks are very good approaches for robots. The main function of Neural Net is to model the interdependence between all the features. Now this can be done manually by selecting possible combinations of features between  themselves upto a certain degree. But this approach has drawbacks:\n\nIt is tedious to go about selecting features.\nIt costs time and additional computer resources to calculate the values of the new features you have introduced.\nSince you cannot visualize data more than 3-D you cannot be absolutely sure that your selected features are enough to model your problem.\n\nNow if you use an NN, the NN will automatically select the combination of features (provided it has enough hidden nodes) by adjusting the weights of connections between and the features and nodes. The main advantages of this approach are:\n\nYou don't have to manually select the feature combinations.\nIf data is still not fitting you can easily increase or decrease the number of nodes without needing to modify the whole network.\nAlso it will be computationally efficient since you don't have to calculate values of factors that don't matter to the problem.\n\nHope this is what you were looking for!\n", "type": 2, "id": "5133", "date": "2018-01-26T05:53:43.800", "score": 1, "comment_count": 0, "parent_id": "2201"}, "9732": {"line": 6490, "body": "The term hidden nodes refers to the cells of inner layers of artificial networks are not exposed for connectivity outside of their connectivity within the network. Their values can be read and visualized, but the network function is not dependent upon such tapping of signals internal to the network. Neither are the input of inner layers and their hidden cells connected to network inputs nor are the outputs of inner layers and their hidden cells connected to network outputs.\nThe purpose of inner layers is related to network flexibility and thus accuracy of the functional approximation that may be achieved. An activation function by itself rarely approximates the desired mapping between input values and output values that is optimally achieved by training.\nAttenuated substitution of one parallel activation function into another leads to greater functional flexibility. Attenuation is achieved by multiplying the vector of layer activation function outputs by a matrix of parameters and feeding the resulting product vector into the next layer's activation function units. In combination with a convergence strategy like gradient descent and a corrective distribution strategy like back-propagation to update the parameters, the flexibility in functionality can be directed to achieve higher resulting accuracy by the end of training.\nThere are no action layers or action cells. The cells in an artificial network do not perform actions other than the evaluation of their activation function (and gating in the case of gated units). The output of the network may lead to actions if the network is a component in a controller, in which case the output of the network is connected to other system components so that action is controlled.\n", "type": 2, "id": "9732", "date": "2018-12-29T04:09:50.037", "score": 0, "comment_count": 0, "parent_id": "2201"}}}
{"line": 1886, "body": "In a neural network when inputting nerve input to sense a 2D environment, how do you differentiate two types of objects (with similar shape and size) so the neural network can treat them differently?\nEach neuron in the input layer of a neural network essentially gets 1 dimensional input (range between two values) but 2 dimensional input would be needed to send both collision and category/type information through each input layer neuron. How do you get around that?\nNote: After having confusion regarding the scenario / situation I'm asking about compared to other more complex scenarios, and the long comment series that ensued, I'm realizing one challenge of this site is that it's much more complicated and diverse subject matter than code, or the various other topics of Stack Exchange where the problems can be very clearly and simply expressed. Here it's more challenging to express your question and scenario clearly to avoid confusion. \nAlso there's probably a higher skill gap between an AI learner / enthusiast, and an expert AI specialist, compared to other fields, so that could potentially lead to even more difficulty communicating the answer / question in ways everyone can understand without confusion. Challenging SE site to ask good questions on!\n", "type": 1, "id": "3329", "date": "2017-05-17T09:40:21.940", "score": 0, "comment_count": 14, "tags": ["neural-networks"], "title": "How can object types be differentiated in the input of a neural network?", "answer_count": 2, "views": 207, "accepted_answer": "3381", "answers": {"3334": {"line": 1890, "body": "Neural networks learn. That's what they are for. For your task there are two sensible scenarios: \n\nYou have a fixed reaction for danger and a fixed reaction for food and you only have to learn how to distinguish between them. In that case you basically try to classify the situation to trigger the right fixed response and this classification would be learned by backpropagation.\nYou directly learn to act for a given situation. In that case you can either use a genetic algorithm or you use reinforcement learning with backpropagation.\n\nI would recommend using a genetic algorithm, because it is significantly easier and also makes sense in this situation. You would randomly initialise your network, let it run around in the environment and remember how much food it ate and how often or how quickly it died. Then you would randomly change the weights of your network and do the same thing again. If it did better this time around you would proceed to use the new weights otherwise you go back to the old weights and try a different random change. \nBy selecting successful random changes it would over time learn to avoid danger and seek out food.\nEdit: To my mind you have a fundamental misunderstanding how perception works. If you see a lion and a cake, do those trigger different kinds of cells on your retina? No! All nerve cells are used to detect all kinds of objects! The classification, i.e. whether you are seeing a lion or a cake happens in the neural network i.e. in the higher regions of your visual cortex, far removed from the initial nerve activation. Your lion might be yellow and your cake might be yellow, only if you analyse the high level structure of your nerve inputs can you decide what you are seeing. That is the task of a neural network. And that high level structure analysis is what a neural network learns. \nWhat seems to confuse you is the example you linked. In that example this very sparse distance measuring is enough to differentiate between walls and boosters in your high level structure analysis, because the different points of the walls that you sample have a certain relative position that you can analyse and conclude that they constitute the wall. \nIn your scenario very sparse distance measuring will not help you obviously. The distance of an object doesn't tell you whether it's a lion or a cake. Distance and color would be a solution to that. Or, more realistically, you have different shapes and much tighter distance sampling, and high level analysis can work out the shape from a couple of closely measured distances. \n", "type": 2, "id": "3334", "date": "2017-05-18T07:00:38.150", "score": 2, "comment_count": 10, "parent_id": "3329"}, "3381": {"line": 1918, "body": "@BlindKungFuMaster answered my question in comments, but his answer doesn't reflect that information. If he changes his answer to contain the actual answer, I'll remove mine. \nThe answer to the question \"How can object types be differentiated in the input of a neural network?\" is:\nUse different sensors (nerves) to detect different types of input. \nSo for example:\n\nIf you want the network to be capable of differentiating two different types of objects in a simple 2D environment, use two different sets of nerves, one to detect one object type, one to detect the other. So if you want to sense 10 points around your \"organism\", have 20 nerves, 10 neurons in your input layer dealing with one object type, 10 dealing with the other. \nThat's the most simple example, dealing with binary differences (object type 1 or object type 2), but it could be less binary, like this: Let's say there are three object types and think of each object types as 1/3 of a color value. So you have three sets of 10 nerves in total transmitting to your input layer (30 neurons) and each nerve set senses either R, G, or B. If you set up your system so that there are three \"objects\" to collide with, stacked on top of each other (like a single object), your neural network will be capable of handling objects differently based on their RGB color value, meaning it can now handle nearly infinite \"types\" of objects. \n\n", "type": 2, "id": "3381", "date": "2017-05-25T15:20:31.780", "score": 0, "comment_count": 0, "parent_id": "3329"}}}
{"line": 1608, "body": "I'm trying to create simple keras NN which will learn to make addition on numbers between 0 and 10. But I am getting the error: \nValueError: Error when checking model target: expected activation_4 to have shape (None, 19) but got array with shape (100, 1)\n\nhere is my code:\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nimport numpy as np\n\nkeras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n\nmodel = Sequential()\nmodel.add(Dense(output_dim=50, input_dim=2))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(output_dim=50))\nmodel.add(Activation(\"softmax\"))\nmodel.add(Dense(output_dim=50))\nmodel.add(Activation(\"softmax\"))\nmodel.add(Dense(output_dim=19))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nx = []\ny = []\n\nfor i in range(0, 10):\n    for j in range(0, 10):\n        x.append((i, j))\n        y.append(i + j)\n\nx = np.array(x)\ny = np.array(y)\nprint(x)\nprint(y)\n\nmodel.fit(x, y, nb_epoch=5, batch_size=32)\n\nhow to fix that?\n", "type": 1, "id": "2928", "date": "2017-03-06T11:35:52.083", "score": 3, "comment_count": 1, "tags": ["neural-networks", "keras"], "title": "keras ValueError: Error when checking model target: expected activation_4 to have shape (None, 19) but got array with shape (100, 1)", "answer_count": 2, "views": 9385, "accepted_answer": "2929", "answers": {"2929": {"line": 1609, "body": "Try to use the model like this, for example:\nmodel = Sequential() \nmodel.add(Dense(50, input_shape=(2,))) \nmodel.add(Activation(\"relu\")) \nmodel.add(Dense(50, activation='softmax')) \nmodel.add(Dense(1, activation='linear')) \nmodel.compile(optimizer='sgd', loss='mse', metrics=[\"accuracy\"])\n\n\nThis means that first layer will have 50 neurons and can receive data in form of matrix with 2 columns and an unspecified number of rows.\nSo you can prepare your data in this form - 2 numbers for adding in each row.\nDense(50, input_shape=(2,))\n\n\nAt the end, you need a layer with 1 neuron and the 'linear' activation, because you expect one simple number as a result.\nDense(1, activation='linear')\n\n\nAnd finally, use 'mse' loss function or something similar. 'categorical_crossentropy' is needed for classification tasks, not regression as needed for you.\nSee: https://keras.io/objectives/\n", "type": 2, "id": "2929", "date": "2017-03-06T12:14:06.937", "score": 0, "comment_count": 0, "parent_id": "2928"}, "2930": {"line": 1610, "body": "You shouldn't use Softmax as an activation function in intermediate layers. Softmax is used to represent a categorical distribution, and should be applied at the point where one makes a categorical prediction (usually the final layer of the network).\nConsider replacing you activation function in all layers except the last one with 'relu' or 'sigmoid'.\n", "type": 2, "id": "2930", "date": "2017-03-06T14:26:28.900", "score": 0, "comment_count": 0, "parent_id": "2928"}}}
{"line": 1094, "body": "Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR-10 dataset:\nhttps://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py\n\"\"\" Convolutional network applied to CIFAR-10 dataset classification task.\n\nReferences:\n    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.\n\nLinks:\n    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.data_utils import shuffle, to_categorical\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\n\n# Data loading and preprocessing\nfrom tflearn.datasets import cifar10\n(X, Y), (X_test, Y_test) = cifar10.load_data()\nX, Y = shuffle(X, Y)\nY = to_categorical(Y, 10)\nY_test = to_categorical(Y_test, 10)\n\n# Real-time data preprocessing\nimg_prep = ImagePreprocessing()\nimg_prep.add_featurewise_zero_center()\nimg_prep.add_featurewise_stdnorm()\n\n# Real-time data augmentation\nimg_aug = ImageAugmentation()\nimg_aug.add_random_flip_leftright()\nimg_aug.add_random_rotation(max_angle=25.)\n\n# Convolutional network building\nnetwork = input_data(shape=[None, 32, 32, 3],\n                     data_preprocessing=img_prep,\n                     data_augmentation=img_aug)\nnetwork = conv_2d(network, 32, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = conv_2d(network, 64, 3, activation='relu')\nnetwork = max_pool_2d(network, 2)\nnetwork = fully_connected(network, 512, activation='relu')\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 10, activation='softmax')\nnetwork = regression(network, optimizer='adam',\n                     loss='categorical_crossentropy',\n                     learning_rate=0.001)\n\n# Train using classifier\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),\n          show_metric=True, batch_size=96, run_id='cifar10_cnn')\n\nIt's a CNN with several layers, ending with 10 outputs, one for each type of object recognized.\nBut now think of a slightly different problem: Let's say I only want to recognize one type of object, but also detect its position within the image frame. Let's say I want to distinguish between:\n\nobject is in center\nobject is left of center\nobject is right of center\nno recognizable object\n\nAssume I build a CNN exactly like the one in the CIFAR-10 example, but only with 3 outputs:\n\ncenter\nleft\nright\n\nAnd of course, if none of the outputs fires, then there is no recognizable object.\nAssume I have a large training corpus of images, with the same kind of object in many different positions within the image, the set is grouped and annotated properly, and I train the CNN using the usual methods.\nShould I expect the CNN to just \"magically\" work? Or are there different kinds of architectures required to deal with object position? If so, what are those architectures?\n", "type": 1, "id": "2279", "date": "2016-11-06T10:27:48.913", "score": 2, "comment_count": 0, "tags": ["image-recognition", "convolutional-neural-networks"], "title": "CNN for detecting not just the nature of the object, but position within image as well", "answer_count": 5, "views": 4219, "accepted_answer": null, "answers": {"2280": {"line": 1095, "body": "I guess one of the simplest approach would be train CNN to detect the object in a given image i.e the CNN has single output whole value indicates the probability of the object being in image and then just apply the CNN by segmenting the image into the desired sections and selecting the section which has the highest and good enough probability. For better results I would suggest to train the CNN on the object images with very less other information aka other objects in the images.\n", "type": 2, "id": "2280", "date": "2016-11-06T11:59:22.643", "score": 1, "comment_count": 0, "parent_id": "2279"}, "2944": {"line": 1622, "body": "You could use another type of CNN that instead of classification is performing regression so it will also give you as output the position(it's not really like that but this is the core idea) .\nSome algorithms are SSD or YOLO.\n", "type": 2, "id": "2944", "date": "2017-03-08T08:02:31.230", "score": 3, "comment_count": 0, "parent_id": "2279"}, "6691": {"line": 4394, "body": "A simple trick can be splitting the image in to three frames vertically and feeding them to the image net and you can decide the position by looking for the frame which has higher probability of the desired category(simply max of all the probs).\nOr else you can try YOLO algorithm which further uses non max suppression and IOU on the frames.\n", "type": 2, "id": "6691", "date": "2018-06-08T16:32:33.823", "score": 0, "comment_count": 0, "parent_id": "2279"}, "10459": {"line": 6957, "body": "Object detection models work in a very similar fashion to what you have proposed. They output dense predictions at reduced resolutions. Each prediction fires if an object center is located within the respective region of the image. Of course, there are various further developments, but the main idea is exactly that.\n", "type": 2, "id": "10459", "date": "2019-02-08T16:10:11.447", "score": 0, "comment_count": 0, "parent_id": "2279"}, "24062": {"line": 16154, "body": "One of the suggestions in the accepted answer was SSD.\nOn their website, SSD mentioned a competitor, faster_rcnn.\nfaster_rcnn was deprecated in favor of Detectron.\nDetectron was deprecated in favor of Detectron2.\nLong live detectron2.\nIt looks pretty cool and powerful:\nhttps://github.com/facebookresearch/detectron2\n", "type": 2, "id": "24062", "date": "2020-10-14T00:29:27.243", "score": 0, "comment_count": 1, "parent_id": "2279"}}}
{"line": 1146, "body": "Background\nI've been interested in and reading about neural networks for several years, but I haven't gotten around to testing them out until recently.\nBoth for fun and to increase my understanding, I tried to write a class library from scratch in .Net. For tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).\nEverything seemed fine when I used a sigmoid function as the activation function, but, after reading about the ReLUs, I decided to switch over for speed.\nProblem\nMy current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate.\nI see two possibilities here:\n\nMy implementation is faulty. (This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result).\n\nAside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function. (Fascinating idea, but I've no idea if it's true or not).\n\n\nMy inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.\nMore details\nFor the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per \"bit\" of input). I have also tried using 1 output (with a 1 (really, >0.9) corresponding to true and a 0 (or <0.1) corresponding to false), as well as two outputs (one signaling true and the other false).\nEach training epoch, I run against a set of 4 inputs $\\{ (00, 0), (01, 1), (10, 1), (11, 0) \\}$.\nI find that the first three converge towards the correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.\n", "type": 1, "id": "2349", "date": "2016-11-17T20:46:24.343", "score": 8, "comment_count": 0, "tags": ["neural-networks", "activation-function", "function-approximation", "relu", "sigmoid"], "title": "Are ReLUs incapable of solving certain problems?", "answer_count": 3, "views": 1283, "accepted_answer": "2384", "answers": {"2350": {"line": 1147, "body": "While I have not determined if there are problems that cannot be solved with ReLU, I have found ample documentation in the literature that XOR is solvable with as few as 1 hidden node.\nThe solution is simpler than I thought. The output layer needs connections, not just to the intermediate layer, but directly to the input layer as well. This allows the network to train XOR effectively.\nOne final note, the XOR is extremely sensitive to the learning rate. Essentially, whatever learning rate is appropriate for the AND and OR functions, is approximately 1000x too large to train XOR effectively.\n", "type": 2, "id": "2350", "date": "2016-11-18T18:23:32.520", "score": 1, "comment_count": 0, "parent_id": "2349"}, "2384": {"line": 1176, "body": "There are a variety of possible things that could be wrong, but let me give you some potentially useful information.\nNeural networks with ReLU activation functions are Turing complete for a computation with on order as many steps as the network contains nodes - for a recurrent network (an RNN), that means the same level of turing completeness as any finite computer. In other words, for any function/algorithm that you want to compute, you can devise a neural network, potentially recurrent, that will approximate/compute it.\nAs an example, suppose that we want to compute the NOR function, which can be used to implement a Turing machine.  We can do it with the following neural network with a ReLU activation function.\nLet the input be\n$$ W = \\begin{bmatrix}x_1 \\ \\ x_2 \\end{bmatrix}$$\nthe weight matrix be\n$$ W = \\begin{bmatrix} -20 \\\\ -20 \\end{bmatrix}$$\nand the bias be\n$$ b = \\begin{bmatrix} 1 \\end{bmatrix}$$\nThen the ReLU unit (or neuron) performs the following operation\n$$o = \\max(Wx + b, 0)$$\nSo, $o = 1$ only when both $x_1$ and $x_2$ are $0$, otherwise, it's always $0$.\nHowever, gradient descent is a finicky way to search for RNNs. There are a wide variety of ways that it might have been failing. In general, once you have very thoroughly checked your gradient, I'd make sure to use Adam as the optimizer and then play with the hyperparameters endlessly until I find an incantation that works.\nFor further reading on general understanding of this level of deep learning's capability limitations, I'd recommend this blog post by Ilya Sutskever, now an OpenAI researcher: http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html?m=1\n", "type": 2, "id": "2384", "date": "2016-11-25T23:02:48.910", "score": 2, "comment_count": 0, "parent_id": "2349"}, "21362": {"line": 14120, "body": "I tried to use 2 hidden ReLU-based unit, 1 output unit to solve the XOR problem and found that gradient will always become really small after training 1000 times.\nThe Loss vs training times:\n\nAnd the gradient looks like:\n\nI think that means the units all dead. The robust way to solve this problem is increase the number of units.\nWhen it comes to 4 units, some times I will success, but sometimes not.\nAnd 5 units, I will fail but the rate decrease.\n\nAnd so on. That is all.\nI will try to use sigmoid + cross entropy instead of ReLU, I imagine linear function will work better in this case.\n", "type": 2, "id": "21362", "date": "2020-05-21T02:54:03.893", "score": 0, "comment_count": 0, "parent_id": "2349"}}}
{"line": 1618, "body": " \nFeedforward or multilayered neural networks, like the one in the image above, are usually characterized by the fact that all weighted connections can be represented as a continuous real number. Furthermore, each node in a layer is connected to every other node in the previous and successive layers.\nAre there any other information processing models other than FFNNs or MLPs? For example, is there any system in which the topology of a neural network is variable? Or a system in which the connections between nodes are not real numbers?\n", "type": 1, "id": "2940", "date": "2017-03-07T20:08:04.537", "score": 6, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "topology", "neuroevolution"], "title": "What are some information processing models besides feedforward or multi-layered neural networks?", "answer_count": 3, "views": 445, "accepted_answer": null, "answers": {"5815": {"line": 3741, "body": "To answer the title, there are many other machine learning models, but neural networks work particularly well for some difficult problems (image classification, speech recognition) which is one of the reasons they have gained popularity.\nTwo particularly simple models are the decision tree and the perceptron. These are rather simple models, but they both have redeemable qualities. A decision tree is useful as it provides a model that is easily understood, while a perceptron is fairly quick and works well for linearly separable data. Another, more advanced, model is the Support Vector Machine.\n\nFor example, is there any system in which the topology of a neural network is variable? \n\nYes, there are many such systems where the topology of the neural network is dynamic throughout training. An entire class of methods labeled TWEANNs are designed to evolve the topology of the networks, one such algorithm is NeuroEvolution of Augmenting Topologies, NEAT (and it's descendants rtNEAT, hyperNEAT, ...).\n", "type": 2, "id": "5815", "date": "2018-03-27T22:14:51.143", "score": 0, "comment_count": 0, "parent_id": "2940"}, "5817": {"line": 3742, "body": "Neural Network equivalents that is not (vanilla) feed forward Neural Nets:\nNeural net structures such as Recurrent Neural Nets (RNNs) and Convolutional Neural Nets (CNNs), and different architectures within those are good examples.\nExamples of different architectures within RNNs would would be: Long Short Term Memory (LSTM) or Gated Recurrent Unit (GRU). Both of these are well described in Colah's blog post on Understanding LSTMs\nWhat are some alternative information processing system beside neural network\nThere are sooo many structures. From the top of my head: (Restricted) Boltzmann machine, auto encoders, monte carlo method and radial basis networks to name a few.\nYou can check out Goodfellow's Deep learning-book that is free online and get the gist of all the structures I mentioned here (most parts requires a bit of math knowledge, but he also writes about them quite intuitively).\nFor Recurrent Neural Nets I recommend Colah's blog post on Understanding LSTMs\nIs there any system in which the topology of a neural network is variable?\nDepends on what you mean with the topology of a neural network:\nI think in the common meaning of topology when talking about Neural Networks is the way in which neurons are connected to form a network, varying in structure as it runs and learns. If this is what you men then the answer, in short, is yes. In multiple ways actually. On the other hand, if you mean in the mathematical sense, this answer would become a book that I wouldn't feel confortable writing. So I'll assume you mean the first.\nWe often do \"regularization\", both on vanilla NN and other structures. One of these regularization techniques are called dropout, which would randomly remove connections from the network as it is training (to prevent something called overfitting, which I'm not gonna go into in this post).\nAnother example for another way would be on the Recurrent Neural Network. They deal with time series, and are equipped for dealing with timeseries of different lengths (thus, \"varying structure\").\nDoes it exist neural net systems where complex numbers are used?\nYes, there are many papers on complex number machine learning structures. A quick google should give you loads of results. For example: DeepMind has a paper on \nAssociative Long Short-Term Memory which explores the use of complex values for an \"associative memory\".\nLinks:\nGoodfellow's Deep Learning-book: deeplearningbook.org\nColah's blogpost on RNN's: colah.github.io\nPaper on DeepMinds Associative LSTM: arxiv:1602.03032\n", "type": 2, "id": "5817", "date": "2018-03-28T09:33:45.573", "score": 2, "comment_count": 0, "parent_id": "2940"}, "6533": {"line": 4280, "body": "A very popular choice are Hidden Markov Models.\n", "type": 2, "id": "6533", "date": "2018-05-28T13:41:42.283", "score": -1, "comment_count": 0, "parent_id": "2940"}}}
{"line": 1738, "body": "I am beginning an image analysis project to recognize images with a particular object centered on the image. If the object is at the center, I give the image a positive label, and if it is anywhere else, or simply not in the image, I give the image a negative label. The object, itself, has a complex pattern, such that statistical methods and basic image processing techniques are not able to detect it. The human eye, however, has no trouble detecting this object. Therefore, I am opting to develop a convolutional network that can parse the complexity of this pattern. The only issue, however, is that convolutional networks are inherently designed to be spatially invariant. Therefore, is it even possible to train the network to focus on the importance of the object being at the center simply by feeding the network many negative examples containing the object anywhere else but the center? Furthermore, is there perhaps a better or more direct way to go about incorporating this spatial aspect into the network's functionality?\n", "type": 1, "id": "3109", "date": "2017-04-05T18:33:05.277", "score": 2, "comment_count": 3, "tags": ["image-recognition", "convolutional-neural-networks"], "title": "Training a convolutional network to recognize object location", "answer_count": 1, "views": 166, "accepted_answer": "3118", "answers": {"3118": {"line": 1746, "body": "To use a convolutional net that isn't spatially invariant, you can make the convolution matrix of size equal to your input image size. Afterwards, just use any desired number of fully connected layers and your network should be able to learn your dataset.\n", "type": 2, "id": "3118", "date": "2017-04-07T17:06:03.280", "score": 0, "comment_count": 1, "parent_id": "3109"}}}
{"line": 2344, "body": "I would like to train a neural network (NN) where the output classes are not (all) defined from the start. More and more classes will be introduced later based on incoming data. This means that, every time I introduce a new class, I would need to retrain the NN.\nHow can I train an NN incrementally, that is, without forgetting the previously acquired information during the previous training phases?\n", "type": 1, "id": "3981", "date": "2017-09-06T09:57:10.807", "score": 26, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "transfer-learning", "incremental-learning", "catastrophic-forgetting"], "title": "Is it possible to train a neural network as new classes are given?", "answer_count": 4, "views": 12175, "accepted_answer": "4053", "answers": {"4053": {"line": 2401, "body": "I'd like to add to what's been said already that your question touches upon an important notion in machine learning called transfer learning. In practice, very few people train an entire convolutional network from scratch (with random initialization), because it is time consuming and relatively rare to have a dataset of sufficient size. \nModern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet. So it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights.\nWhen you need a ConvNet for image recognition, no matter what your application domain is, you should consider taking an existing network, for example VGGNet is a common choice.\nThere are a few things to keep in mind when performing transfer learning:\n\nConstraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can't arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides \"fit\").\nLearning rates. It's common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don't wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization).\n\nAdditional reference if you are interested in this topic: How transferable are features in deep neural networks?\n", "type": 2, "id": "4053", "date": "2017-09-18T16:37:27.630", "score": 11, "comment_count": 1, "parent_id": "3981"}, "3984": {"line": 2346, "body": "Here is one way you could do that.\nAfter training your network, you can save its weights to disk. This allows you to load this weights when new data becomes available and continue training pretty much from where your last training left off. However, since this new data might come with additional classes, you now do pre-training or fine-tuning on the network with weights previously saved. The only thing you have to do, at this point, is make the last layer(s) accommodate the new classes that have now been introduced with the arrival of your new dataset, most importantly include the extra classes (e.g., if your last layer initially had 10 classes, and now you have found 2 more classes, as part of your pre-training/fine-tuning, you replace it with 12 classes). In short, repeat this circle : \n\n", "type": 2, "id": "3984", "date": "2017-09-06T13:59:03.557", "score": 6, "comment_count": 1, "parent_id": "3981"}, "24139": {"line": 16208, "body": "There are several ways to add new classes to the trained model, which require just training for the new classes.\n\nIncremental training (GitHub)\ncontinuously learn a stream of data (GitHub)\nonline machine learning (GitHub)\nTransfer Learning Twice\nContinual learning approaches (Regularization, Expansion, Rehearsal) (GitHub)\n\n", "type": 2, "id": "24139", "date": "2020-10-19T09:19:26.043", "score": 1, "comment_count": 3, "parent_id": "3981"}, "24527": {"line": 16473, "body": "You could use transfer learning (i.e. use a pre-trained model, then change its last layer to accommodate the new classes, and re-train this slightly modified model, maybe with a lower learning rate) to achieve that, but transfer learning does not necessarily attempt to retain any of the previously acquired information (especially if you don't use very small learning rates, you keep on training and you do not freeze the weights of the convolutional layers), but only to speed up training or when your new dataset is not big enough, by starting from a model that has already learned general features that are supposedly similar to the features needed for your specific task. There is also the related domain adaptation problem.\nThere are more suitable approaches to perform incremental class learning (which is what you are asking for!), which directly address the catastrophic forgetting problem. For instance, you can take a look at this paper Class-incremental Learning via Deep Model Consolidation, which proposes the Deep Model Consolidation (DMC) approach. There are other continual/incremental learning approaches, many of them are described here or in more detail here.\n", "type": 2, "id": "24527", "date": "2020-11-10T10:16:01.320", "score": 0, "comment_count": 0, "parent_id": "3981"}}}
{"line": 1856, "body": "I am looking at a diagram of ZFNet in an attempt to understand how CNNs are designed effectively. I'm working with the CIFAR10 set in pytorch.\nIn the first layer, I understand the depth of 3 (224x224x3) is the number of color channels in the image.\nIn the second layer I understand the 110x110 is is (224 - ( 7 * 2)) / 2\nI also understand how pooling works to create a size reduction.\nBut where does the depth of 96 come from in the second layer? Is this the new \"batch size\"? Is it totally arbitrary? Bonus points if someone can direct me to a reference that can help me understand how all these dimensions relate to each other.\n", "type": 1, "id": "3287", "date": "2017-05-08T16:08:26.650", "score": 3, "comment_count": 0, "tags": ["convolutional-neural-networks"], "title": "How is the depth of a CNN layer determined?", "answer_count": 2, "views": 5267, "accepted_answer": "3289", "answers": {"3289": {"line": 1858, "body": "The 96 is the amount of filter maps (also: filter kernels). It is a fundamental of convolutional neural networks. The exact number is not arbritary, although there is no equation or exact rule of restricting the number.\nIf you have a CNN one single convolution operation would be pointless: since it used for the whole image information it can generalize, but only to specific (meaning: finite amount of) features. Easy example: if a 7x7 filter in the first layer concentrates on round shapes, it can not generalize on let's say red cubes at the same time.\nTherefore you convolutional layers have several filter kernels, i.e. several weights where each is used for a convolution. The result of each of these convolutions is one filter map, i.e. the image information convolved by a kernel.\nTypically you look at your filter kernels and your problem's domain to figure out what an appropriate number of filter kernels could be. You also have to keep in mind that too few kernels could possibly lose information and overfit to specific patterns, while too many kernels could possibly underfit. Especially when you have far more parameters (primarily the weights of your network) than training data your network will normally perform bad and you have to reduce its size.\nThe filter kernels should not be confused with batch size. The batch size is the amount of samples (here: images) you train in parallel. Each training step of your network does not only consist of feeding a single image, but feeding batch size number of images, usually combined with batch normalization steps between the layers. Hence, this has nothing to do the amount of filter kernels / convolutions of your network.\n", "type": 2, "id": "3289", "date": "2017-05-08T23:56:28.040", "score": 2, "comment_count": 1, "parent_id": "3287"}, "6138": {"line": 3988, "body": "In simple terms, it can explain as below, \nLet's say you have 10 filters where each filter is the size of 5x5x3. What does this mean? the depth of this layer is 10 which is equal to the number of filters. Size of each filter can be defined as we want e.g., 5x5x3 in this case where 3 is the depth of the previous layer. To be precise, depth of each filer in the next layer should be 10 ( nxnx10) where n can be defined as you want like 5 or something else. Hope will make everything clear. \n", "type": 2, "id": "6138", "date": "2018-04-22T19:59:32.070", "score": 0, "comment_count": 0, "parent_id": "3287"}}}
{"line": 1857, "body": "For a classification task (I'm showing a pair of exactly two images to a CNN that should answer with 0 -> fake pair or 1 -> real pair) I am struggling to figure out how to design the input.\nAt the moment the network's architecture looks like this:\nimage-1                       image-2\n   |                             |\nconv layer                    conv layer\n   |                             |\n   _______________ _______________\n                  |\n            flattened vector\n                  |\n          fully-connected layer\n                  |\n           reshape to 2D image\n                  |\n              conv layer\n                  |\n              conv layer\n                  |\n              conv layer\n                  |\n            flattened vector\n                  |\n                output\n\nThe conv layers have a 2x2 stride, thus halfing the images' dimensions. I would have used the first fully-connected layer as the first layer, but then the size of it doesn't fit in my GPU's VRAM. Thus, I have the first conv layers halfing the size of the images first, then combining the information with a fully-connected layer and then doing the actual classification with conv layers for the combined image information.\nMy very first idea was to simply add the information up, like (image-1 + image-2) / 2...but this is not a good idea, since it heavily mixes up image information.\nThe next try was to concatenate the images to have one single image of size 400x100 instead of two 200x100 images. However, the results of this approach were quite unstable. I think because in the center of the big, concatenated image convolutions would convolve information of both images (right border of image-1 / left border of image-2), which again mixes up image information in not really senseful way.\nMy last approach was the current architecture, simply leaving the combination of image-1 and image-2 up to one fully-connected layer. This works - kind of (the results show a nice convergence, but could be better).\nWhat is a reasonable, \"state-of-the-art\" way to combine two images for a CNN's input?\nI clearly can not simply increase the batch size and fit the images there, since the pairs are related to each other and this relationship would get lost if I simply feed just one image at a time and increase the batch size.\n", "type": 1, "id": "3288", "date": "2017-05-08T23:34:09.467", "score": 3, "comment_count": 6, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks", "image-recognition", "classification"], "title": "How to \"combine\" two images for CNN input (classification task)?", "answer_count": 3, "views": 14294, "accepted_answer": "3555", "answers": {"3555": {"line": 2043, "body": "You can combine the image output using concatenation. Please refer to this paper:\nhttp://ivpl.eecs.northwestern.edu/sites/default/files/07444187.pdf\nYou can have a look at the Figure 2. And if you are using caffe, there is a layer called Concat layer. You can use it for your purpose.\nI am not fully clear about what you want to do. But like you said, if you want to pass the image values from the first layer to some layers. Try reading about skip architectures.\nIf you want to use this network as real/fake finder, you can take the difference between two images and convert it to classification problem.\nHope it helps.\n", "type": 2, "id": "3555", "date": "2017-06-28T02:05:05.570", "score": 1, "comment_count": 1, "parent_id": "3288"}, "5187": {"line": 3269, "body": "I'm not sure what you mean by pairs. But a common pattern for dealing w/ pair-wise ranking is a siamese network:\n\nWhere A and B are a a pos, negative pair and then the Feature Generation Block is a CNN architecture which outputs a feature vector for each image (cut off the softmax) and then the network tried to maximise the regression loss between the two images. The two networks share the same parameters and thus in the end you have one model which can accurately disambiguate between a positive or negative pair.\n", "type": 2, "id": "5187", "date": "2018-01-29T09:48:47.500", "score": 1, "comment_count": 0, "parent_id": "3288"}, "11869": {"line": 8029, "body": "eggie5 actually has a good solution for you. This approach is a tried and tested way to solve the same problem you are trying to solve.  \nHowever, if you still want to concatenate the images and do this your way, you should concatenate the images along the channel dimension.\nFor example, by combining two $200\\times 100 \\times c$ feature vectors (where c is the number of channels) you should get a single $200\\times 100 \\times 2c$ feature vector. \nThe kernels of the next convolution look through all the channels of the feature vector $x \\times x$ pixels at a time.\nIf we combine along the channel dimension, it becomes easier for the network to compare pixel values at corresponding positions in both images. Since the objective is to predict similarity or dissimilarity, this is ideal for us.\n", "type": 2, "id": "11869", "date": "2019-04-17T15:43:35.970", "score": 0, "comment_count": 1, "parent_id": "3288"}}}
{"line": 1288, "body": "I am new to neural-network and I am trying to understand mathematically what makes neural networks so good at classification problems. \nBy taking the example of a small neural network (for example, one with 2 inputs, 2 nodes in a hidden layer and 2 nodes for the output), all you have is a complex function at the output which is mostly sigmoid over a linear combination of the sigmoid.\nSo, how does that make them good at prediction? Does the final function lead to some sort of curve fitting?\n", "type": 1, "id": "2524", "date": "2016-12-21T20:47:37.700", "score": 15, "comment_count": 0, "tags": ["neural-networks", "classification", "prediction"], "title": "What makes neural networks so good at predictions?", "answer_count": 4, "views": 2068, "accepted_answer": null, "answers": {"2591": {"line": 1346, "body": "In Neural Networks we consider everything in high dimension and try to find a hyperplane that classify them by small changes...\nProbably it is hard to prove that it works but intuition says if it can be classified you can do it by add a relaxed plane and let it move amongst data to find a local optimum... \n", "type": 2, "id": "2591", "date": "2017-01-01T17:02:33.327", "score": 0, "comment_count": 0, "parent_id": "2524"}, "2595": {"line": 1348, "body": "With Neural Networks you simply classify datas. If you classify correctly, so you can do future classifications.\nHow It Works?\nSimple neural networks like Perceptron can draw one decision boundary in order to classify datas.\nFor example suppose you want to solve simple AND problem with simple Neural Network. You have 4 sample data containing x1 and x2 and weight vector containing w1 and w2. Suppose initial weight vector is [0 0]. If you made calculation which depend on NN algoritm. At the end, you should have a weight vector [1 1] or something like this.\n\nPlease focus on the graphic. \nIt says: I can classify input values into two classes (0 and 1). Ok. Then how can I do this? It is too simple. First sum input values (x1 and x2). \n\n0+0=0\n0+1=1\n1+0=1\n1+1=2\n\nIt says:\n\nif sum<1.5 then its class is 0\nif sum>1.5 then its class is 1\n\n", "type": 2, "id": "2595", "date": "2017-01-02T12:09:49.667", "score": 1, "comment_count": 0, "parent_id": "2524"}, "2572": {"line": 1329, "body": "Neural networks are good at classifying. In some situations that comes down to prediction, but not necessarily. \nThe mathematical reason for the neural networks prowess at classifying is the universal approximation theorem. Which states that a neural network can approximate any continuous real-valued function on a compact subset. The quality of the approximation depends on the number of neurons. It has also been shown that adding the neurons in additional layers instead of adding them to existing layers improves the quality of the approximation faster. \nAdd to that the not well-understood effectiveness of the backpropagation algorithm and you have a setup then can actually learn the function that the UAT promises or something close. \n", "type": 2, "id": "2572", "date": "2016-12-30T06:18:56.870", "score": 7, "comment_count": 0, "parent_id": "2524"}, "7174": {"line": 4748, "body": "Neural networks excel at a variety of tasks, but to get an understanding of exactly why, it may be easier to take a particular task like classification and dive deeper.\nIn simple terms, machine learning techniques learn a function to predict which class a particular input belongs to, depending on past examples. What sets neural nets apart is their ability to construct these functions that can explain even complex patterns in the data. The heart of a neural network is an activation function like Relu, which allows it to draw some basic classification boundaries like: \nBy composing hundreds of such Relus together, neural networks can create arbitrarily complex classification boundaries, for example:\nIn this article, I try to explain the intuition behind what makes neural networks work: https://medium.com/machine-intelligence-report/how-do-neural-networks-work-57d1ab5337ce\n", "type": 2, "id": "7174", "date": "2018-07-17T05:17:31.387", "score": 1, "comment_count": 0, "parent_id": "2524"}}}
{"line": 2947, "body": "I am trying to find literature on a network architecture that takes the following as in input:\n\nAction (like 'Up', 'Down', etc)\nImage of the current state\n\nand outputs:\n\nImage of next state\n\nI already have a lot of training data for the inputs. However, I am trying to find relevant literature/architecture for this problem.\n", "type": 1, "id": "4774", "date": "2017-12-17T23:00:00.030", "score": 2, "comment_count": 1, "tags": ["neural-networks", "reinforcement-learning", "reference-request", "game-ai", "model-request"], "title": "Is there a neural network in the literature that predicts the next game state based on the current state and the action?", "answer_count": 3, "views": 186, "accepted_answer": null, "answers": {"4775": {"line": 2948, "body": "You'll probably want to start out with \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" (arXiv link: https://arxiv.org/abs/1507.08750). That's from 2015 though, I'm sure there have been lots of other interesting developments since then. This paper may still be a good starting point though, and provide you with the correct terminology to plug into google / google scholar to find more recent papers that build on top of this. Google scholar also provides functionality to automatically find papers that cite this one (interesting recent papers will probably cite this one).\nAs an additional point, you may want to reconsider your desired output. Given a current state-image and action, it may be easier to train a network to predict only the change in image (i.e., predicting NEW_IMAGE - OLD_IMAGE), rather than predicting the full image. You can then always still manually reconstruct the predicted new image simply by adding that output to the old image again. I'm quite sure I've seen this being done in a more recent paper too, but don't remember exactly the title / authors.\n", "type": 2, "id": "4775", "date": "2017-12-18T13:12:00.410", "score": 0, "comment_count": 0, "parent_id": "4774"}, "9413": {"line": 6294, "body": "I tried something similar before for 2048 game. I used the state of the board as x, and the move as y. I just trained the neural network with this dataset. The architecture is like a couple of layers with relu and the final layer as softmax. The major thing is that we should not feed the wrong moves in the dataset to the NN, or else the NN also tend to learn the bad moves, which in turn makes it less smarter.\nI gathered my dataset by running the minimax on 2048 and assigning a reward for each move, and then eliminating the bad ones on it.\nthe above process also depends on the way you are taking the feature vector, if your feature vector is an image, then it makes sense to use CNN.\nhttps://github.com/navjindervirdee/2048-deep-reinforcement-learning\nDQN is also a good option. but do checkout the above link, it helped me too.\nMy repo: https://github.com/williamscott701/AI-vs-2048\nmy results are not actually that great.\n", "type": 2, "id": "9413", "date": "2018-12-08T18:44:06.283", "score": 0, "comment_count": 0, "parent_id": "4774"}, "27528": {"line": 18892, "body": "This is a whole sub-field of reinforcement learning known as model-based reinforcement learning. The idea in model based RL is to learn the mapping from current state/action to next state in order to facilitate learning good policies.\nIf you are dealing with images as inputs I would recommend checking out the Dreamer papers. The most recent being this one.\n", "type": 2, "id": "27528", "date": "2021-04-27T00:17:54.453", "score": 0, "comment_count": 1, "parent_id": "4774"}}}
{"line": 2282, "body": "I was trying to code a single layer perceptron to understand binary AND:\n1 1 1\n0 1 0\n1 0 0\n0 0 0  \nI made up this code \n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\nint main()\n{\nint input1, input2;\nfloat weight1 = 0.3, weight2 = 0.4;\nint output;\nint training1, training2, expectedoutput;\nint i;\nint j=1;\n\n//TRAINING\nfor(i=0; i<10000;i++)\n{   \n\n\n    if(j=1)\n    {\n        training1 = 0;\n        training2 = 1;\n        expectedoutput = 0;\n    }\n    if(j=2)\n    {\n        training1 = 1;\n        training2 = 0;\n        expectedoutput = 0;\n    }\n    if(j=3)\n    {\n        training1 = 0;\n        training2 = 0;\n        expectedoutput = 0;\n    }\n    if(j=4)\n    {\n        training1 = 1;\n        training2 = 1;\n        expectedoutput = 1;\n        j=1;\n    }\n    output = weight1*training1 + weight2*training2 + 2;\n\n    if(output != expectedoutput )\n    {\n        weight1 = weight1 + 0.156 * training1 * (expectedoutput - output);\n        weight2 = weight2 + 0.156 * training2 * (expectedoutput - output);\n    }\n    j++;\n}\n\nprintf(\"training done\\n\");\nprintf(\"weight1 = %f\" \"weight2 = %f\\n\",weight1,weight2);\n\n//TESTING THE PERCEPTRON\nfor(i=0; i<5 ; i++)\n{\nscanf (\"%d%d\", &input1, &input2 );\noutput = weight1*input1 + weight2*input2;\nprintf(\"\\n%d\\n\", output);\n}\nreturn 1;    \n}\n\nits supposed to input the 4 cases repeatedly and with a learning rate of 0.156 (which i set randomly) and i used the threshold as a weight of 2. \nHowever after the training the perceptron still doesnt give the expected output. Is my understanding of perceptron rule wrong? Please help thank you!\n", "type": 1, "id": "3898", "date": "2017-08-26T15:32:01.580", "score": 1, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "artificial-neuron"], "title": "debugging perceptron for digital AND circuit", "answer_count": 1, "views": 86, "accepted_answer": "3901", "answers": {"3901": {"line": 2285, "body": "You have several flaws in your code that will lead to unexpected behavior. I identified the following flaws you need to address. Once you have fixed them, you should output your updated weights during each step of training to see what the learning algorithm actually does and if it is going in the right direction. I will not address pure style aspects (like using a simpler case-statement instead of a series of ifs and stuff like that) and focus on the real errors.\nSkipping training case 1\nOnce you are in training case 4, you set j to 1, expecting to use training case 1 next. But later in your code you increase j by one (j++) and go directly to training case 2, skipping the first one. This means you only run through training case 1 during your first pass of the loop.\nAssigning j a value instead of comparing it\n\nif(j=1)\n\nThe if statement will always be true, because you do not compare j to 1 but you set j to 1. You basically assign a value and test if the value is true. Correct would be:\n\nif(j==1)\n\nIgnoring bias during test\nDuring the test steps after training you forget to add the bias that you used during training:\n\noutput = weight1*input1 + weight2*input2\n\nshould actually be:\n\noutput = weight1*input1 + weight2*input2 + 2\n\nOtherwise your perceptron behaves differently during training and testing.\nPerceptron output must be 1 or 0\nThose were all implementation issues. This point actually seems to come from a misunderstanding of perceptrons. A real perceptron can either fire or not, meaning it outputs either 1 or 0, nothing in between. You calculate your output the following way and use the output to compare to the expected output:\n\noutput = weight1*training1 + weight2*training2 + 2;\nif(output != expectedoutput )\n\nYour output here is a float value. Only in edge cases it will result in 1 or 0. What you actually want to do for a real perceptron is:\n\nif (weight1*training1 + weight2*training2 + 2 >= 0) {\n   output = 1\n  } else {\n   output = 0\n  }\nif(output != expectedoutput)\n\nAfter you fixed those errors and studied the output of your learning algorithms, you should be able to get the perceptron to work. If you have any questions, please leave a comment and I will try to help.\n\nEdit:\nI don't have a C compiler here, so I quickly implemented your solution with my suggested fixes in Python 3.6 and the results are very close to the correct solution. The final weights obviously have to be -1 and -1 for an AND Gate with bias 2. The learning process gets very close but never reaches -1 and -1 exactly, which leads to the wrong output in the end. This is a very good showcase to illustrate why you prefer a sigmoid neuron instead of a perceptron in modern neural networks. Here is a relevant question from cross-validated concerning the difference of those two types of neurons.\n", "type": 2, "id": "3901", "date": "2017-08-27T06:41:06.693", "score": 0, "comment_count": 2, "parent_id": "3898"}}}
{"line": 2293, "body": "In this note Justin Domke says that\n\nIn practice, neural networks seem to usually find a reasonable solution when the number of layers is not too large, but find poor solutions when using more than, say, 2 hidden layers.\n\nBut in Bengio's remark, he says \n\nVery simple. Just keep adding layers until the test error does not improve anymore.\n\nThere seems to be a conflict. Can anyone explain why they suggest differently? Or am I missing something?\n", "type": 1, "id": "3910", "date": "2017-08-28T02:25:21.380", "score": 3, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "hidden-layers"], "title": "Are these statements about the performance of neural networks as a function of the number of hidden layers contradictory?", "answer_count": 3, "views": 107, "accepted_answer": null, "answers": {"3911": {"line": 2294, "body": "There are many problems requiring more than two hidden layers. Randomly select a recent Google journal paper on deep learning, you'll see their network could have something like 5 (or more) hidden layers.\nJustin Domke wrote his notes for students, so he probably tried to make his points as simple as possible. For a \"typical\" machine learning problem that students would most likely work on, two hidden layers should be sufficient. But that doesn't add up for a real practical problem. \"Deep\" learning usually mean more than two hidden layers.\nNumber of hidden layers is network design that nobody knows for sure. Yoshua Bengio's suggestion is common and simple. It's not a mathmatic proof, but simply a guideline if you don't know what to  do. You just repeat and repeat, until you see the test error no longer improve.\n", "type": 2, "id": "3911", "date": "2017-08-28T02:37:00.607", "score": 2, "comment_count": 0, "parent_id": "3910"}, "3912": {"line": 2295, "body": "In fact they are telling the same thing: Plot the x-axis as the number of hidden layers, and the y-axis as the performance (e.g. classification accuracy), then this curve will have an upside-down U shape.\nJustin's note is clearly saying the same thing as what I wrote above, with added note that the maximum of the curve will happen when x = 2, and Bengio's note is saying the same thing without telling you where the peak could be.\n", "type": 2, "id": "3912", "date": "2017-08-28T03:42:53.030", "score": -1, "comment_count": 0, "parent_id": "3910"}, "20204": {"line": 13208, "body": "Your first link is from 2011, which essentially predates the current deep learning explosion. In the many years that have since passed (AlexNet 2012, ResNet 2015) we have since found that if you keep adding layers, we generally do see improved performance.\n\nThis is due to improved training techniques and optimization breakthroughs (residual connections, ReLU, dropout etc.). But do note that the result can be diminishing. In particular, take a look at Deep Equillibrium Models, which essentially allow us to train (in the limit equivalence) infinite depth neural networks. \n", "type": 2, "id": "20204", "date": "2020-04-12T19:48:58.233", "score": 0, "comment_count": 0, "parent_id": "3910"}}}
{"line": 2397, "body": "Introduction\nI am currently writing an engine to play a card game, as there is no engine yet for this particular game.\nAbout the game\nThe game is similar to Magic: The Gathering. There is a commander, which has health and abilities. Players have an energy pool, which they use to put minions and spells on the board. Minions have health, attack values, costs, etc. Cards also have abilities, these are not easily enumerated. Cards are played from the hand, new cards are drawn from a deck. These are all aspects it would be helpful for the neural network to consider.\nIdea\nI am hoping to be able to introduce a neural network to the game afterwards, and have it learn to play the game. So, I'm writing the engine in such a way that is helpful for an AI player. There are choice points, and at those points, a list of valid options is presented. Random selection would be able to play the game (albeit not well).\nI have learned a lot about neural networks (mostly NEAT and HyperNEAT) and even built my own implementation. Neural networks are usually applied to image recognition tasks or to control a simple agent.\nProblem/question\nI'm not sure if or how I would apply neural networks to make selections with cards, which have a complex synergy. How could I design and train a neural network for this game, such that it can take into account all the variables? Is there a common approach?\nI know that Keldon wrote a good AI for RftG, which has a decent amount of complexity, but I am not sure how he managed to build such an AI.\nAny advice? Is it feasible? Are there any good examples of this? How were the inputs mapped?\n", "type": 1, "id": "4048", "date": "2017-09-17T20:12:20.860", "score": 15, "comment_count": 2, "tags": ["neural-networks", "game-ai", "neat", "card-games"], "title": "How can I design and train a neural network to play a card game (similar to Magic: The Gathering)?", "answer_count": 5, "views": 14581, "accepted_answer": null, "answers": {"4071": {"line": 2411, "body": "This is completely feasible, but the way the inputs are mapped would greatly depend on the type of card game, and how it's played.\nI'll take into account a few possibilities:\n\nDoes time matter in this game? Would a past move influence a future one? In this case, you'd be better off using Recurrent Neural Networks (LSTMs, GRUs, etc.).\nWould you like the Neural Network to learn off of data you collect, or learn on its own? If on its own, how? If you collect data of yourself playing the game tens or hundreds of times, feed it into the Neural Net, and make it learn from you, then you're doing something called \"Behavioural Cloning\". However, if you'd like the NN to learn on its own, you can do this 2 ways:\na) Reinforcement Learning - RL allows the Neural Net to learn by playing against itself lots of times.\nb) NEAT/Genetic Algorithm - NEAT allows the Neural Net to learn by using a genetic algorithm.\n\nHowever, again, in order to get more specific as to how the Neural Net's inputs and outputs should be encoded, I'd have to know more about the card game itself.\n", "type": 2, "id": "4071", "date": "2017-09-20T07:13:26.623", "score": 2, "comment_count": 2, "parent_id": "4048"}, "5316": {"line": 3358, "body": "I think you raise a good question, especially WRT to how the NNs inputs & outputs are mapped onto the mechanics of a card game like MtG where the available actions vary greatly with context. \nI don't have a really satisfying answer to offer, but I have played Keldon's Race for the Galaxy NN-based AI - agree that it's excellent- and have looked into how it tackled this problem.\nThe latest code for Keldon's AI is now searchable and browseable on github.\nThe ai code is in one file. It uses 2 distinct NNs, one for \"evaluating hand and active cards\" and the other for \"predicting role choices\".\nWhat you'll notice is that it uses a fair amount on non-NN code to model the game mechanics. Very much a hybrid solution. \nThe mapping of game state into the evaluation NN is done here. Various relevant features are one-hot-encoded, eg the number of goods that can be sold that turn.\n\nAnother excellent case study in mapping a complex game into a NN is the Starcraft II Learning Environment created by Deepmind in collaboration with Blizzard Entertainment. This paper gives an overview of how a game of Starcraft is mapped onto a set of features that a NN can interpret, and how actions can be issued by a NN agent to the game simulation.\n", "type": 2, "id": "5316", "date": "2018-02-15T03:46:45.293", "score": 4, "comment_count": 0, "parent_id": "4048"}, "5541": {"line": 3524, "body": "You would definitely want your network to know crucial information about the game, like what cards AI agent has(their values and types), mana pool, how many cards on the table and their values, number of the turn and so on. These things you must figure on your own, the question you should ask yourself is \"If I add this value to input how and why it will improve my system\". But the first thing to understand is that most of NNs are designed to have a constant input size, and I would assume this is matters in this game since players can have a different amount of cards in their hand or on the table. For example, you want to let NN know what cards it has, let's assume the player can have a maximum of 5 cards in his hand and each card can have 3 values(mana, attack and health), so you can encode this as 5*3 vector, where first 3 values represent card number one and so on. But what if the player has currently 3 cards, a simple approach would be to assign zeros to last 6 inputs, but this may cause problems since some cards can have 0 mana cost or 0 attack. So you need to figure out how to solve this problem. You may look for NN models that can handle variable input size or figure out how to encode input as a vector of constant size. \nSecondly, outputs are also constant size vectors. In case of this type of game, it can be a vector that encodes actions that the agent can take. So let's say we have 3 actions: put a card, skip turn and concede. So it can be one hot encoder, e.g. if you have 1 0 0 output, this means that agent should put some card. To know what card it should put you can add another element to output which will produce a number in the range of 1 to 5 (5 is max number of cards in the hand).\nBut the most important part of training a neural network is that you will have to come up with a loss function that is suitable for your task. Maybe standard loss functions like Mean-squared loss or L2 will be good, maybe you will need to change them in order to fit your needs. This is the part where you will need to make a research. I've never worked with NEAT before, but as I understood correctly it uses some genetic algorithm to create and train NN, and GA use some fitness function to select an individual. So basically you will need to know what metric you will be using to evaluate how good you model performs and based on this metric you will change parameters of the model.\nPS.\nIt is possible to solve this problem with the neural network, however, neural networks are not magic and not the universal solution to all problems. If your goal is to solve this certain problem I would also recommend you to dig into the game theory and its application in the AI. I would say, that solving this problem would require complex knowledge from different fields of AI.\nHowever, If your goal is to learn about neural networks I would recommend taking much simpler tasks. For example, you can implement NN that will work on benchmark dataset, for example, NN that will classify digits from MNIST dataset. The reason for this is that a lot of articles was written about how to do classification on this dataset and you will learn a lot and you will learn faster from implementing simple things.\n", "type": 2, "id": "5541", "date": "2018-03-06T16:10:26.510", "score": 2, "comment_count": 0, "parent_id": "4048"}, "7984": {"line": 5370, "body": "Yes.  It is feasible.\nOverview of the Question\nThe design goal of the system seems to be gain a winning strategic advantage by employing one or more artificial networks in conjunction with a card game playing engine.\nThe question shows a general awareness of the basics of game-play as outlined in Morgenstern and von Neuman's Game Theory.\n\nAt specific points during game-play a player may be required to execute a move.\nThere is a fininte set of move options according to the rules of the game.\nSome strategies for selecting a move produce higher winning records over multiple game plays than other strategies.\nAn artificial network can be employed to produce game-play strategies that are victorious more frequently that random move selection.\n\nOther features of game-play may or may not be as obvious.\n\nAt each move point there is a game state, which is needed by any component involved in improving game-play success.\nIn addition to not knowing when the opponent will bluff, in card games, the secret order of shuffled cards can introduce the equivalent of a virtual player the moves of which approximate randomness.\nIn three or more player games, the signaling of partners or potential partners can add an element of complexity to determining the winning game strategy at any point.  Based on the edits, it does not appear like this game has such complexities.\nPsychological factors such as intimidation can also play a role in winning game-play.  Whether or not the engine presents a face to the opponent is unknown, so this answer will skip over that.\n\nCommon Approach Hints\nThere is a common approach to mapping both inputs and outputs, but there is too much to explain in a Stack Exchange answer.  These are just a few basic principles.\n\nAll of the modeling that can be done explicitly should be done.  For instance, although an artificial net can theoretically learn how to count cards (keeping track of the possible locations of each of the cards), a simple counting algorithm can do that, so use the known algorithm and feed those results into the artificial network as input.\nUse as input any information that is correlated with optimal output, but don't use as inputs any information that can not possibly correlate with optimal output.\nEncode data to reduce redundancy in the input vector, both during training and during automated game-play.  Abstraction and generalization are the two common ways of achieving this.  Feature extraction can be used as tools to either abstract or generalize.  This can be done at both inputs and outputs.  An example is that if, in this game, J > 10 in the same way that A > K, K > Q, Q > J and 10 > 9, then encode the cards as an integer from 2 through 14 or 0 through 12 by subtracting one.  Encode the suits as 0 through 3 instead of four text strings.\n\nThe image recognition work is only remotely related, too different from card game-play to use directly, unless you need to recognize the cards from a visual image, in which case LSTM may be needed to see what the other players have chosen for moves.  Learning winning strategies would more than likely benefit from MLP or RNN designs, or one of their derivative artificial network designs.\nWhat an Artificial Network Would Do and Training Examples\nThe primary role of artificial networks of these types is to learn a function from example data.  If you have the move sequences of real games, that is a great asset to have for your project.  A very large number of them will be very helpful for training.\nHow you arrange the examples and whether and how you label them is worth consideration, however without the card game rules it is difficult to give any reliable direction.  Whether there are partners, whether it is score based, whether the number of moves to a victory, and a dozen other factors provide the parameters of the scenario needed to make those decisions.\nStudy Up\nThe main advise I can give is to read, not so much general articles on the web, but read some books and some of the papers you can understand on the above topics. Then find some code you can download and try after you understand the terminology well enough to know what to download.\nThis means book searches and academic searches are much more likely to steer you in the right direction than general web searches.  There are thousands of posers in the general web space, explaining AI principles with a large number of errors.  Book and academic article publishers are more demanding of due diligence in their authors.\n", "type": 2, "id": "7984", "date": "2018-09-15T04:23:14.300", "score": 3, "comment_count": 0, "parent_id": "4048"}, "31706": {"line": 20712, "body": "\nI'm not sure if or how I would apply neural networks to make\nselections with cards, which have a complex synergy. How could I\ndesign and train a neural network for this game, such that it can take\ninto account all the variables? Is there a common approach?\n\nAdvice 0.\nI would highly recommend to design it the way most neural networks are designed. The most common pattern is to have n layers, each one might have different size, one input layer, one output layer, a few hidden layers between them.\nAdvice 1.\nDifferentiate between neural network itself (brain), its environment (game conditions and rules), its sensors (inputs) and its \"hands\" (outputs). Why you might want to call it \"hands\"?  If you simulate a player in a card game, you basically want him to use his hands to play. In other situations it might be legs, or wings, or even the gas pedal.\nHow to design the inputs:\nJust create a neural network with a common pattern, figure out, what variables a real player would analyze before throwing a card, then try to translate those variables into signals. This step might actually be a bit tricky and it's also what actually happens in biological neural networks. The electrical signals in our brains are really weak, even though they might carry bits that make up big numbers.\nWhat I mean by making weak signals out of numbers is dividing varying diapasons of numbers by their maximal values. For example, instead of putting in 5, which would represent a card with rank 6, divide 5 by 13 (or whatever value represents the highest rank). Basically if your diapason of input for one neuron is 0 to 13, divide it by 13 to get a value (or signal) in codomain [0; 1], which is a suitable input for the sigmoid function compared to the values in codomain [0; 13]. To visualize, take a look at the graph of the sigmoid function.\n\nThe difference between f(10) and f(8) is ~0.00029, whereas the difference between f(10/13) and f(8/13) is ~0.034, which obviously has much more impact on the output. So, make sure you translate all the values into the diapason where your function is most \"sensitive\", in this case in [-4; 4]\nAdvice 2.\nEvery time you need a decision from the AI, create a pool of possible decisions (e. g. pool of possible cards the player can throw right now) that you can index by an integer. Then you might want to multiply the output value in its codomain [0; 1] by the amount of possible decisions - 1 to be able to interpret it as index. Or you might have the amount of output neurons that corresponds to the maximal amount of possible moves and interpret the index of the neuron with the highest value as the index of the array with possible decisions.\nHow to train it:\nIf you create AI for a game, I would recommend to take a look at genetic algorithms. The basic idea is to create a population of players (hundreds or thousands) with randomly generated weights and biases in their NNs, restrict their possibilities by the rules of the game, and let them play. Then perform selection by their fitness function, which in this case might just be the score of each player, crossover and mutation of their genes (of single bits or even numbers) to create the next generation, and so on. Repeat this process until you come up with a satisfying solution. I recommend you genetic algorithms for this case, because it might be quite hard to find training data for traditional methods of training NNs. And if you're able to generate training data yourself, then you might also be able to program all the behaviour manually, in which case you don't even need NNs.\nIf you're interested in training NNs using genetic algorithms, you should read some external literature, since it's a pretty big topic. You can also check out my github repo, where I train AI to play snake using GAs.\n", "type": 2, "id": "31706", "date": "2021-09-16T10:27:39.543", "score": 0, "comment_count": 0, "parent_id": "4048"}}}
{"line": 2332, "body": "Recently I was working on a problem to do some cost analysis of my expenditure for some particular resource.\nI usually make some manual decisions from the analysis and plan accordingly.\nI have a big data set in excel format and with hundreds of columns, defining the use of the resource in various time frames and types(other various detailed use).\nI also have information about my previous 4 years of data and actual resource usage and cost incurred accordingly.\nI was hoping to train a NN to predict my cost beforehand and plan even before I can manually do the cost analysis.\nBut the biggest problem I'm facing is the need to identify the features for such analysis. I was hoping there is some way to identify the features from the data set.\nPS - I have idea about PCA and some other feature set reduction techniques, what I'm looking at is the way to identify them in the first place.\n", "type": 1, "id": "3965", "date": "2017-09-04T07:20:24.417", "score": 11, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "datasets", "feature-selection"], "title": "How do I select the relevant features of the data?", "answer_count": 4, "views": 242, "accepted_answer": null, "answers": {"6554": {"line": 4298, "body": "there is no hard-and-fast-rule for feature selection , you have to manually examine the dataset and try different techniques for feature engineering . \nAnd there is no rule that you should apply neural networks for this , neural networks are time-consuming to train , instead you can experiment with decision tree based methods(random forests ) since your data is anyway in tabular structure.\n", "type": 2, "id": "6554", "date": "2018-05-29T16:00:07.817", "score": 0, "comment_count": 1, "parent_id": "3965"}, "9207": {"line": 6168, "body": "It is wise to consider not just the correlation of resource engagement with cost, but also the return on the cost of resource engagement.  The typical challenge is that those returns are almost always cumulative or delayed.  A case of accumulation is when the resource is the continuous tuning or improvement of a process the absence of which slows the generation of revenue.  A case of delay is when research resources incur costs without revenue impact for a period of time but the revenue generation that begins if the research delivers productive results may be a substantial factor above the total cost of the results delivered.\nThe reason expense data by itself can lead to maladaptive network learning is because a network that is trained to reduce, for instance, marketing expenses will zero them.  That would usually cause a decreasing sales lead trend until the business folds.  Without including the returns in the training information, no useful learning may occur.\nA basic MLP (multi-layer perceptron) will not learn the temporal characteristics of the data, the accumulation and delay aspects.  You will need a stateful network.  The most consistently successful network type for this kind of learning as of this writing is the LSTM (long short term memory) network type or one of its derivative variants.  Revenue and balance data must be used in conjunction with expense data to train the network to predict business results for any given sequence of proposed resource engagements (fully detailed budgetary plan).\nThe loss function must properly balance sort term with medium and long term financial objectives.  Negative available cash should produce a pronounced increase in the loss function so that such avoidance of basic risks to reputation and the cost of credit is learned.\nWhich columns in your data have strong correlations with return on investment is difficult to determine in advance.  You can immediately exclude columns that conform to any one of the following criteria.\n\nAlways empty\nOther constants, those that have the same value for every row\nThose that can always be derived from other columns\n\nThe data can be reduced in other ways\n\nFully describing data by characterizing trends in simple ways\nUsing indices to specify long strings with 100% accuracy by assigning each string a number\nCompression\nOtherwise reducing redundancy in the data\n\nRBMs (restricted Boltzmann machines) can extract features from the data and PCAs can illuminate the low information content columns, but the significance of the columns in terms of their correlation with revenue will not be identified using these devices in their basic form.\n", "type": 2, "id": "9207", "date": "2018-11-27T09:04:58.847", "score": 0, "comment_count": 0, "parent_id": "3965"}, "9644": {"line": 6435, "body": "Since you have all your data in a table, a relatively simple thing to do is to consider each column independently, and then seeing if the output variable (cost incurred) has a correlation to that.\nIf the column has no (or very low correlation) with the output variable, then consider it to be not important.  The ones that make the cut are then considered further.\nThis is obviously not very different from how a decision tree algorithm would work (such as ID3).\n", "type": 2, "id": "9644", "date": "2018-12-21T21:28:12.207", "score": 1, "comment_count": 0, "parent_id": "3965"}, "9192": {"line": 6160, "body": "That's a great question and probably one of the most difficult tasks on ML.\nYou do have a few options: \n\nYou can use weighting algorithms (e.g. Chi-squared) to understand which features are contributing most to your output\nYou can use other ML algorithms to classify whether a feature is contributing to your predictions or not\nYou may use other ML algorithms (other than NN) that inherently provide you with feature weightings (e.g. Random Forest) \n\nHope that helps \n", "type": 2, "id": "9192", "date": "2018-11-26T23:24:25.557", "score": 0, "comment_count": 0, "parent_id": "3965"}}}
{"line": 1922, "body": "I am not looking for an efficient way to find primes (which of course is a solved problem). This is more of a \"what if\" question.\nSo, in theory, could you train a neural network to predict whether or not a given number $n$ is composite or prime? How would such a network be laid out?\n", "type": 1, "id": "3389", "date": "2017-05-26T15:15:31.397", "score": 34, "comment_count": 1, "tags": ["neural-networks", "prediction", "primality-test"], "title": "Could a neural network detect primes?", "answer_count": 4, "views": 16087, "accepted_answer": "7598", "answers": {"3392": {"line": 1924, "body": "In theory, a neural network can approximate any given function. This result is known as the universal approximation theorem.\nHowever, if you train a network with the numbers $0$ to $N$, you cannot guarantee that the network will classify numbers outside that range correctly ($n > N$).\nSuch a network would be a regular feed-forward network (or MLP) as recurrency does not add anything to the classification of the given input. The number of layers and nodes of that NN can only be found through trial and error.\n", "type": 2, "id": "3392", "date": "2017-05-26T17:33:08.773", "score": 3, "comment_count": 2, "parent_id": "3389"}, "3461": {"line": 1970, "body": "yes it is feasible, but consider that integer factorization problem is an NP-something problem and BQP problem.\nbecause of this, it is impossible that a neural network purely based on classical computing finds prime number with 100% accuracy, unless P=NP.\n", "type": 2, "id": "3461", "date": "2017-06-08T01:15:58.567", "score": -3, "comment_count": 1, "parent_id": "3389"}, "7598": {"line": 5069, "body": "Early success on prime number testing via artificial networks is presented in A Compositional Neural-network Solution to Prime-number Testing, Laszlo Egri, Thomas R. Shultz, 2006.\nThe knowledge-based cascade-correlation (KBCC) network approach showed the most promise, although the practicality of this approach is eclipsed by other prime detection algorithms that usually begin by checking the least significant bit, immediately reducing the search by half, and then searching based other theorems and heuristics up to $floor(\\sqrt{x})$.  However the work was continued with Knowledge Based Learning with KBCC, Shultz et. al. 2006\nThere are actually multiple sub-questions in this question.  First, let's write a more formal version of the question: \"Can an artificial network of some type converge during training to a behavior that will accurately test whether the input ranging from $0$ to $2^n-1$, where $n$ is the number of bits in the integer representation, represents a prime number?\"\n\nCan it by simply memorizing the primes over the range of integers?\nCan it by learning to factor and apply the definition of a prime?\nCan it by learning a known algorithm?\nCan it by developing a novel algorithm of its own during training?\n\nThe direct answer is yes, and it has already been done according to 1. above, but it was done by over-fitting, not learning a prime number detection method.  We know the human brain contains a neural network that can accomplish 2., 3., and 4., so if artificial networks are developed to the degree most think they can be, then the answer is yes for those.  There exists no counter-proof to exclude any of them from the range of possibilities as of this answer's writing.\nIt is not surprising that work has been done to train artificial networks on prime number testing because of the importance of primes in discrete mathematics, its application to cryptography, and, more specifically, to cryptanalysis.  We can identify the importance of digital network detection of prime numbers in the research and development of intelligent digital security in works like A First Study of the Neural Network Approach in the RSA Cryptosystem, G.c. Meletius et. al., 2002.  The tie of cryptography to the security of our respective nations is also the reason why not all of the current research in this area will be public.  Those of us that may have the clearance and exposure can only speak of what is not classified.\nOn the civilian end, ongoing work in what is called novelty detection is an important direction of research.  Those like Markos Markou and Sameer Singh are approaching novelty detection from the signal processing side, and it is obvious to those that understand that artificial networks are essentially digital signal processors that have multi-point self tuning capabilities can see how their work applies directly to this question.  Markou and Singh write, \"There are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics.\"\nOn the cognitive mathematics side, the development of a mathematics of surprise, such as Learning with Surprise: Theory and Applications (thesis), Mohammadjavad Faraji, 2016 may further what Ergi and Shultz began.\n", "type": 2, "id": "7598", "date": "2018-08-16T10:21:10.127", "score": 16, "comment_count": 0, "parent_id": "3389"}, "11506": {"line": 7738, "body": "I'm an undergraduate researcher at Prairie View A&M university. I just spent a few weeks tweaking an MLPRegressor model to predict the $n$th prime number. It recently stumbled into a super low minimum, where the first $1000$ extrapolations outside of the training data produced error less than $.02$ percent. Even at $300000$ primes out, it was about $.5$ percent off. My model was simple: $10$ hidden layers, trained on a single processor for less than 2 hours.\nTo me, it begs the question, \"Is there a reasonable function that produces the nth prime number?\" Right now, the algorithms become computationally very taxing for extreme $n$. Check out the time gaps between the most recent largest primes discovered. Some of them are years apart. I know it's been proven that if such a function exists, it will not be polynomial.\n", "type": 2, "id": "11506", "date": "2019-03-28T21:23:52.587", "score": 2, "comment_count": 3, "parent_id": "3389"}}}
{"line": 1074, "body": "I am trying to build an agent to play carrom. The problem statement is roughly to estimate three parameters (normalized) : \n\nforce\nangle of striker\nposition of strike \n\nSince the state and action space both are continuous, I thought of discretizing the output such that I have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle, 10 outputs for force [ranging from 0 to 1] and 20 outputs for the position [ranging from 0 to 1].\nThus I will have 300 output of my neural network, but this number seems a bit too high compared to normal neural networks in practice. \nI was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action.\nIs there a generic way to approach such problems represented in 2D space. \n", "type": 1, "id": "2250", "date": "2016-11-01T19:42:17.713", "score": 2, "comment_count": 0, "tags": ["deep-learning", "deep-neural-networks", "models", "reinforcement-learning"], "title": "Network representation for Q-Learning in carrom", "answer_count": 1, "views": 188, "accepted_answer": "2258", "answers": {"2258": {"line": 1079, "body": "Discretizing the output will probably be counter-productive in this situation; it would remove flexibility by taking away the fine-grained continuous ranges between each discretization bucket, but also blow up the size of the network, reducing manageability and performance. Fragmenting the outputs into buckets in this way may also lead to information loss and more difficulty in convergence because of the fact that each bucket is partially isolated from the others. \nAfter causing myself needless hassle in the past by mismatching input and output dimensions, I'd simply do this if I were in your shoes: 1) keep it simple and use 3 continuous (well, semi-continuous, depending on the highest precision your programming framework allows) outputs for all of the above reasons; 2) clamp the angles between -45 to 225 by whatever method works best for you, like ceiling/floor hard clamping, adding weight terms, etc.; 3) go big on the hidden layer(s) to maximize information sharing across the inputs and eventual outputs, force, angle of strike, position of strike, etc. This is more likely to fine-tune the precision of the outputs, thereby making good use of the semi-continuous scales. \nI'm also wondering if a convolutional neural net might work in this situation; their most popular use case is in image processing, but I don't see why you can't treat the force, angle and position as surrogate spatial dimensions. I'm not sure how many or what type of inputs you have, but 3 continuous outputs might be conducive to a 3D rather than a 2D space. Convolutionals are often used for those, as well as higher-dimensional and temporal data. I hope that helps. \n", "type": 2, "id": "2258", "date": "2016-11-04T04:38:15.947", "score": 0, "comment_count": 0, "parent_id": "2250"}}}
{"line": 3477, "body": "I am doing a project on visual place recognition in changing environments.  The CNN used here is mostly AlexNet, and a feature vector is constructed from layer 3.\nDoes anyone know of similar work using other CNNs, for example, VGGnet (which I am trying to use) and the corresponding layers?\nI have been trying out the different layers of VGGnet-16.  I am trying to get the nearest correspondence to the query image by using the cosine difference between the query image and database images. So far no good results.\n", "type": 1, "id": "5474", "date": "2018-03-01T12:50:55.907", "score": 6, "comment_count": 1, "tags": ["machine-learning", "convolutional-neural-networks", "computer-vision", "visual-place-recognition"], "title": "Which neural networks are suitable for visual place recognition?", "answer_count": 1, "views": 278, "accepted_answer": null, "answers": {"6620": {"line": 4348, "body": "Neural networks construct increasingly complex representations of data on each of their layers , so you are free to choose any neural network architechture for this purpose . since the lower layers of neural network (layers near the input layer) mostly compute low level representations of the image (like gabor filters etc) most architechtures won't have much difference at this level. so you can use VGGnet if you want with proper fine-tuning from layer 3 itself.\n", "type": 2, "id": "6620", "date": "2018-06-01T21:41:30.590", "score": 0, "comment_count": 2, "parent_id": "5474"}}}
{"line": 3190, "body": "I am trying to develop a neural network which can identify design features in CAD models (i.e. slots, bosses, holes, pockets, steps).\nThe input data I intend to use for the network is a n x n matrix (where n is the number of faces in the CAD model). A '1' in the top right triangle in the matrix represents a convex relationship between two faces and a '1' in the bottom left triangle represents a concave relationship. A zero in both positions means the faces are not adjacent. The image below gives an example of such a matrix.\n\nLets say I set the maximum model size to 20 faces and apply padding for anything smaller than that in order to make the inputs to the network a constant size.\nI want to be able to recognise 5 different design features and would therefore have 5 output neurons - [slot, pocket, hole, boss, step]\nWould I be right in saying that this becomes a sort of 'pattern recognition' problem? For example, if I supply the network with a number of training models - along with labels which describe the design feature which exists in the model, would the network learn to recognise specific adjacency patterns represented in the matrix which relate to certain design features?\nI am a complete beginner in machine learning and I am trying to get a handle on whether this approach will work or not - if any more info is needed to understand the problem leave a comment. Any input or help would be appreciated, thanks. \n", "type": 1, "id": "5096", "date": "2018-01-22T21:40:56.423", "score": 10, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks", "pattern-recognition"], "title": "Using neural network to recognise patterns in matrices", "answer_count": 3, "views": 1907, "accepted_answer": null, "answers": {"5097": {"line": 3191, "body": "As far as I understand, yes your problem is related to pattern recognition. Since the approach is to classify inputs with labels you previously provide for the neural net, I think a convolutional neural networks could work for you problem. \n", "type": 2, "id": "5097", "date": "2018-01-23T05:01:46.390", "score": 0, "comment_count": 0, "parent_id": "5096"}, "6507": {"line": 4262, "body": "\nWould I be right in saying that this becomes a sort of 'pattern recognition' problem? \n\nTechnically, yes. In practice: no.\nI think you might be interpreting the term \"pattern recognition\" a bit too literal. Even though wikipedia defines Pattern recognition as \"a branch of machine learning that focuses on the recognition of patterns and regularities in data\", it's not about solving problems that can \"easily\" be deduced by logical reasoning.\nE.g. you say that\n\nA '1' in the top right triangle in the matrix represents a convex relationship between two faces and a '1' in the bottom left triangle represents a concave relationship\n\nThis is true always. In a typical machine learning situation, you wouldn't (usually) have this prior knowledge. At least not to the extent that it would b be tractable to \"solve by hand\".\nPattern recognition is conventionally a statistical approach to solving problems when they get too complex to analyze with conventional logical reasoning and simpler regression models. Wikipedia also states (with a source) that pattern recognition \"in some cases considered to be nearly synonymous with machine learning\".\nThat being said: you could use pattern recognition on this problem. However, it seems like overkill in this case. Your problem, as far as I can understand, has an actual \"analytical\" solution. That is: you can, by logic, get a 100% correct result all the time. Machine learning algorithms could, in theory, also do this, and in that case, and this branch of ML is referred to as Meta Modelling[1].\n\nFor example, if I supply the network with a number of training models - along with labels which describe the design feature which exists in the model, would the network learn to recognise specific adjacency patterns represented in the matrix which relate to certain design features?\n\nIn a word: Probably. Best way to go? Probably not. Why not, you ask?\nThere is always the possibility that your model doesn't learn exactly what you want. In addition you have many challenges like overfitting that you'd need to concern yourself about. It's a statistical approach, as I said. Even if it classifies all your test data as 100% correct, there is no way (unless you check the insanely intractable maths) to be 100% sure that it will always classify correctly. I further suspect that you're also likely to end up spending more time working on your model then the time it would take to just deduce the logic.\nI also disagree with @Bitzel: I would not do a CNN (convolutional neural network) on this. CNNs are used when you want to look at specific parts of the matrix, and the relation and connectedness between the pixels are important -- for example on images. Since you only have 1s and 0s, I strongly suspect that a CNN would be vastly overkill. And with all the sparsity (many zeros) you'd end up with a lot of zeros in the convolutions. \nI'd actually suggest a plain vanilla (feed forward) neural network, which, despite the sparsity, I think will be able to do this classification pretty easily.\n", "type": 2, "id": "6507", "date": "2018-05-24T18:42:36.037", "score": 1, "comment_count": 1, "parent_id": "5096"}, "9422": {"line": 6300, "body": "The Problem\nThe training data for the proposed system is as follows.\n\nA Boolean matrix representing the surface adjacency of a solid geometric design\nAlso represented in the matrix is differentiation between interior and exterior angles of edges\nLabels (described below)\n\nConvex and concave are not the correct terms to describe surface gradient discontinuities. An interior edge, such as made by an end mill, is not actually a concave surface. Surface gradient discontinuity, from the point of view of the idealized solid model, has a zero radius. An exterior edge is not a convex portion of a surface for the same reason.\nThe intended output of the trained system proposed is a Boolean array indicating the presence of specific solid geometric design features.\n\nOne or more slot\nOne or more boss\nOne or more holes\nOne or more pockets\nOne or more steps\n\nThis array of Boolean values is also used as labels for training.\nPossible Caveats in Approach\nThere are mapping incongruities in this approach. They fall roughly into one of four categories.\n\nAmbiguity created by mapping the topology in the CAD model to the matrix -- solid geometries that have primary not captured in the matrix encoding proposed\nCAD models for which no matrix exists -- cases where edges change from inner to outer angles or emerge from curvature\nAmbiguity in the identification of features from the matrix -- overlap between features that could identify the pattern in the matrix\nMatrices describing features that are not among the five -- this could become a data loss issue downstream in development\n\nThese are just a few examples of topology issues that may be common in some mechanical design domains and obfuscate the data mapping.\n\nA hole has the same matrix as a box frame with internal radii.\nExternal radii may lead to oversimplification in the matrix.\nHoles that intersect with edges may be indistinguishable from other topology in matrix form.\nTwo or more intersecting through holes may present adjacency ambiguities.\nFlanges and ribs supporting round bosses with center holes may be indistinguishable.\nA ball and a torus have the same matrix.\nA disk and band with a hexagonal cross with a 180 degree twist have the same matrix.\n\nThese possible caveats may or may not be of concern for the project defined in the question.\nSetting a face size balances efficiency with reliability but limits usability. There may be approaches that leverage one of the variants of RNNs, which may permit coverage of arbitrary model sizes without compromising efficiency for simple geometries. Such an approach may involve splaying the matrix out as a sequence for each example, applying a well conceived normalization strategy to each matrix. Padding may be effective if there are no tight constraints on training efficiency and a practical maximum for number of faces exists.\nConsidering Count and Certainty as Output\nTo handle some of these ambiguities, a certainty $\\in [0.0, 1.0]$ could be the range of the activation functions of the output cells without changing the labeling of the training data.\nThe possibility of using a non-negative integer output, as an unsigned binary representation created by aggregating multiple binary output cells, instead of a single Boolean per feature should be at least considered as well. Downstream, the capability to count features may become important.\nThis leads to five realistic permutations to consider, that could be produced by the trained network for each feature of each solid geometry model.\n\nBoolean indicating existence\nNon-negative integer indicating instance count\nBoolean and real certainty of one or more instance\nNon-negative integer representing most likely instance count and real certainty of one or more instances\nNon-negative real mean and standard deviation\n\nPattern Recognition or What?\nIn the current culture, applying an artificial network to this problem is not normally described as pattern recognition in the sense of computer vision or audio processing. It is thought of as learning a complex functional mapping via convergence in the rough direction of an idea mapping, given proximity, accuracy, and reliability criteria. The parameters of the function $f$, given inputs $\\mathcal{X}$, are driven toward the associated labels $\\mathcal{Y}$ during training.\n$$f(\\mathcal{X}) \\implies \\mathcal{Y}$$\nIf the concept class being functionally approximated by the network is sufficiently represented in the sample used for training and the sample of training examples is drawn in the same way as the target application will later draw, the approximation is likely to be sufficient.\nIn the world of information theory, there is a blurring of the distinction between pattern recognition and functional approximation, as there should be in that higher level AI conceptual abstraction.\nFeasibility\n\nWould the network learn to map matrices to [the array of] Boolean [indicators] of design features?\n\nIf the above listed caveats are acceptable to the project stakeholders, the examples are well labeled and provided in sufficient number, and the data normalization, loss function, hyper-parameters, and layer arrangements are set up well, it is likely convergence will occur during training and a reasonable automated feature identification system. Again, its usability hinges on new solid geometries being drawn from the concept class like the training examples were. System reliability relies on training being representative of later use cases.\n", "type": 2, "id": "9422", "date": "2018-12-09T05:37:16.240", "score": 0, "comment_count": 0, "parent_id": "5096"}}}
{"line": 2339, "body": "When designing a machine-learning system, there are various parameters that have to be determined. I am interested in the following general question: is it possible to construct a dataset on which the system will have good performance with some specific set of parameters, but not with other paramteres?\nTo be more concrete, let's focus on neural networks. Suppose we have a simple neural network: a multilayer perceptron with a single hidden layer. The size of the input is fixed, the activation function is fixed (e.g. tanh), and the output is binary. The only parameter that has to be determined is the size of the hidden layer. \nMy question is: given a number $n$, is it possible to construct a dataset $D_n$ such that:\n\nThe MLP with $n$ hidden nodes has good performance on $D_n$ (e.g. in 10-fold cross validation);\nThe MLP with $n-1$  hidden nodes has bad performance on $D_n$\n\n?\nNote: I asked in CS theory but got no reply.\n", "type": 1, "id": "3972", "date": "2017-09-05T05:20:56.677", "score": 5, "comment_count": 0, "tags": ["neural-networks", "machine-learning"], "title": "Constructing a dataset that scores well only for a specific set of hyper parameter values", "answer_count": 2, "views": 78, "accepted_answer": "7862", "answers": {"6563": {"line": 4305, "body": "Hidden layer number is an indicator of the dimension of the weight matrix , so once you train a network with certain number of  hidden layer neurons  , the weights get fixed at the point you stop training. so changing the number hidden layer neurons makes the previous weights incompatible with it due to change in weight-matrix dimenstion . even if you alter the weight matrix to work with the new network it is again equivalent to a network with randomly assigned weights. so a fully trained networks becomes a new network once you change its hidden layer dimension and gives bad performance. This hase a very little to do with the dataset.\n", "type": 2, "id": "6563", "date": "2018-05-29T20:49:08.923", "score": 0, "comment_count": 0, "parent_id": "3972"}, "7862": {"line": 5266, "body": "I'm not familiar with any existing, robust methods to generate such a dataset. Here are some thoughts though.\nYou propose using an MLP with a single hidden layer. That means we have two weight matrices, and two activation functions (one for hidden layer, one for output layer). Some notation:\n\n$d$: dimensionality of input vectors\n$n$: dimensionality of hidden layer\nYou mentioned binary output, so I'll assume that dimensionality is $2$\n$W_n^{(1)} \\in \\mathbb{R}^{d \\times n}$: the first weight matrix in the scenario where we're using $n$ hidden nodes\n$W_n^{(2)} \\in \\mathbb{R}^{n \\times 2}$: the second weight matrix in the scenario where we're using $n$ hidden nodes\n$g$: first activation function\n$h$: second activation function\n\nThen, given an input vector $x \\in \\mathbb{R}^d$, our neural network will generate output $f_n(x) = h(W_n^{(2)} (g(W_n^{(1)}x)))$.\nNow, in general we of course expect Neural Networks to only find a local optimum, but if you want a robust solution you'll want it to be able to handle the worst case, and the worst case for your \"adversarial\" task is when the Neural Network manages to find the global optimum. So, we'll assume it can find the global optimum.\nEssentially, what you're looking for is a dataset $D_n$ containing a number of input vectors $x$, such that:\n\n$f_n(x) = h(W_n^{(2)} (g(W_n^{(1)}x)))$ provides good / optimal results (after training to the global optimum)\n$f_{n - 1}(x) = h(W_{n - 1}^{(2)} (g(W_{n - 1}^{(1)}x)))$ provides poor results (even after training to the global optimum of this setup).\n\nIn other words, you want to find a collection of vectors $x$ such that it becomes impossible to find a collection of weights in the $n - 1$ case where $f_{n-1}(x)$ is a good approximation of $f_n(x)$. You want to make it impossible that $f_{n - 1}(x) \\approx f_n(x)$.\n\nNow this has turned into a clear mathematical problem. I'm not familiar with any established methods in mathematics to solve a problem like this, maybe there are though. My best guess at this point in time would be a procedure like the following:\n\nGenerate random \"ground truth\" versions of the weight matrices $W^{(1)*}_{n}$ and $W^{(2)*}_{n}$ (just completely random matrices).\nGenerate completely random input vectors $x$. Compute the corresponding ground truth labels as $f^*_n(x) = h(W_n^{(2)*} (g(W_n^{(1)*}x)))$.\nHope that the neural network with $n$ hidden nodes can recover the ground truth weight matrices that were previously generated randomly.\nHope that the neural network with $n - 1$ hidden nodes cannot find an accurate approximation.\n\nIn theory, the MLP with $n$ hidden nodes should be able to learn the exact ground truth function. In theory, under certain conditions, the MLP with $n$ hidden nodes should not be able to learn the exact ground truth function. I suspect those \"certain conditions\" would be that the rows/columns of the weight matrices should be linearly independent, which is likely with randomly generated matrices, but I'm not 100% sure on this. Even if it can't learn the exact ground truth, it may still be capable of learning an approximation... there may be ways to find upper bounds on how close such an approximation could get, but I'm not sure.\n", "type": 2, "id": "7862", "date": "2018-09-06T19:57:45.970", "score": 2, "comment_count": 1, "parent_id": "3972"}}}
{"line": 1876, "body": "I was able to extract the license plate from a given car image, using Matlab. I would like to use deep neural networks to recognize the characters on the plate now. How can i proceed further? I don't have any experience with deep neural networks implementation.\n", "type": 1, "id": "3313", "date": "2017-05-15T11:33:39.930", "score": 2, "comment_count": 2, "tags": ["neural-networks", "optical-character-recognition"], "title": "How can I use deep neural networks to recognize characters on vehicle license plate?", "answer_count": 1, "views": 695, "accepted_answer": "3314", "answers": {"3314": {"line": 1877, "body": "Before jumping into any machine learning task it is good to consider your dataset and the features you wish to use. License plates are unique thus feeding the entire plate into a machine learning algorithm would not yield very accurate results. First, you will want to make sure that the extracted license plate images are similar in size. Then you will want to separate the license plate such that you can separate the different digits and letters into their own respective images. Thus your dataset that will be used to train your network will consists of many single character images which contain either a letter or a digit. When you have finished training your model, future license plates will also have to be broken down in this way, classified and then the outputs be combined to get the license plate number.\nWhat is a neural network?\nA neural network (NN) is a series of nodes which contain a simple, continuous, differentiable function such as logistic regression. Alone these nodes are not very powerful, however, the beauty comes in when they are connected in networks. The tuning of such a network can learn highly complex functions. Typically, a NN is set up with three layers. The input layer, where you will feed in your features, the hidden layer and the output layer. Naturally, if you want a deep NN then you just need to add more hidden layers. But, there is a trap. The deeper your network is the harder it will be to train, thus you will need MORE data. \nThe input-layer\nThis is where you will be feeding in your images. If you have a 16*16 pixel image then you will have 256 input nodes. This layer typically does not perform any function on the input, it is simply taking in the input. It will then feed the input to the hidden layer. You can use some dimensionality reduction techniques to try and eliminate some useless information from your dataset such as the white spaces in the corner of the image. The less input nodes you have, the more informative your samples become, thus with higher information entropy your network will be able to learn faster, or more rigidly with the same amount of data. \nThe hidden layers\nYou are free here to choose how many layers you want and how many nodes you want in each layer. I suggest using grid search and cross-validation as a means to determine which configuration is ideal for your dataset. Too many layers and nodes and you will not end up with a fully trained networks once you run out of training data (too much bias). Too little nodes, your network will not be able to learn the subtleties which are embedded within the data (too much variance). You want to find yourself at the crossroad where these two are minimized. Choose wisely. \nThe output layer\nThis layer can be configured in a few different ways. Typically people like to do one output node for each class. In your case there are 26 letters in the alphabet and 10 digits. This gives 36 possible output classes. So you will need 36 nodes in your output layer.\nHow do I train this?\nThe training of the NN is by tuning the weights that will be applied to the inputs of each of your nodes. Try to draw your dense network, for each line connecting nodes you will have a weight. I am sure you can see how this becomes quite huge. You will initialize your weights randomly to start. Then you will take each of your examples and pass them through your network. This is called a feed-forward pass through. This will provide you with an output. You will compare your output with your ground truth value using a cost function. This is often the root mean squared error (RMSE) but there are many other options. Then using the derivative you will back-propagate the error through your network to see which weights caused the error and to what extent. The derivative will then allow you to update those weights proportionately.\nWOW do I need to program all that?\nNope not anymore!!! You can use Keras in python (my recommendation) or other similar frameworks which have made it a real tea sipping pleasure to program a variety of NN configurations. \n\nMy recommendation\nI would start with a simple NN and see what kind of accuracy I can achieve. From there I would try to use convolutional neural networks which have been shown to garner much better results on datasets comprised of images. These can also be implemented in Keras. CNN require more data than typical NN thus you might need to get access to more images.\nIt is always best to train your dataset only on images you have in your original data. However, when it comes to images, adding to your data by artificially shifting images by 1-5 pixels or rotating by 1-5 degrees can add variability which can supplement your original data. \nMake sure that the license plate pictures you are using to train your network are taken with a similar system as the images you will be using in the future. You will not get very good results if you train your network on images taken directly behind cars and then you expect to implement your solution in traffic cameras monitoring traffic from above. \n", "type": 2, "id": "3314", "date": "2017-05-15T13:27:20.490", "score": 0, "comment_count": 7, "parent_id": "3313"}}}
{"line": 1842, "body": "These types of questions may be problem-dependent, but I have tried to find research that addresses the question whether the number of hidden layers and their size (number of neurons in each layer) really matter or not.\nSo my question is, does it really matter if we for example have 1 large hidden layer of 1000 neurons vs. 10 hidden layers with 100 neurons each?\n", "type": 1, "id": "3262", "date": "2017-05-04T13:06:37.990", "score": 15, "comment_count": 0, "tags": ["neural-networks"], "title": "1 hidden layer with 1000 neurons vs. 10 hidden layers with 100 neurons", "answer_count": 4, "views": 20881, "accepted_answer": "3263", "answers": {"3263": {"line": 1843, "body": "Basically, having multiple layers (aka a deep network) makes your network more eager to recognize certain aspects of input data. For example, if you have the details of a house (size, lawn size, location etc.) as input and want to predict the price. The first layer may predict:\n\nBig area, higher price\nSmall amount of bedrooms, lower price\n\nThe second layer might conclude:\n\nBig area + small amount of bedrooms = large bedrooms = +- effect\n\nYes, one layer can also 'detect' the stats, however it will require more neurons as it cannot rely on other neurons to do 'parts' of the total calculation required to detect that stat.\nCheck out this answer\n", "type": 2, "id": "3263", "date": "2017-05-04T13:10:53.287", "score": 13, "comment_count": 8, "parent_id": "3262"}, "3267": {"line": 1844, "body": "\nI think you have a confusion in the basics of the neural networks.\n  Every layer has a separate activation function and input/output\n  connection weights.\n\nThe output of the first hidden layer will be multiplied by a weight, processed by an activation function in the next layer and so on.\nSingle layer neural networks are very limited for simple tasks, deeper NN can perform far better than a single layer. \nHowever, do not use more than layer if your application is not fairly complex. In conclusion, 100 neurons layer does not mean better neural network than 10 layers x 10 neurons but 10 layers are something imaginary unless you are doing deep learning. start with 10 neurons in the hidden layer and try to add layers or add more neurons to the same layer to see the difference. learning with more layers will be easier but more training time is required.\n", "type": 2, "id": "3267", "date": "2017-05-05T00:34:03.793", "score": 0, "comment_count": 0, "parent_id": "3262"}, "3269": {"line": 1846, "body": "There are so many aspects.\n1. Training:\nTraining deep nets is a hard job due to the vanishing (rearly exploding) gradient problem. So building a 10x100 neural-net is not recommended.\n2. Trained network performance:\n\nInformation loss:\nThe classical usage of neural nets is the classification problem. Which means we want to get some well defined information from the data. (Ex. Is there a face in the picture or not.)\nSo usually classification problem has a lot of input, and few output, whats more the size of the hidden layers are descend from input to output.\nHowever, we loss information using less neurons layer by layer. (Ie. We cannot reproduce the original image based on the fact that is there a face on it or no.) So you must know that you loss information using 100 neurons if the size of the input is (lets say) 1000.\nInformation complexity: However the deeper nets (as Tomas W mentioned) can fetch more complex information from the input data. Inspite of this its not recommended to use 10 fully connected layers. Its recommended to use convolutional/relu/maxpooling or other type of layers. Firest layers can compress the some essential part of the inputs. (Ex is there any line in a specific part of the picture) Second layers can say: There is a specific shape in this place in the picture. Etc etc.\n\nSo deeper nets are more \"clever\" but 10x100 net structure is a good choice.\n", "type": 2, "id": "3269", "date": "2017-05-05T14:21:31.097", "score": 4, "comment_count": 0, "parent_id": "3262"}, "8021": {"line": 5393, "body": "If the problem you are solving is linearly separable, one layer of 1000 neurons can do better job than 10 layers with each of 100 neurons.\nIf the problem is non linear and not convex, then you need deep neural nets. \n", "type": 2, "id": "8021", "date": "2018-09-17T18:41:57.850", "score": 1, "comment_count": 1, "parent_id": "3262"}}}
{"line": 2719, "body": "I have a set of images that I already trained a CNN to classify successfully. I wonder if it would be possible to encode the images (using XOR in combination with a key of the same length as the image) and train a new net on them. \nThinking logically, the features still exist in the same relation to each other, just in a different form (encoded). Considering that neural networks are incredible at pattern recognition, I assume that it would still be doable.\nFor people, who cannot imagine how a xor-encoded image would look like:\n\nFor a human, it may look like rubbish, but the information is definitely there.\nWould love to read your opinion.\n", "type": 1, "id": "4472", "date": "2017-11-09T10:27:06.057", "score": 3, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "image-recognition"], "title": "Would convolutional NN recognize patterns in encoded images?", "answer_count": 3, "views": 359, "accepted_answer": "4485", "answers": {"4485": {"line": 2726, "body": "First you should give it a try, because anyone's guess could be off, as there isn't really a complete high level analytic model of how neural networks behave on real data. Most results with neural networks are informed by theory, but involve a whole lot of experimental testing.\nI suspect your network might at least learn something from the new images, but would struggle to get anything like the same accuracy as without the noise, because the CNN filters rely on being able to detect similar features at different positions. In your scrambled image, there will not be any meaningful and consistent edges/corners etc that a single learned feature detector could learn to match (and therefore present to the next layer).\nA fully-connected network would not have this limitation, and would learn just as well on a set of binary features that have been xor'd identically in each position for each example, as it did on the original copy (i.e. only if the picture was 1 bit depth). It would learn less well if each feature was a scaled 8-bit pixel value that was xor'd with the same 8 bit random number in each pixel position, because that would introduce many more non-linear mappings between input and output. Of course a fully-connected network will generally not learn image tasks as well as CNNs in the first place . . . but if it could learn anything useful at all for your image problem, then it will probably out-perform CNNs after the scrambling effect.\nAs a CNN usually has a few fully-connected layers, then it may be possible to get something from your scrambled images.\n\nThinking logically, the features still exist in the same relation to each other, just in a different form (encoded)\n\nIn terms of being recognisable in the way that a CNN filter extracts them, then the features do not exist. That is a problem.\n", "type": 2, "id": "4485", "date": "2017-11-10T22:31:11.353", "score": 4, "comment_count": 1, "parent_id": "4472"}, "4486": {"line": 2727, "body": "The answer to your question depends on the nature of the noise that you have xor-ed the image with. If it is the case that the noise is random (or pseudorandom in the formal sense), then it is provably the case that the original pattern will not be learnable in the statistical learning theory sense; this scenario is equivalent to the application of a one-time pad.\nTo quote the relevant Wikipedia article:\n\nOne-time pads are \"information-theoretically secure\" in that the encrypted message (i.e., the ciphertext) provides no information about the original message to a cryptanalyst (except the maximum possible length[16] of the message). This is a very strong notion of security first developed during WWII by Claude Shannon and proved, mathematically, to be true for the one-time pad by Shannon about the same time.\n\n", "type": 2, "id": "4486", "date": "2017-11-11T01:08:36.100", "score": 0, "comment_count": 1, "parent_id": "4472"}, "4490": {"line": 2730, "body": "My gut feel based on the paper I will mention below is that YES, if you apply the same XOR operation on the train and test data, you will be able to train a very \"accurate\" classifier. \nTo elaborate on my \"gut\" feel, please allow me to introduce to you what I personally think is one of the most important paper that came out this year(in fact this paper won the best paper award at ICLR 2017):\n Understanding deep learning requires rethinking generalization. \nIn this paper, the authors showed that deep learning models will generalize to \"any\" datasets. To give an example of the sort of experiment they conducted on this paper: \n\nThey randomly shuffled the training and test set's labels around in such a manner that for example some images of cats were labeled as dogs whiles some dogs were named cats whilst some cats and dogs images remained correctly labeled. Now it is well understood that deep learning models(including CNNs) are quite resistant to a few noisy labels but in the experiments conducted in the paper mentioned above this was a significant amount of noisy which begs the question why neural networks still performed well on what ended up being a garbage dataset. \n\nThe moral of the story is that contrary to what most researchers believed in the past namely that deep learning models magically discover lower level features, middle-level features, and higher-level features hidden within the dataset more like the V1 system of the mammalian brain by learning to compress data, they seem to just memorize anything you give them, including random data. \nIn short the paper mentioned above showed that deep learning models generalize well to completely random noise(in your case, think images generated from random pixels). Deep learning models will generalize well to anything, anything. And if they can generalize to random data which have no structure, then images that underwent a fixed, predefined transformation like XOR have nothing to a deep learning model. \nI must say, this are very worrying findings - to me at least. \n", "type": 2, "id": "4490", "date": "2017-11-11T18:59:40.683", "score": 0, "comment_count": 0, "parent_id": "4472"}}}
{"line": 2700, "body": "I've mostly seen (e.g. in  The Unreasonable Effectiveness of Recurrent Neural Networks) that when training RNN on text for something like language modeling, the text is usually featurized character-by-character using a 1-hot encoding.\nFor example, the text \"hello\" would be represented like\n{h: 1, e: 0, l: 0, o: 0}\n{h: 0, e: 1, l: 0, o: 0}\n{h: 0, e: 0, l: 1, o: 0}\n{h: 0, e: 0, l: 1, o: 0}\n{h: 0, e: 0, l: 0, o: 1}\n\nI was wondering if one could just as well use the ASCII encoding of the text and feed the bits in one by one. So the input \"hello\" would be input like\n0110100001100101011011000110110001101111 \n\nWould the RNN have a disproportionately harder time having to figure out how the arbitrary and complex 8-bit ASCII encoding should be used? Or would the ASCII encoding lead to about the same performance as the nicer 1-hot encoding?\n", "type": 1, "id": "4436", "date": "2017-11-05T11:58:43.207", "score": 4, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "natural-language-processing", "recurrent-neural-networks", "long-short-term-memory"], "title": "Training RNN's on text: Can you use an ASCII encoding just as well as a one-hot character encoding?", "answer_count": 1, "views": 202, "accepted_answer": null, "answers": {"31708": {"line": 20714, "body": "My understanding is that the ASCII encoding would not get the best performance or results from the RNN because the ASCII codes for each character are not meaningful; they are arbitrary. If the number of each ASCII code represented something meaningful about the letter, it would work better. But they don't.\nThe same principles apply as when deciding how to encode any categorical data. If your categories are ordinal (eg. 'First', 'Second' .. or 'Age group 18-24', Age group 25-35' .. or even 'Social Class E', 'Social Class D' ..), then assigning a single numerical value to each class might work well. But in categorical data where there is no meaningful order, one hot encoding will work better.\nThis is an example of the principle of giving neural networks the most expressive data that we can. In the case of non-ordinal, arbitrary categories, one-hot is more expressive to the next layer of neurons (will stimulate them more distinctly) than using a numerical encoding.\n", "type": 2, "id": "31708", "date": "2021-09-16T11:21:15.597", "score": 0, "comment_count": 0, "parent_id": "4436"}}}
{"line": 3207, "body": "As many papers point out, for better learning curve of a NN, it is better for a data-set to be normalized in a way such that values match a Gaussian curve.\nDoes this process of feature normalization apply only if we use sigmoid function as squashing function? If not what deviation is best for the tanh squashing function?\n", "type": 1, "id": "5116", "date": "2018-01-24T23:21:22.067", "score": 3, "comment_count": 0, "tags": ["neural-networks", "datasets"], "title": "Data-set values feature scaling: sigmoid vs tanh", "answer_count": 2, "views": 568, "accepted_answer": "5124", "answers": {"5118": {"line": 3209, "body": "Yes it applies, and no it shouldn't matter that much between the two activation functions.\n", "type": 2, "id": "5118", "date": "2018-01-25T03:19:50.767", "score": 0, "comment_count": 1, "parent_id": "5116"}, "5124": {"line": 3215, "body": "It has little to do with activation functions.\nSay you have a 2 input Neural Net with 1 node in the hidden layer and 1 node in the output layer all with the sigmoid activation function.\nSay suppose we are solving a bipolar classification problem. Now say one of the inputs is in the order of 10^4 and the other in order of 10 only. Now the Neural Net will propagate this values to the output layer via the hidden layer.\nYou get an error delta which is propagated back to the input layer.\nNow as per gradient descent rule (if you look at the formula) the weight reduction will directly be proportional to delta * x_i where x_i is the ith input. SInce delta the net error due to both inputs is same for both, so The NN first decreases/increases the weight of a connection to make it to scale, and then after that the real learning which we are interested in occurs. Also if you see institutionally at the beginning if we give random weights the input which is larger will have more contribution to the output. It basically dictates the output. Now as the NN learns it reduces the weight-age of this large input to counter balance its high value. But if you do this at the beginning  voila! the NN will train much faster. \nIts like you have 2 kids, one naughtier than other. You leave them alone in a home and one of them breaks something (delta). But since you have no idea who did it you will contribute the delta to them equally. But as you learn about the kids nature your view (weight-age) about who broke things when you are not at home changes. Normalization is basically someone warning you already who the naughtier kid is and so you are able to take a balanced viewpoint from the beginning. (Very Bad example, but I could not think any better).\nThis example takes only 2 input so it might seem not much of a gain but in real life it will be if there are a large number of inputs and a lot of magnitude difference in them.\nI may have missed something or other mathematical subtleties and I will be grateful if someone points them out.\n", "type": 2, "id": "5124", "date": "2018-01-25T15:08:30.673", "score": 0, "comment_count": 0, "parent_id": "5116"}}}
{"line": 3320, "body": "I'm trying to implement some image super-resolution models on medical images. After reading a set of papers, I found that none of the existing models use any activation function for the last layer.\nWhat's the rationale behind that?\n", "type": 1, "id": "5265", "date": "2018-02-10T01:54:04.027", "score": 3, "comment_count": 0, "tags": ["deep-learning", "computer-vision", "deep-neural-networks", "activation-function", "image-super-resolution"], "title": "Why is no activation function used at the final layer of super-resolution models?", "answer_count": 2, "views": 2080, "accepted_answer": null, "answers": {"5680": {"line": 3636, "body": "I am not into the field of super-resolution, but I think this question applies to general neural network construction.\nUsually, you try to solve a classification problem or a regression problem with your neural network.\n\nFor classification, you try to predict probabilities that a specific output corresponds to a specific class. Therefore, every output value should be a probability and therefore have a range between 0 and 1. To achieve this, you usually use a softmax or sigmoid function as your last layer to squash the output between 0 and 1. In addition to this (which is wanted in classification tasks), these functions raise the probability output of likely classes while decreasing the probability of all other unlikely classes (therefore enforcing the network to choose for one specific class over the others).\n\nFor the regression task, you are not looking for probability values as your output values but instead for real-valued numbers. In such a case, no activation function is wanted, since you want to be able to approximate any possible real value and not probabilities.\n\n\nSo, in the case of super-resolution, I think the generated output is a map where each value corresponds to a pixel value of the super-resolution image. In that case, your pixels are real number values and no probabilities. So, you are solving a regression problem.\nBut you could also go with a classification approach, where you have 256 output maps that give probability to each possible pixel value between $0$ and $255$.\n", "type": 2, "id": "5680", "date": "2018-03-14T12:17:34.923", "score": 5, "comment_count": 0, "parent_id": "5265"}, "5272": {"line": 3325, "body": "As discussed here:\nhttps://www.researchgate.net/post/What_should_be_my_activation_function_for_last_layer_of_neural_network \nLinear is the preferred activation function.\nBut then linear activation function is equivalent to no activation function at all:\nhttps://datascience.stackexchange.com/questions/13696/lack-of-activation-function-in-output-layer-at-regression\n", "type": 2, "id": "5272", "date": "2018-02-11T01:12:42.457", "score": -1, "comment_count": 0, "parent_id": "5265"}}}
{"line": 1882, "body": "Obviously, finding suitable hyper-parameters for a neural network is a complex task and problem or domain-specific. However, there should be at least some \"rules\" that hold most times for the size of the filter (or kernel)!\nIn most cases, intuition should be to go for small filters for detecting high-frequency features and large kernels for low-frequency features, right? For example, $3 \\times 3$ kernel filters for edge detection, color contrast, etc., and maybe $11 \\times 11$ for detecting full objects, when the objects occupy an area of roughly $11 \\times 11$ pixels.\nIs this \"intuition\" more or less generally true? How can we decide which kernel's sizes should be chosen for a specific problem - or even for one specific convolutional layer?\n", "type": 1, "id": "3321", "date": "2017-05-16T10:47:40.513", "score": 8, "comment_count": 1, "tags": ["convolutional-neural-networks", "image-recognition", "hyperparameter-optimization", "hyper-parameters", "filters"], "title": "How do we choose the kernel size depending on the problem?", "answer_count": 2, "views": 6093, "accepted_answer": null, "answers": {"5707": {"line": 3658, "body": "Take a look at this article. It give tools to actually understand what your filters have learn and show what you can do next to optimize your hyper-parameters. Also check more recent articles that seek to provide interpretations of what NN learn. \n", "type": 2, "id": "5707", "date": "2018-03-16T12:49:43.390", "score": 2, "comment_count": 0, "parent_id": "3321"}, "8378": {"line": 5640, "body": "One key to the answer is in the question, \"Even for one specific conv layer.\"  It is not a good idea to build deep convolution networks on the assumption that a single kernel size most aptly applies to all layers.  When perusing the configurations that proved successful in publications, it becomes apparent that configurations that varying through their layers are more commonly found to be optimal.\nThe other key is to understand that two layers of 11x11 kernels have a 21x21 reach, and ten layers of 5x5 kernels have a 41x41 reach.  A mapping from one level of abstraction to the next need not be completed in one layer.\nGeneralities regarding kernel sizes exist, but they are functions of the typical input characteristics, the desired output of the network, the computing resources available, resolution, size of the data set, and whether they are still images or movies.\nRegarding input characteristics, consider this case: The images are shot with a large depth of field under poor lighting conditions, such as in security scenarios, so the aperture of the lens is wide open, causing objects at some ranges of distance to be out of focus, or there can be motion blur.\nUnder such conditions a single 3x3 kernel will not detect many edges.  If the edge may span five pixels, the choice exists as to how many layers are dedicated to its detection.  What factors affect that choice is based on what other characteristics exist in the input data.\nExpect that as acceleration hardware develops (in VLSI chips dedicated to this purpose) that the computing resource constraints will decrease in priority as a factor in kernel size selection.  Currently, the computation time is significant and forces the decision about how to balance layer count and layer size to be mostly a matter of cost.\nThis question begs another question.  Can an oversight machine learner learn how to automatically balance the configuration of deep convolution networks?  It could then be re-executed whenever additional computing resources are provisioned.  It would be surprising if there weren't at least a dozen labs working on exactly this capability.\n", "type": 2, "id": "8378", "date": "2018-10-13T09:53:58.070", "score": 0, "comment_count": 0, "parent_id": "3321"}}}
{"line": 2408, "body": "Below is an excerpt in an instructor's manual on ML that is explaining deep neural networks, using cat recognition (what else!) from images as example. On how DL performs this feat, the excerpt said that,\n\nAssume that the first layer returns the number of pixels that are\n  brown/black/blue/red, and the second layer finds the most common\n  color, and the third layer returns \"cat\" if previous layer had\n  supplied \"brown\". [..] Mathematically, this model would be, for the\n  first layer, [ sum(r = 255, g=255, b=255), ..., ..., sum(r=255, g=0,\n  b=0)] -- this is just a set of appropriately positioned relu functions\n  (okay, for r=234, we'd need two relu functions, so two layers, but you\n  get the idea). The second layer would be a softmax layer. The third\n  layer is simply an identity!\n\nNow I worked with deep nets, but I am not sure how I can structure a DL to do this. ReLu is simply a max(0,x), so how would I filter out pixel vals for example 128,128,128 and sum them up? Wouldn't the convolution layer play a role here too? What would the layout of a simple deep net be that does what is described above?\nThanks,\n", "type": 1, "id": "4061", "date": "2017-09-19T13:06:22.270", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks"], "title": "ReLu, Sum and Convolution Layers to Count Pixels of Certain Color", "answer_count": 1, "views": 247, "accepted_answer": "4224", "answers": {"4224": {"line": 2543, "body": "I can judge from the excerpt only, but I don't see any mention of a convolutional layer there. This doesn't mean that convolution isn't suitable for this task (in fact, it's the best method for image classification so far), it's just seems the instructor proposes this particular network architecture, without convolutions.\nSince this architecture is more illustrative and by no means a standard, I can't be sure I fully understand it. But it seems like the first layer outputs a vector of size 4, which contains the counts of four selected colors. The ReLu operation here is a fancy way of filtering extra colors, based on channel value (note that they are close to either 0 or 255 in all four colors). If you select different four colors, this filtering will have to be done differently too, so it's basically hardwired. The second layer is more straightforward.\nIn general, you shouldn't consider this a \"true\" deep network, but merely a puzzle to get familiar with ReLu operation.\n", "type": 2, "id": "4224", "date": "2017-10-08T08:12:04.967", "score": 0, "comment_count": 0, "parent_id": "4061"}}}
{"line": 2208, "body": "Frameworks like PyTorch and TensorFlow through TensorFlow Fold support Dynamic Computational Graphs and are receiving attention from data scientists.\nHowever, there seems to be a lack of resource to aid in understanding Dynamic Computational Graphs.\nThe advantage of Dynamic Computational Graphs appears to include the ability to adapt to a varying quantities in input data.  It seems like there may be automatic selection of the number of layers, the number of neurons in each layer, the activation function, and other NN parameters, depending on each input set instance during the training.  Is this an accurate characterization?\nWhat are the advantages of dynamic models over static models?  Is that why DCGs are receiving much attention?  In summary, what are DCGs and what are the pros and cons their use?\n", "type": 1, "id": "3801", "date": "2017-08-11T05:31:09.903", "score": 23, "comment_count": 0, "tags": ["neural-networks"], "title": "What is a Dynamic Computational Graph?", "answer_count": 4, "views": 13022, "accepted_answer": "3803", "answers": {"3871": {"line": 2260, "body": "Many deep neural networks have a static data-flow graph, which roughly means that the structure of its computation (its computation graph) remains stable over different inputs. This is good since we can leverage this feature for performance, such as by mini-batching (processing a bunch of inputs at once).\nBut some neural networks could have a different computation graph for each input. This causes some problems (batching difficulties, graph construction is computationally expensive), and hence these networks are a bit hard to use.\nThe paper you link overcomes this problem by proposing a method that can batch several computation graphs into one. Then, we can do our usual NN techniques. \nBenefits are speedups, which incentivizes researchers to explore different structures and be more creative I guess.\n\nThe advantage of Dynamic Computational Graphs appears to include the ability to adapt to a varying quantities in input data. It seems like there may be automatic selection of the number of layers, the number of neurons in each layer, the activation function, and other NN parameters, depending on each input set instance during the training. Is this an accurate characterization?\n\nThis is incorrect.\n", "type": 2, "id": "3871", "date": "2017-08-23T14:38:08.203", "score": 0, "comment_count": 0, "parent_id": "3801"}, "3803": {"line": 2210, "body": "Two Short Answers\nThe short answer from a theoretical perspective is that ...\n\nA Dynamic Computational Graph is a mutable system represented as a directed graph of data flow between operations.  It can be visualized as shapes containing text connected by arrows, whereby the vertices (shapes) represent operations on the data flowing along the edges (arrows).\n\nNote that such a graph defines dependencies in the data flow but not necessarily the temporal order of the application of operations, which can become ambiguous in the retention of state in vertices or cycles in the graph without an additional mechanism to specify temporal precedence.\nThe short answer from an applications development perspective is that ...\n\nA Dynamic Computational Graph framework is a system of libraries, interfaces, and components that provide a flexible, programmatic, run time interface that facilitates the construction and modification of systems by connecting a finite but perhaps extensible set of operations.\n\nThe PyTorch Framework\nPyTorch is the integration of the Torch framework with the Python language and data structuring. Torch competes with Theano, TensorFlow, and other dynamic computational system construction frameworks.\n\n\n------   Additional Approaches to Understanding   ------\n\nArbitrary Computational Structures of Arbitrary Discrete Tensors\nOne of the components that can be used to construct a computational system is an element designed to be interconnected to create neural networks.  The availability of these supports the construction of deep learning and back-propagating neural networks.  A wide variety of other systems involving the assembly of components that work with potentially multidimensional data in arbitrarily defined computational structures can also be constructed.\nThe data can be scalar values, such as floating-point numbers, integers, or strings, or orthogonal aggregations of these, such as vectors, matrices, cubes, or hyper-cubes.  The operations on the generalization of these data forms are discrete tensors and the structures created from the assembly of tensor operations into working systems are data flows.\nPoints of Reference for Understanding the Dynamic Computation Concept\nDynamic Computational Graphs are not a particularly new concept, even though the term is relatively new.  The interest in DCGs among computer scientists is not as new as the term Data Scientist.  Nonetheless, the question correctly states that there are few well-written resources available (other than code examples) from which one can learn the overall concept surrounding their emergence and use.\nOne possible point of reference for beginning to understand DCGs is the Command design pattern which is one of the many design patterns popularized by the proponents of object-oriented design.  The Command design pattern considers operations as computation units the details of which are hidden from the command objects that trigger them.  The Command design pattern is often used in conjunction with the Interpreter design pattern.\nIn the case of DCGs, the Composite and Facade design patterns are also involved to facilitate the definition of plug-and-play discrete tensor operations that can be assembled together in patterns to form systems.\nThis particular combination of design patterns to form systems is actually a software abstraction that largely resembles the radical idea that led to the emergence of the Von Neumann architecture, central to most computers today.  Von Neumann's contribution to the emergence of the computer is the idea of permitting arbitrary algorithms containing Boolean logic, arithmetic, and branching to be represented and stored as data -- a program.\nAnother forerunner of DCGs is expression engines.  Expression engines can be as simple as arithmetic engines and as complex as applications such as Mathematica.  A rules engine is a little like DCGs except that rules engines are declarative and meta-rules for rules engines operate on those declarations.\nPrograms Manipulating Programs\nWhat these have in common with DCGs is that the flow of data and operations to be applied can be defined at run time.  As with DCGs, some of these software libraries and applications have APIs or other mechanisms to permit operations to be applied to functional details.  It is essentially the idea of a program permitting the manipulation of another program.\nAnother reference point for understanding this principle at a primitive level is the switch-case statement available in some computer languages.  It is a source code structure whereby the programmer essentially expresses, \"We're not sure what must be done, but the value of this variable will tell the real-time execution model what to do from a set of possibilities.\"\nThe switch-case statement is an abstraction that extends the idea of deferring the decision as to the direction of computation until run time.  It is the software version of what is done inside the control unit of a contemporary CPU and an extension of the concept of deferring some algorithm details.  A table of functors (function pointers) in C or polymorphism in C++, Java, or Python are other primitive examples.\nDynamic Computation takes the abstraction further.  They defer most if not all of the specification of computations and the relationships between them to run time.   This comprehensive generalization broadens the possibilities of functional modification at run time.\nDirected Graph Representation of Computation\nThat's what the Dynamic Computational model is.  Now for the Graph part.\nOnce one decides to defer the choice of operations to be performed until run time, a structure is required to hold the operations, their dependency relationships, and perhaps mapping parameters.  Such a representation is more than a syntactic tree (such as a tree representing the hierarchy of source code).  Unlike an assembly language program or machine code, it must be easily and arbitrarily mutable.  It must contain more information than a data flow graph and much more than a memory map.  What must that data structure that specifies the computational structure look like?\nFortunately, any arbitrary, finite, bounded algorithm can be represented as a directed graph of dependencies between specified operations.  In such a graph, the vertices (often represented as nodes of various shapes when displayed) represent operations performed on the data and the edges (often represented as arrows when displayed) are digital representations of information originating resulting from some operation (or system input) and upon which other operations (or system output) depend.\nKeep in mind that the directed graph is neither an algorithm (in that a precise sequence of operations is specified) nor a declaration (in that data can be explicitly stored and loops, branches, functions, and modules may be definable and nested).\nMost of these Dynamic Computational Graph frameworks and libraries permit the components to do computations on the component input that support machine learning.  Vertices in the directed graph can be simulations of neurons for the construction of a neural net or components that support differential calculus.  These frameworks present possibilities of constructs that can be used for deep learning in a more generalized sense.\nIn the Context of Computer History\nAgain, nothing mentioned thus far is new to computer science.  LISP permits computational schematics to be modified by other algorithms.  And generalized input dimensionality and numerocity is built into a number of longstanding plug-and-play interfaces and protocols.  The idea of a framework for learning dates back to the same mid-Twentieth Century period too.\nWhat is new and gaining in popularity is a particular combination of integrated features and the associated set of terminology, an aggregation of existing terminology for each of the features, leading to a wider base for comprehension by those already studying for and working in the software industry.\n\nContemporary (trendy) flavor of API interfaces\nObject orientation\nDiscrete tensor support\nThe directed graph abstraction\nInteroperability with popular languages and packages that support big data, data mining, machine learning, and statistical analysis\nSupport for arbitrary and systematic neural network construction\nThe possibility of dynamic neural network structural adaptation (which facilitates experimentation on neural plasticity)\n\nMany of these frameworks support adaptability to changing input dimensionality (number of dimensions and the range of each).\nSimilarity to Abstract Symbol Trees in Compilers\nA dependency graph of inputs and outputs of operations also appears within abstract symbol trees (AST), which some of the more progressive compilers construct during the interpretation of the source code structure.  The AST is then used to generate assembler instructions or machine instructions in the process of linking with libraries and forming an executable.  The AST is a directed graph that represents the structure of data, operations performed, and the control flow specified by the source code.\nThe data flow is simply the set of dependencies between operations, which must be inherent in the AST for the AST to be used to create execution instructions in assembler or machine code that precisely follows the algorithm specified in the source code.\nDynamic Computational Graph frameworks, unlike switch-case statements or AST models in compilers, can be manipulated in real-time, optimized, tuned (as in the case of plastic artificial nets), inverted, transformed by tensors, decimated, modified to add or remove entropy, mutated according to a set of rules, or otherwise translated into derivative forms.  They can be stored as files or streams and then retrieved from them.\nThis is a trivial concept for LISP programmers or those that understand the nature of John von Neumann's recommendation to store operational specifications as data.  In this later sense, a program is a data stream to instruct, through a compiler and operating system, a dynamic computational system implemented in VLSI digital circuitry.\nAchieving Adaptable Dimensionality and Numerocity\nIn the question is the comment that one doesn't, \"Need to have data set -- that all the instances within it have the same, a fixed number of inputs.\" That statement does not promote accurate comprehension.  There are clearer ways to say what is true about input adaptability.\nThe interface between a DCG and other components of an overall system must be defined, but these interfaces may have dynamic dimensionality or numerocity built into them.  It is a matter of abstraction.\nFor instance, a discrete tensor object type presents a specific software interface, yet a tensor is a dynamic mathematical concept around which a common interface can be used.  A discrete tensor may be a scalar, a vector, a matrix, a cube, or a hypercube, and the range of dependent variables for each dimension may be variable.\nIt can be the case that the number of nodes in a layer of the system defined in a Dynamic Computational Graph can be a function of the number of inputs of a particular type, and that too can be a computation deferred to run time.\nThe framework may be programmed to select layer structure (an extension of the switch-case paradigm again) or calculate parameters defining the structure sizes and depth or activation.  However, these sophisticated features are not what qualifies the framework as a Dynamic Computational Graph framework.\nWhat Qualifies a Framework to Support Dynamic Computational Graphs?\nTo qualify as a Dynamic Computational Graph framework, the framework must merely support the deferring of the determination of algorithm to run time, therefore opening the door to a plethora of operations on the computational dependencies and data flow at run time.  The basics of the operations deferred must include the specification, manipulation, execution, and storage of the directed graphs that represent systems of operations.\nIf the specification of the algorithm is NOT deferred until run time but is compiled into the executable designed for a specific operating system with only the traditional flexibility provided by low-level languages such as if-then-else, switch-case, polymorphism, arrays of functors, and variable-length strings, it is considered a static algorithm.\nIf the operations, the dependencies between them, the data flow, the dimensionality of the data within the flow, and the adaptability of the system to the input numerocity and dimensionality are all variable at run time in a way to create a highly adaptive system, then the algorithm is dynamic in these ways.\nAgain, LISP programs that operate on LISP programs, rules engines with meta-rule capabilities, expression engines, discrete tensor object libraries, and even relatively simple Command design patterns are all dynamic in some sense, deferring some characteristics to run time.  DCGs are flexible and comprehensive in their capabilities to support arbitrary computational constructs in such a way to create a rich environment for deep learning experimentation and systems implementation.\nWhen to Use Dynamic Computational Graphs\nThe pros and cons of DCGs are entirely problem-specific.  If you investigate the various dynamic programming concepts above and others that may be closely tied to them in the associated literature, it will become obvious whether you need a Dynamic Computational Graph or not.\nIn general, if you need to represent an arbitrary and changing model of computation to facilitate the implementation of the deep learning system, mathematical manipulation system, adaptive system, or another flexible and complex software construct that maps to the DCG paradigm well, then a proof of concept using a Dynamic Computational Graph framework is a good first step in defining your software architecture for the problem's solution.\nNot all learning software uses DCG's, but they are often a good choice when the systematic and possibly continuous manipulation of an arbitrary computational structure is a run time requirement.\n", "type": 2, "id": "3803", "date": "2017-08-12T00:23:11.337", "score": 10, "comment_count": 2, "parent_id": "3801"}, "7759": {"line": 5196, "body": "Dynamic Computational Graphs are simply modified CGs with a higher level of abstraction. The word 'Dynamic' explains it all: how data flows through the graph depends on the input structure,i.e the DCG structure is mutable and not static. One of its important applications is in NLP neural networks.\n", "type": 2, "id": "7759", "date": "2018-08-30T08:46:46.173", "score": 0, "comment_count": 0, "parent_id": "3801"}, "7780": {"line": 5213, "body": "In short, dynamic computation graphs can solve some problems that static ones cannot, or are inefficient due to not allowing training in batches.\nTo be more specific, modern neural network training is usually done in batches, i.e. processing more than one data instance at a time. Some researchers choose batch size like 32, 128 while others use batch size larger than 10,000. Single-instance training is usually very slow because it cannot benefit from hardware parallelism. \nFor example, in Natural Language Processing, researchers want to train neural networks with sentences of different lengths. Using static computation graphs, they would usually have to first do padding, i.e. adding meaningless symbols to the beginning or end of shorter sentences to make all sentences of the same length. This operation complicates the training a lot (e.g. need masking, re-define evaluation metrics, waste a significant amount of computation time on those padded symbols).  With a dynamic computation graph, padding is no longer needed (or only needed within each batch).\nA more complicated example would be to (use neural network to) process the sentences based on its parsing trees. Since each sentence has its own parsing tree, they each requires a different computation graph, which means training with a static computation graph can only allow single-instance training. An example similar to this is the Recursive Neural Networks.\n", "type": 2, "id": "7780", "date": "2018-09-01T04:19:58.553", "score": 2, "comment_count": 0, "parent_id": "3801"}}}
{"line": 2893, "body": "I'm testing various learning rates and neural network configurations. I'm testing over 10000 games, with the first 2000 having random starting moves and general randomness throughout of about 20%, i.e. 20% of moves are random.\nIn all configurations, I initiate the weights to random values.\nWhat I've found is that in all configurations, Player 1 will win the majority of games, or Player 2 will. There's no 50/50 split.\nIs this expected or normal? I am training on the Connect Four game.\n", "type": 1, "id": "4703", "date": "2017-12-10T00:35:09.153", "score": 2, "comment_count": 8, "tags": ["neural-networks", "reinforcement-learning"], "title": "Is it expected that during self-play reinforcement learning that player 1 or player 2 wins the majority of games?", "answer_count": 1, "views": 226, "accepted_answer": null, "answers": {"13063": {"line": 8958, "body": "From wikipedia article on  Connect Four:\n\nConnect Four is a solved game. The first player can always win by\n  playing the right moves.\n\nIt's pretty reasonable that even without training 1st player win more often by small margin.\nWith some training  1st player should mostly win.\n", "type": 2, "id": "13063", "date": "2019-06-26T07:00:56.820", "score": -2, "comment_count": 0, "parent_id": "4703"}}}
{"line": 4055, "body": "I'm trying to write my own implementation of NEAT and I'm stuck on the network evaluate function, which calculates the output of the network.\nNEAT as you may know contains a group of neural networks with continuously evolving topologies by the addition of new nodes and new connections. But with the addition of new connections between previously unconnected nodes, I see a problem that will occur when I go to evaluate, let me explain with an example:\n\nINPUTS = 2 yellow nodes\nHIDDEN = 3 blue nodes\nOUTPUT = 1 red node\n\nIn the image a new connection has been added connecting node3 to node5, how can I calculate the output for node5 if I have not yet calculated the output for node3, which depends on the output from node5?\n(not considering activation functions)\nnode5 output =  (1 * 0.5) + (1 * 0.2) + (node3 output * 0.8)\nnode3 output =  ((node5 output * 0.7) * 0.4)\n\n", "type": 1, "id": "6231", "date": "2018-05-01T18:10:40.283", "score": 6, "comment_count": 0, "tags": ["neural-networks", "genetic-algorithms", "evolutionary-algorithms", "neat"], "title": "How to evaluate a NEAT neural network?", "answer_count": 6, "views": 2528, "accepted_answer": "6233", "answers": {"6233": {"line": 4057, "body": "Consider the execution order, 5 will have an invalid value because it hasn't been set form 3 yet. However the second time around it should have a value set. The invalid value should falloff after sufficient training.\n0 -> 5\n1 -> 5\n5 -> 2\n2 -> 3\n3 -> 4\n3 -> 5\nRESTART\n0 -> 5\n1 -> 5\n\n", "type": 2, "id": "6233", "date": "2018-05-01T19:28:32.923", "score": 4, "comment_count": 7, "parent_id": "6231"}, "11177": {"line": 7510, "body": "I can think of two possible ways of enforcing NEAT to create a feed forward network. One elegant one and one a little more cumbersome one;\n\nOnly allow the \"add connection\" mutation to connect a node with another node that have a higher maximum distance from an input node. This should result in feed forward network, without much extra work. (Emergent properties are great!)\nRun as you did and create a fully connected network with NEAT and then prune it during a forward pass. After creating the network, run through it and remove connections that try to connect to a node already used in the forward pass (example 3->5). Alternatively just remove unused input connections to nodes during the forward pass. Given how NEAT mutates, it should not be possible that you remove a vital connection and cut the the network in two. This property of NEAT make sure your signal will always be able to reach the output, even if you remove those \"backwards pointing\" connections. \n\nI believe these should work, however i have not tested them.\nThe original NEAT paper assumed a feed forward ANN, even though its implementation as described would result in a fully connected network. I think it was just an assumption of the paradigm they worked in. The confusion is fully understandable.\n", "type": 2, "id": "11177", "date": "2019-03-12T17:00:35.483", "score": 3, "comment_count": 0, "parent_id": "6231"}, "13853": {"line": 9599, "body": "In my implementation, I used a recursion system to calculate the output nodes. It works as follows:\n\nAssume a feed-forward network\n\n\nOnly allow the \"add connection\" mutation to connect a node with another node >that have a higher maximum distance from an input node. This should result in >feed forward network, without much extra work. (Emergent properties are great!)\n\n\nDefine function x, a recursive function that takes in a node number\nDefine function y, a second function that takes in a node and returns all the connections with that node as an output\n\nIn the recursive function:\n\nCall function y\nCall function x on function y outputs\nIf the parameter for x is any input node, return the node value.\n\nThis was the most elegant way of implementing I could think of, and its a lot simpler than explicitly tracking all of the connections.\n", "type": 2, "id": "13853", "date": "2019-08-08T01:49:02.833", "score": 2, "comment_count": 0, "parent_id": "6231"}, "13863": {"line": 9606, "body": "Hello chris i am also implementing this algorithm from scratch and the way i go about activating my mlp net is as follows:\nI instantiate a list of nodes(actives), this is set to all input nodes initially, i then pass that to a function that initializes an empty list(next actives) and proceeds to loop through each set of conns for each node in the actives list, it adds each \"to\" node from those connections to the \"next actives) list unless its an output node or has already been activated, once all the \"actives\" list is looped through, i call the function again this time passing \"next actives\" as the actives list unless \"next actives\" is still empty after then i know the net has been fully activated. \nin this scenario the connection from node three to node five would be evaluated but it would not be added to the next actives list because it had already been activated, preventing an infinite loop.\n", "type": 2, "id": "13863", "date": "2019-08-08T18:40:01.940", "score": 2, "comment_count": 0, "parent_id": "6231"}, "17121": {"line": 11447, "body": "Following is the pseudo code of the NEAT's network evaluation (converted from original source code), \nUntil all the outputs are active\n    for all non-sensor nodes\n        activate node\n        sum the input\n    for all non-sensor and active nodes\n        calculate the output\n\nNote that there is no recursion for feed forwarding concepts according to the original author. \n", "type": 2, "id": "17121", "date": "2019-12-16T07:56:05.977", "score": 3, "comment_count": 0, "parent_id": "6231"}, "25143": {"line": 16979, "body": "Okay, so instead of telling you to just not have recurrent connections, i'm actually going to tell you how to identify them.\nFirst thing you need to know is that recurrent connections are calculated after all other connections and neurons. So which connection is recurrent and which is not depends on the order of calculation of your NN.\nAlso, the first time when you put data into the system, we'll just assume that every connection is zero, otherwise some or all neurons can't be calculated.\nLets say we have this neural network:\nNeural Network\nWe devide this network into 3 layers (even though conceptually it has 4 layers):\nInput Layer  [1, 2]\nHidden Layer [5, 6, 7]\nOutput Layer [3, 4]\n\nFirst rule: All outputs from the output layer are recurrent connections.\nSecond rule: All outputs from the input layer may be calculated first.\nWe create two arrays. One containing the order of calculation of all neurons and connections and one containing all the (potentially) recurrent connections.\nRight now these arrays look somewhat like this:\nOrder of \ncalculation: [1->5, 2->7 ]\n\nRecurrent:   [ ]\n\nNow we begin by looking at the output layer. Can we calculate Neuron 3? No? Because 6 is missing. Can we calculate 6? No? Because 5 is missing. And so on. It looks somewhat like this:\n3, 6, 5, 7\n\nThe problem is that we are now stuck in a loop. So we introduce a temporary array storing all the neuron id's that we already visited:\n[3, 6, 5, 7]\n\nNow we ask: Can we calculate 7? No, because 6 is missing. But we already visited 6...\n[3, 6, 5, 7,] <- 6\n\nThird rule is: When you visit a neuron that has already been visited before, set the connection that you followed to this neuron as a recurrent connection.\nNow your arrays look like this:\nOrder of \ncalculation: [1->5, 2->7 ]\n\nRecurrent:   [6->7 ]\n\nNow you finish the process and in the end join the order of calculation array with your recurrent array so, that the recurrent array follows after the other array.\nIt looks somethat like this:\n[1->5, 2->7, 7, 7->4, 7->5, 5, 5->6, 6, 6->3, 3, 4, 6->7]\n\nLet's assume we have [x->y, y]\nWhere x->y is the calculation of x*weight(x->y)\nAnd\nWhere y is the calculation of Sum(of inputs to y). So in this case Sum(x->y) or just x->y.\nThere are still some problems to solve here. For example: What if the only input of a neuron is a recurrent connection? But i guess you'll be able to solve this problem on your own...\n", "type": 2, "id": "25143", "date": "2020-12-12T20:41:19.403", "score": 0, "comment_count": 2, "parent_id": "6231"}}}
{"line": 2874, "body": "What is the fundamental difference between convolutional neural networks and recurrent neural networks? Where are they applied?\n", "type": 1, "id": "4683", "date": "2017-12-08T14:48:36.857", "score": 11, "comment_count": 1, "tags": ["neural-networks", "convolutional-neural-networks", "recurrent-neural-networks", "comparison"], "title": "What is the fundamental difference between CNN and RNN?", "answer_count": 5, "views": 7568, "accepted_answer": null, "answers": {"4685": {"line": 2876, "body": "Basically, a CNN saves a set of weights and applies them spatially. For example, in a layer, I could have 32 sets of weights (also called feature maps). Each set of weights is a 3x3 block, meaning I have 3x3x32=288 weights for that layer. If you gave me an input image, for each 3x3 map, I slide it across all the pixels in the image, multiplying the regions together. I repeat this for all 32 feature maps, and pass the outputs on. So, I am learning a few weights that I can apply at a lot of locations.\nFor an RNN, it is a set of weights applied temporally (through time). An input comes in, and is multiplied by the weight. The networks saves an internal state and puts out some sort of output. Then, the next piece of data comes in, and is multiplied by the weight. However, the internal state that was created from the last piece of data also comes in and is multiplied by a different weight. Those are added and the output comes from an activation applied to the sum, times another weight. The internal state is updated, and the process repeats.\nCNN's work really well for computer vision. At the low levels, you often want to find things like vertical and horizontal lines. Those kinds of things are going to be all over the images, so it makes sense to have weights that you can apply anywhere in the images.\nRNN's are really good for natural language processing. You can imagine that the next word in a sentence will be highly influenced by the ones that came before it, so it makes sense to carry that internal state forward and have a small set of weights that can apply to any input.\nHowever, there are many more applications. In addition, CNN's have performed well on NLP tasks. There are also more advanced versions of RNN's called LSTM's that you could check out.\nFor an explanation of CNN's, go to the Stanford CS231n course. Especially check out lecture 5. There are full class videos on YouTube.\nFor an explanation of RNN's, go here.\n", "type": 2, "id": "4685", "date": "2017-12-08T17:18:19.240", "score": 7, "comment_count": 1, "parent_id": "4683"}, "4687": {"line": 2877, "body": "On a basic level, an RNN is a neural network whose next state depends on its past state(s), while a CNN is a neural network that does dimensionality reduction (make large data smaller while preserving information) via convolution. See this for more info on convolutions \n", "type": 2, "id": "4687", "date": "2017-12-08T18:54:08.843", "score": 0, "comment_count": 1, "parent_id": "4683"}, "12290": {"line": 8356, "body": "Recurrent neural networks (RNNs) are artificial neural networks (ANNs) that have one or more recurrent (or cyclic) connections, as opposed to just having feed-forward connections, like a feed-forward neural network (FFNN). \nThese cyclic connections are used to keep track of temporal relations or dependencies between the elements of a sequence. Hence, RNNs are suited for sequence prediction or related tasks.\nIn the picture below, you can observe an RNN on the left (that contains only one hidden unit) that is equivalent to the RNN on the right, which is its \"unfolded\" version. For example, we can observe that $\\bf h_1$ (the hidden unit at time step $t=1$) receives both an input $\\bf x_1$ and the value of the hidden unit at the previous time step, that is, $\\bf h_0$. \n\nThe cyclic connections (or the weights of the cyclic edges), like the feed-forward connections, are learned using an optimisation algorithm (like gradient descent) often combined with back-propagation (which is used to compute the gradient of the loss function).\nConvolutional neural networks (CNNs) are ANNs that perform one or more convolution (or cross-correlation) operations (often followed by a down-sampling operation). \nThe convolution is an operation that takes two functions, $\\bf f$ and $\\bf h$, as input and produces a third function, $\\bf g = f \\circledast h$, where the symbol $\\circledast$ denotes the convolution operation. In the context of CNNs, the input function $\\bf f$ can e.g. be an image (which can be thought of as a function from 2D coordinates to RGB or grayscale values). The other function $\\bf h$ is called the \"kernel\" (or filter), which can be thought of as (small and square) matrix (which contains the output of the function $\\bf \n h$). $\\bf f$ can also be thought of as a (big) matrix (which contains, for each cell, e.g. its grayscale value).\nIn the context of CNNs, the convolution operation can be thought of as dot product between the kernel $\\bf h$ (a matrix) and several parts of the input (a matrix).\nIn the picture below, we perform an element-wise multiplication between the kernel $\\bf h$ and part of the input $\\bf h$, then we sum the elements of the resulting matrix, and that is the value of the convolution operation for that specific part of the input. \n\nTo be more concrete, in the picture above, we are performing the following operation\n\\begin{align}\n\\sum_{ij}\n\\left(\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 1\n\\end{bmatrix}\n\\otimes\n\\begin{bmatrix}\n1 & 0 & 1\\\\\n0 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix}\n\\right)\n=\n\\sum_{ij}\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix}\n= 4\n\\end{align}\nwhere $\\otimes$ is the element-wise multiplication and the summation $\\sum_{ij}$ is over all rows $i$ and columns $j$ (of the matrices).\nTo compute all elements of $\\bf g$, we can think of the kernel $\\bf h$ as being slided over the matrix $\\bf f$.\nIn general, the kernel function $\\bf h$ can be fixed. However, in the context of CNNs, the kernel $\\bf h$ represents the learnable parameters of the CNN: in other words, during the training procedure (using e.g. gradient descent and back-propagation), this kernel $\\bf h$ (which thus can be thought of as a matrix of weights) changes. \nIn the context of CNNs, there is often more than one kernel: in other words, it is often the case that a sequence of kernels $\\bf h_1, h_2, \\dots, h_k$ is applied to $\\bf f$ to produce a sequence of convolutions $\\bf g_1, g_2, \\dots, g_k$. Each kernel $\\bf h_i$ is used to \"detect different features of the input\", so these kernels are different from each other.\nA down-sampling operation is an operation that reduces the input size while attempting to maintain as much information as possible. For example, if the input size is a $2 \\times 2$ matrix $\\bf f = \\begin{bmatrix} 1 & 2 \\\\ 3 & 0 \\end{bmatrix}$, a common down-sampling operation is called the max-pooling, which, in the case of $\\bf f$, returns $3$ (the maximum element of $\\bf f$).\nCNNs are particularly suited to deal with high-dimensional inputs (e.g. images), because, compared to FFNNs, they use a smaller number of learnable parameters (which, in the context of CNNs, are the kernels). So, they are often used to e.g. classify images.\nWhat is the fundamental difference between RNNs and CNNs? RNNs have recurrent connections while CNNs do not necessarily have them. The fundamental operation of a CNN is the convolution operation, which is not present in a standard RNN.\n", "type": 2, "id": "12290", "date": "2019-05-13T21:21:24.660", "score": 7, "comment_count": 0, "parent_id": "4683"}, "16142": {"line": 10612, "body": "In the case of applying both to natural language, CNN's are good at extracting local and position-invariant features but it does not capture long range semantic dependencies. It just consider local key-phrasses. \nSo when the result is determined by the entire sentence or a long-range semantic dependency CNN is not effective as shown in this paper where the authors compared both architechrures on NLP takss.\nThis can be extended for general case. \n", "type": 2, "id": "16142", "date": "2019-10-29T12:19:52.823", "score": 0, "comment_count": 0, "parent_id": "4683"}, "12702": {"line": 8689, "body": "CNN vs RNN \n\nA CNN will learn to recognize patterns across space while RNN is useful for solving temporal data problems.\nCNNs have become the go-to method for solving any image data challenge while RNN is used for ideal for text and speech analysis.\nIn a very general way, a CNN will learn to recognize components of an image (e.g., lines, curves, etc.) and then learn to combine these components to recognize larger structures (e.g., faces, objects, etc.) while an RNN will similarly learn to recognize patterns across time. So a RNN that is trained to convert speech to text should learn first the low level features like characters, then higher level features like phonemes and then word detection in audio clip.\n\n\nCNN \nA  convolutional network (ConvNet) is made up of layers.\nIn a convolutional network (ConvNet), there are basically three types of layers:\n\n\nConvolution layer\nPooling layer\nFully connected layer\n\nOf these, the convolution layer applies convolution operation on the input 3D tensor. Different filters extract different kinds of features from an image. The below GIF illustrates this point really well:\n\nHere the filter is the green 3x3 matrix while the image is the blue 7x7 matrix.\nMany such layers passes through filters in CNN to give an output layer that can again be a NN Fully connected layer or a 3D tensor.\n\nFor example, in the above example, the input image passes through convolutional layer, then pooling layer, then convolutional layer, pooling layer, then the 3D tensor is flattened like a Neural Network 1D layer, then passed to a fully connected layer and finally a softmax layer. This makes a CNN.\n\nRNN \nRecurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. \n\nHere, $x_{t-1}$, , $x_{t}$ and $x_{t+1}$  are the values of inputs data that occur at a specific time steps and are fed into the RNN that goes through the hidden layers namely $h_{t-1}$, , $h_{t}$ and $h_{t+1}$ which further produces output $o_{t-1}$, , $o_{t}$ and $o_{t+1}$ respectively. \n", "type": 2, "id": "12702", "date": "2019-06-05T14:08:11.323", "score": 3, "comment_count": 0, "parent_id": "4683"}}}
{"line": 3698, "body": "I'm new to the world of machine learning. My question is how can I determine the size of the biases in a neural network (with backpropagation algorithm)? Currently, I have a 2-layer neural network (1 hidden and 1 output layer). Here's the code:\nimport numpy as np\nfrom matplotlib import pyplot as plt \n\nsigmoid = lambda x : 1 / (1 + np.exp(-x))\ndsigmoid = lambda y: y * (1 - sigmoid(y))\n\n# This function performs the given function (func) to the whole numpy array\ndef mapFunc(array, func) :\n    newArray = array.copy()\n    for element in np.nditer(newArray, op_flags=['readwrite']) :\n        element[...] = func(element)\n    return newArray\n\nclass NeuralNetwork :\n\ndef __init__(self, input_nodes, hidden_nodes, output_nodes) :\n    self.input_nodes = input_nodes\n    self.hidden_nodes = hidden_nodes\n    self.output_nodes = output_nodes\n\n    self.W_ih = np.random.rand(hidden_nodes, input_nodes)\n    self.W_ho = np.random.rand(output_nodes, hidden_nodes)\n\n    self.B_ih = np.random.rand(hidden_nodes, 1)\n    self.B_ho = np.random.rand(output_nodes, 1)\n\n    self.learningRate = 0.1\n\ndef predict(self, inputs) :\n    # Calculate hidden's output\n    H_output = np.dot(self.W_ih, inputs)\n    H_output += self.B_ih\n    H_output = mapFunc(H_output, sigmoid) # Activation\n\n    # Calculate output's output\n    O_output = np.dot(self.W_ho, H_output)\n    O_output += self.B_ho\n    O_output = mapFunc(O_output, sigmoid) # Activation\n\n    return O_output\n\ndef train(self, inputs, target) :\n    # Calculate hidden's output\n    H_output = np.dot(self.W_ih, inputs)\n    H_output += self.B_ih\n    H_output = mapFunc(H_output, sigmoid) # Activation\n\n    # Calculate output's output\n    O_output = np.dot(self.W_ho, H_output)\n    O_output += self.B_ho\n    O_output = mapFunc(O_output, sigmoid) # Activation\n\n    # Calculate output error :\n    O_error = O_output - target\n\n    # Calculate output delta\n    O_gradient = mapFunc(O_output, dsigmoid)\n    O_gradient = np.dot(O_gradient, np.transpose(O_error)) * self.learningRate\n\n    W_ho_delta = np.dot(O_gradient, np.transpose(H_output))\n\n    self.W_ho -= W_ho_delta\n    self.B_ho -= O_gradient\n\n    # Calculate hidden error :\n    W_ho_t = np.transpose(self.W_ho)\n    H_error = np.dot(W_ho_t, O_error)\n\n    # Calculate hidden delta :\n    H_gradient = mapFunc(H_output, dsigmoid)\n    H_gradient = np.dot(H_gradient, np.transpose(H_error)) * self.learningRate\n\n    W_ih_delta = np.dot(H_gradient, inputs)\n\n    self.W_ih -= W_ih_delta\n    self.B_ih += H_gradient\n\n    return O_output\n\n\nn = NeuralNetwork(2, 2, 1)\n\ninputs = np.matrix([[1], [0], [1], [1], [0], [1], [0], [0]])\n\ninput_list = []\ninput_list.append([[1], [0]])\ninput_list.append([[0], [1]])\ninput_list.append([[1], [1]])\ninput_list.append([[0], [0]])\n\ntarget = np.matrix([[0], [0], [1], [1]])\n\noutputs = []\nfor i in range(50000) :\n    ind = np.random.randint(len(input_list))\n    inp = input_list[ind]\n    out = n.train(inp, target[ind]).tolist()\n    outputs.append(out[0][0])\n\nprint outputs\nplt.plot(outputs)\n\nplt.show()\n\nnewInput = [[1], [1]]\nprint (n.predict(newInput))\n\nIn the train function, the line self.B_ih += H_gradient throws me an error about their sizes not being equal. I even tried to make the biases only a single number but that didn't help as it gets changed by H_gradient to a matrix. So, is there something wrong in the bias itself or I did some other step(s) wrong?\n", "type": 1, "id": "5757", "date": "2018-03-20T17:21:26.060", "score": 2, "comment_count": 0, "tags": ["neural-networks", "backpropagation"], "title": "How to determine the size of biases?", "answer_count": 1, "views": 149, "accepted_answer": null, "answers": {"5766": {"line": 3702, "body": "The biases appear in the activation function and can be seen as node feature. Suppose you have act = sigmoid, X your input layer and W your first hidden layer weight matrix.\nThen the output matrix of this hidden layer will be act(X.dot(W)+b). \nSo your bias should be a vector containing a random value for each node of the current hidden layer. \nIt will shift randomly the weight and so the activation function. See here (usually it prevent from overfitting).\nI didn't read the whole code so I don't know if the problem comes from here, but why don't you learn how to use tensorflow instead of re-write everything ? There are a lot of tutorials in the net, like MNIST.\n", "type": 2, "id": "5766", "date": "2018-03-21T16:56:19.723", "score": 0, "comment_count": 2, "parent_id": "5757"}}}
{"line": 3808, "body": "In a nutshell: I want to understand why a one hidden layer neural network converges to a good minimum more reliably when a larger number of hidden neurons is used. Below a more detailed explanation of my experiment:\nI am working on a simple 2D XOR-like classification example to understand the effects of neural network initialization better. Here's a visualisation of the data and the desired decision boundary:\n\nEach blob consists of 5000 data points. The minimal complexity neural network to solve this problem is a one-hidden layer network with 2 hidden neurons. Since this architecture has the minimum number of parameters possible to solve this problem (with a NN) I would naively expect that this is also the easiest to optimise. However, this is not the case. \nI found that with random initialization this architecture converges around half of the time, where convergence depends on the signs of the weights. Specifically, I observed the following behaviour:\nw1 = [[1,-1],[-1,1]], w2 = [1,1] --> converges\nw1 = [[1,1],[1,1]],   w2 = [1,-1] --> converges\nw1 = [[1,1],[1,1]],   w2 = [1,1] --> finds only linear separation\nw1 = [[1,-1],[-1,1]], w2 = [1,-1] --> finds only linear separation\n\nThis makes sense to me. In the latter two cases the optimisation gets stuck in suboptimal local minima. However, when increasing the number of hidden neurons to values greater than 2, the network develops a robustness to initialisation and starts to reliably converge for random values of w1 and w2. You can still find pathological examples, but with 4 hidden neurons the chance that one \"path way\" through the network will have non-pathological weights is larger. But happens to the rest of the network, is it just not used then? \nDoes anybody understand better where this robustness comes from or perhaps can offer some literature discussing this issue?\nSome more information: this occurs in all training settings/architecture configurations I have investigated. For instance, activations=Relu, final_activation=sigmoid, Optimizer=Adam, learning_rate=0.1, cost_function=cross_entropy, biases were used in both layers. \n", "type": 1, "id": "5904", "date": "2018-04-05T08:59:43.157", "score": 8, "comment_count": 2, "tags": ["neural-networks", "optimization"], "title": "Why does a one-layer hidden network get more robust to poor initialization with growing number of hidden neurons?", "answer_count": 2, "views": 324, "accepted_answer": "18695", "answers": {"5926": {"line": 3822, "body": "You grasped a bit of the answer.\n\nIn the latter two cases the optimisation gets stuck in suboptimal local minima. \n\nWhen you have only 2 dimensions, a local minima exists. When you have more dimensions, this minima gets harder and harder to reach, as its likelihood decreases. Intuitively, you have a lot more dimensions through which you can improve than if you only had 2 dimensions.\nThe problem still exists, even with 1000 neurons you could find a specific set of weights which was a local minimum. However, it just becomes so much less likely.\n", "type": 2, "id": "5926", "date": "2018-04-06T10:17:42.757", "score": 1, "comment_count": 6, "parent_id": "5904"}, "18695": {"line": 12783, "body": "I may have scratched the surface of a much larger problem when I asked this question. In the meantime I have read Lottery Hypothesis paper: https://arxiv.org/pdf/1803.03635.pdf \nBasically, if you overparameterise your network you are more likely to find a random initialisation that performs well: A winning ticket. The paper above shows that you can actually prune away the unneeded parts of the network after training. However, you need to overparameterise the network initially in order to increase the chance of randomly sampling a winning ticket configuration. \nI believe the case in my question above is a minimal example of this. \n", "type": 2, "id": "18695", "date": "2020-03-17T16:43:01.340", "score": 0, "comment_count": 0, "parent_id": "5904"}}}
{"line": 1980, "body": "I am working on this code for spam detection using recurrent neural networks. \nQuestion 1. I am wondering whether this field (using RNNs for email spam detection) worths more researches or it is a closed research field. \nQuestion 2. What is the oldest published paper in this field? \nQuesiton 3. What are the pros and cons of using RNNs for email spam detection over other classification methods?\n", "type": 1, "id": "3472", "date": "2017-06-10T06:36:21.650", "score": 5, "comment_count": 2, "tags": ["classification", "recurrent-neural-networks"], "title": "Spam Detection using Recurrent Neural Networks", "answer_count": 2, "views": 4300, "accepted_answer": null, "answers": {"5012": {"line": 3119, "body": "Initially, spam detection relied on simple rule based techniques to sort out spam. However following Paul Graham's famed article 'A Plan for Spam'  the Naive Bayes approach became very popular to the point that it became regarded as the baseline for dealing with spam. \nHowever following breakthroughs in deep learning, researchers have now turned their focus to neural networks to help them deal with the perenial problem of spam emails. Google recently reported that by introducing NN's to g-mail's spam filters. It took them from 99.5% to over 99.9% accuracy, suggesting that neural networks especially when used in conjunction with Bayesian classification may be effective for enhancing spam filters. You can refer to the link below to read about Google's success story https://www.wired.com/2015/07/google-says-ai-catches-99-9-percent-gmail-spam/\nDeveloping a spam filter using neural networks is basically a classification problem. You need to follow the steps below to develop such a system. (Nikhil B 2016)\n\nCollect a dataset of spam and legitimate email messages. Label these datasets. You can find email and spam datasets here http://csmining.org/index.php/spam-email-datasets-.html\nProcess these messages with feature extraction and vectorising techniques i.e. tf-idf vectorizer, word2vec, bag-of-words e.t.c. \nOnce you have vectorised the dataset succesfully, apply a supervised learning NN algorithm i.e. radial basis network, multi-layer perceptron (MLP) or backpropagation.\nTrain your labelled data-set on the neural network. Once training is complete you can use cross validation to calculate the precision of your trained model using the test dataset. \n\nSome of the advantages of using NN's for spam detection over other methods include.\n\nNeural networks have a higher accuracy of identifying spam as demonstrated by google. \nThey have a lower false positive rate compared to other methods such as rule based techniques.\nTheir main disadvantage is that they require specialised computing hardware to deploy.\n\nSome old influential papers published in the field include.  \nMachine Learning Techniques in Spam Filtering (2004) http://ats.cs.ut.ee/u/kt/hw/spam/spam.pdf\nDetecting Spam Blogs: A Machine Learning Approach (2006) https://www.aaai.org/Papers/AAAI/2006/AAAI06-212.pdf\nA review of machine learning approaches to Spam filtering (2009) https://www.sciencedirect.com/science/article/pii/S095741740900181X\n", "type": 2, "id": "5012", "date": "2018-01-15T13:03:45.860", "score": 1, "comment_count": 0, "parent_id": "3472"}, "8397": {"line": 5653, "body": "\nQuestion 1. I am wondering whether this field (using RNNs for email spam detection) worths more researches or it is a closed research field.\n\nUse of RNNs to detect spam grew out of the use of artificial networks to detect fraud in telecommunications and the financial industry as a result of the rise of attacks on long distance lines, ATMs, banks, and credit card systems in online and at data centers supporting physical points of sale.\nAlthough basic RNN design has given way to the newer LSTM and GRU approaches and its variants and extensions, artificial networks are now one of the primary fraud detection technologies. The dominance of this fraud detection strategy extends to SPAM detection, with its close ties to fraudulence. The spammers present the appearance of a relationship with their recipients that does not exist.\nThe improvement on computing designs to recognize patterns in time series data and the application of those designs for fraud detection and countermeasures and the detection and routing or deletion of of unwanted incoming information will be a stable area of research and development for the foreseeable future.\n\nQuestion 2. What is the oldest published paper in this field?\n\nThere is no oldest published paper. The first papers on RNN are given in this answer: Where can I find the original paper that introduced RNNs?, but the move from pattern based detection to artificial networks to stateful artificial networks was gradual. The earliest deployments of these networks in server side or client side solutions occurred before any papers were published on the specific topic of RNN use in spam detection.\n\nQuestion 3. What are the pros and cons of using RNNs for email spam detection over other classification methods?\n\nSpam also has a strong temporal element.  What one considers undesirable spam in one year may be considered mission critical email a few years later, and vice versa. The performance in this space includes speed, accuracy, and reliability of classification, but also adaptation to changing user classification needs.\nIt is because of these four performance characteristics in tandem that stateful networks derived from RNNs are commonly used for spam detection. The need for gated learning and forgetting at the cell level to support the variable adaptivity makes the LSTM and GRU variants common choices.\nSemantic document classification is riding on an emerging set of technologies, which are primarily artificial network designs that begin to broach the threshold of cognitive understanding of the text by storing linguistic structure in forms that allow analogy, comparison, and composition between them. Semantic algorithms that perform these operations on fuzzy associations in combination with recursive artificial networks may emerge as the dominant design as such designs are further developed.\nReferences\nDetecting Spam Blogs: A Machine Learning Approach, Pranam Kolari, 2006\nAutomated labeling of bugs and tickets using attention-based mechanisms in recurrent neural networks, Volodymyr Lyubinets et. al., 2018\nSpam Filter Through Deep Learning and Information Retrieval, Weicheng Zhang, 2018\nAn Unsupervised Neural Network Approach to Profiling the Behavior of Mobile Phone Users for Use in Fraud Detection, Peter Burge, John Shawe-Taylor, Journal of Parallel and Distributed Computing, Volume 61 Issue 7, July 2001, pp 915-925\nIntelligent junk mail detection using neural networks, Michael Vinther, June 2002\nMining for fraud, Margaret Weatherford, IEEE Intelligent Systems, 2002\nDiscovering golden nuggets: data mining in financial application, D Zhang, L Zhou, IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, Vol. 34, No. 4, November 2004\nA Comprehensive Survey of Data Mining-based Fraud Detection Research, C Phua, V Lee, K Smith, R Gayler - Arxiv preprint arXiv, 2007\n", "type": 2, "id": "8397", "date": "2018-10-14T00:35:54.287", "score": 0, "comment_count": 0, "parent_id": "3472"}}}
{"line": 2677, "body": "A question for developers of projects for pattern recognition. How best to organize the architecture of such a service?\nAt what stage do you conduct logic? (for example, for the recognition of a photo of a male blue jacket, a cascade of queries is performed: \"recognizing men\" -> \"recognizing the jacket\" -> \"recognizing the color of the jacket.\")\nDoes it make sense to implement all search options within a single neural network or is it better to create a set of individual neuronets that are confined to fairly simple tasks?\n", "type": 1, "id": "4409", "date": "2017-11-01T00:09:06.357", "score": 3, "comment_count": 0, "tags": ["neural-networks", "ai-design", "pattern-recognition"], "title": "Image recognition service architecture", "answer_count": 2, "views": 51, "accepted_answer": null, "answers": {"4410": {"line": 2678, "body": "That is one of the good example for research. Personally, I prefer to segment out all the desired outputs at once. Then, check the success rate. If you cannot hit the success rate that you desire, you can go for more specific solutions for the specific problem that you face.\nHowever, in general, the localization, segmentation, recognition are implemented in same network and are obtained all-at-once. \n", "type": 2, "id": "4410", "date": "2017-11-01T08:41:34.787", "score": 2, "comment_count": 0, "parent_id": "4409"}, "4437": {"line": 2701, "body": "I would use a single network:\nThe essence of the question is whether or not doing all the classification work at once is more efficient than running individual classifiers for each stage.\nThe recent \"You Only Look Once\" algorithm (\"YOLO\") is based on the fact that the convolutional networks can reuse a lot of the interim calculations if you combine them into one. Because of this, they are able to perform real-time object detection on images across thousands of classes.\nYou can express your hierarchical classifier with YOLO (man, jacket and jacket colour classes). Depending on your needs, you might want to model the jacket colour as a scalar output of an approximate R,G,B value for the colour rather than having named classes for the colours.\nThis everything-at-once implementation gives you runtime efficiencies for the inference step and much faster training since the classes share common abstractions in the earlier layers of the net.\nDetails, the YOLO version 2 paper, and a cool demonstration video featuring James Bond are available here: https://pjreddie.com/darknet/yolo/\nThe paper itself, \"YOLO9000\", is available on Arxiv: https://arxiv.org/abs/1612.08242\n", "type": 2, "id": "4437", "date": "2017-11-05T13:40:10.120", "score": 0, "comment_count": 0, "parent_id": "4409"}}}
{"line": 2829, "body": "A blog post called \"Text Classification using Neural Networks\" states that the derivative of the output of a sigmoid function is used to measure error rates.\nWhat is the rationale for this?\nI thought the derivative of a sigmoid function output is just the slope of the sigmoid line at a specific point. \nMeaning it's steepest when sigmoid output is 0.5 (occuring when the sigmoid function input is 0).\nWhy does a sigmoid function input of 0 imply error (if i understand correctly)?\nSource: https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6\n\nWe use a sigmoid function to normalize values and its derivative to\n  measure the error rate. Iterating and adjusting until our error rate\n  is acceptably low.\n\ndef sigmoid(x):\n    output = 1/(1+np.exp(-x))\n    return output\n\ndef sigmoid_output_to_derivative(output):\n    return output*(1-output)\n\ndef train(...)\n    ...\n    layer_2_error = y - layer_2\n    layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n    ...\n\nUPDATE\nApologies. I don't think I was clear (I've updated the title)\nI understand we don't need to use sigmoid as the activation funtion (we could use relu, tanh or softmax).\nMy question is about using the derivative to measure the error rate (full quotation from article above in yellow) -> what does the derivative of the activation function have to do with measuring/fixing the \"error rate\"?\n", "type": 1, "id": "4629", "date": "2017-11-30T20:57:09.410", "score": 5, "comment_count": 0, "tags": ["neural-networks", "training"], "title": "How does an activation function's derivative measure error rate in a neural network?", "answer_count": 5, "views": 2032, "accepted_answer": null, "answers": {"4631": {"line": 2831, "body": "Measuring the error rate of a neural network does not involve the derivative of the sigmoid function at all. It only needs the neural networks outputs, and the expected outputs. It does not matter how the neural network got to those outputs, the outputs only have to be based on the inputs. What the other of that specific text is trying to say is that the derivative of the sigmoid function is important when you use the algorithm back propagation to train the neural network. This method involves using derivatives to optimize the neural network. While this technique is more complicated with neural networks because of how many variables are involved, you can easily see the basics of this approach if you look at a calculus textbook and look at the chapter that is usually titled Applications of the derivative.\n", "type": 2, "id": "4631", "date": "2017-12-01T00:39:55.820", "score": 1, "comment_count": 0, "parent_id": "4629"}, "4642": {"line": 2839, "body": "The derivative of your loss is the \"slope\" at that given prediction. So by \"moving down along the slope\", we can reduce the value of the loss function. \nIntuitively, one can imagine a ball rolling down a slope in the direction of the tangent. Eventually the ball will roll towards the lowest point (global minima), or a small pothole (local minima), or even some weird looking shape (saddle point) if you are very unlucky.\nWith some fixed input/output pair, we can consider the loss function as a value of the parameters (weights). Here, the Z axis is the value of your loss f (what you are trying to optimize), while the X,Y axis are the parameters of the loss f (what you are allowed change):\n \nNote that the image provided is a bit misleading. In practice, we only know the value of the loss function at the location we have probed. The entire graph can only be drawn if we evaluate the loss function for every parameter. So we can't just look at the picture and know which location is the best.\nRecall that the goal of the prediction task is to update the parameters in a way such that we minimize the loss function for some given input/output pair. Assuming the loss function is reasonably smooth, then our best bet of reaching a lower value of the error rate is by taking a small step towards the direction of the slope, which is the derivative of the function. The size of the \"small step\" is called the learning rate. \nUnder the assumption that the loss function looks similar given different input/output pairs, by iteratively moving down the slope for each datapoint, we can eventually reach a \"good\" set of weights that give a low error.\nThis is the motivation and idea of the back-propagation algorithm. \nA good follow-up question might be: Why do we say that the loss function is  similar given different data points? This is basically the same as assuming that there is some regularity in the data such that there is a \"most normal looking loss function\" that the data-set naturally approaches.\nHopefully that helps!\n", "type": 2, "id": "4642", "date": "2017-12-01T21:32:55.040", "score": 0, "comment_count": 0, "parent_id": "4629"}, "4632": {"line": 2832, "body": "You dont need a sigmoid function if you dont want one.  Any differentiable function will do.  Sigmoid functions are just one of many suitable functions. You could write your own differentiable function if you want a propriety solution. \n", "type": 2, "id": "4632", "date": "2017-12-01T02:46:02.760", "score": 2, "comment_count": 1, "parent_id": "4629"}, "4639": {"line": 2837, "body": "This derivative is used when calculating the error of your machine learning algorithm during gradient based minimization methods. \nRead below for more info. \nWhen performing supervised classification (with X, Y data vectors of inputs and outcome data to train with) you begin with the error function \nE(X, Y; \u03b8)= \u2211i (\u0192(xi; \u03b8)-yi)2\nfor total error over all data instances i, where f is your neural network, linear regression,...method of interest and \u03b8 is the set of weights. The goal here is to find weights that minimize your error when predicting training data (y) (which ideally generalizes to new data as well). To be explicit, \u0192(xi; \u03b8); outputs value of interest which should be yi. And E measures how far off it is in prediction. \nSo to train your classifier, you optimize E with something like gradient descent. Thus when \u2202E/\u2202\u03b8 = 0 (for a particular \u03b8), that means you hit a local minimum for the error function, or a point where the error in the current state of the predictor is low, meaning it is (hopefully) a good predictor.\nNote the \u0192 here is not the same as an activation function, as a neural network is defined differently than in linear regression, etc. and must perform a special kind of gradient descent called backpropagation. \nSo when you take \u2202E/\u2202\u03b8, what does it equal for a neural net? You should note the activation functions derivative is involved which is how it's used to measure error so to say. \n", "type": 2, "id": "4639", "date": "2017-12-01T16:27:52.320", "score": 4, "comment_count": 1, "parent_id": "4629"}, "6849": {"line": 4513, "body": "In order to train a neural network, you have to adjust each weights and biases to reduce the cost to as minimum as possible. The only way to do so is to subtract a small amount of the partial derivative of cost w.r.t. w, b from the respective parameters.\nif J is our cost function, after each iteration:\nw = w - lr*dJ_dw,      //where lr is a small scalar called learning rate and dJ_dw is the partial derivative of cost function w.r.t. w\n\nand same for bias\nb = b - lr*dJ_db,    //dJ_db is the partial derivative of cost function w.r.t b\n\nlet's look at how the partial derivatives are calculated.\nusing sigmoid as activation function, and squared error function as cost, we have:\nz = w*x + b\na = sigmoid(z)       // sigmoid(z) = 1.0 / (1.0 + exp(-z)), a is the final output\n\nJ = (a - y)*(a - y)  // where y is the expected output\n\nFor us to calculate partial derivatives of this cost function w.r.t. w, b, we need to use, chain rule of derivatives as:\ndJ_dw = dJ_da * da_dz * dz_dw    // dJ_da is the partial derivative of cost w.r.t. activation, a\ndJ_db = dJ_da * da_dz * dz_db\n\nin the above equations, da_dz is the derivative of activation function (sigmoid in our case) which is sigmoid(z).(1 - sigmoid(z))\n", "type": 2, "id": "6849", "date": "2018-06-22T07:06:53.663", "score": 0, "comment_count": 0, "parent_id": "4629"}}}
{"line": 2943, "body": "The capsule neural network seems to be a good solution for problems that involve hierarchies. For example, a face is composed of eyes, a nose and ears; a hand is made of fingers, nails, and a palm; and a human is composed of a face and hands.\nMany problems in NLP can be seen as hierarchical problems: there are words, sentences, paragraphs, and chapters, whose meaning changes based on the style of lower levels.\nAre there any research papers (which I should be aware of) on the application of capsule neural networks to NLP problems? \nAre there related research papers, which have been investigating hierarchical complexity within the domain of NLP, which could be easily translated to Capsule Network?\n", "type": 1, "id": "4766", "date": "2017-12-16T18:51:09.517", "score": 8, "comment_count": 1, "tags": ["neural-networks", "natural-language-processing", "applications", "capsule-neural-network"], "title": "Have capsule neural networks been used to NLP problems?", "answer_count": 3, "views": 2630, "accepted_answer": "20571", "answers": {"5072": {"line": 3170, "body": "For us to answer this question. First, we need to look at why capsule networks outperform convolution neural networks by as much as 45% in recognizing images that have been rotated, translated or are under a different pose. We can find Geof Hinton's paper on capsule networks here for reference https://arxiv.org/pdf/1710.09829v1.pdf\nIn a CNN architecture, a convolution layer is usually followed by a max-pooling layer. This is so that the lower levels can detect low-level features, like edges, while the high-level layers can detect abstraction like eyes. However, the application of max-pooling leads to the loss of important information regarding the location and spatial relationship between certain features. \nOn the other hand, this is where capsule networks excel, the way they represent certain features is locally invariant. This is why capsule networks can recognize images under different lighting conditions and deformations. They are likely to excel at applications such as video and object tracking but not necessarily NLP.\nThe current approach in NLP maps words and phrases to vectors. From there, we exploit the concept of vectors and distances between them (cosine, euclidean, etc.) to perform operations such as: finding the similarity between words and even documents, machine translation, and natural language understanding (NLU). \nCapsule networks are unlikely to succeed in NLP. This is because algorithms that aim to find the hierarchical structure of natural languages or approaches that focus on grammar have met little success. Research by Stanford University aiming at finding the hierarchical structure of natural languages can be found here https://nlp.stanford.edu/projects/project-induction.shtml\nAlthough conclusive research regarding other applications of capsule networks has not yet been conducted. They are likely to excel at applications such as video intelligence and object tracking but not necessarily NLP.\n", "type": 2, "id": "5072", "date": "2018-01-19T18:15:08.177", "score": 3, "comment_count": 1, "parent_id": "4766"}, "6857": {"line": 4517, "body": "There has been some recent work on this: Investigating Capsule Networks with Dynamic Routing for Text Classification\nSeems some are having some success with it.\n", "type": 2, "id": "6857", "date": "2018-06-22T19:47:32.280", "score": 0, "comment_count": 2, "parent_id": "4766"}, "20571": {"line": 13490, "body": "Geoffrey Hinton has started working on Thought Vectors at Google: https://en.wikipedia.org/wiki/Thought_vector\nThe basic idea is similar to his original idea with Capsule Networks, where activation happens by vectors instead of scalars, which allows the network to capture transformations: for example while traditional CNN needs to see object from all perspectives of three dimensional space, the Capsule networks are able to extrapolate transformations such as stretching much better.\nThought Vectors guide NLP similarly; one could say that there are two grammars, the linguistic grammar and the narrative grammar which is more universal (Vladimir Propp, Joseph Campbell, John Vervake). While dependency grammars do great job at understanding linguistic grammar, we lack tools for meaning extraction, which is narrative bound. Thus Thought Vectors could, at least in theory, give us a framework for matching the meaning of a word within a context rather than just lexically and grammarly trying to approximate the meaning through average co-occurances.\nNeural Networks with Thought Vectors would be highly complex and beyond our computational resources today (Hinton predicts in one paper, that we would get there around 2035), however, one could conduct empirical research already by giving a heuristic structure for Thought Vectors by utilizing narrative systems that do compute more easily. One could for example have text segments annotated with writing theories or other such devices that would approximate the Thought Vectors conceptually. For example annotating the text with state transformations of conflict driven partially ordered causal link planner (cPOCL, Gervas et al.) or use a writing theory framework such as Dramatica to annotate known movie scripts (http://dramatica.com/theory http://dramatica.com/analysis).\nHinton himself is currently active in NLP research: https://research.google/people/GeoffreyHinton/\nHere is a nice explanation of Thought Vectors: https://pathmind.com/wiki/thought-vectors\n", "type": 2, "id": "20571", "date": "2020-04-23T09:50:52.870", "score": 2, "comment_count": 0, "parent_id": "4766"}}}
{"line": 2370, "body": "I'm developing a Game AI which tries to master racing simulations. I already trained a CNN (alexnet) on ingame footage of me playing the game and the pressed keys as the target. As the CNN is only making predictions on a frame-to-frame basis, and I resized the image input to 160x120 due to GPU memory limitations, it cannot read the speedometer, thus seems not to have a feeling for its current velocity. I thought of different ways to fix this issue:\n\nCrop the captured image down to the size of the speedometer, which displays the current speed in mph, and feed the low resolution game image, as well as the relatively high-res image (70x30) of the current speed into the neural network, which makes predictions based on the two images.\n\nAs I don't know whether alexnet can serve as an OCR as well, my second thought was to use an existing one (like tesseract-ocr/PyTesser) on the cropped image and feed its output to the fully connected layer.\n\nI already tried to implement an optical-flow algorithm, but sadly, non of the python ones seems to output good real-time results. I wonder whether I can input the current frame as well as the last one, and let alexnet figure out the movement.\n\n\nAs the processing has to happen in real time, and the only performance reviews of pytesser I found reported a processing time of ~100ms (never tested that). My Question is, what method would work best.\nThanks!\nEdit: Optical flow would have the advantage of the AI knowing in which direction other cars are moving as well.\n", "type": 1, "id": "4013", "date": "2017-09-12T17:59:38.443", "score": 6, "comment_count": 0, "tags": ["convolutional-neural-networks", "gaming", "game-ai"], "title": "Game AI - Fast python OCR or cropped image input", "answer_count": 1, "views": 463, "accepted_answer": "4022", "answers": {"4022": {"line": 2378, "body": "Option 1 would be a very interesting one from a research perspective. I cannot imagine that CNNs have to capacity at the moment, to learn the concept of numbers and apply them in a useful way. If it was an analog speedometer, things would be different. But it would be really interesting to try it and see what you can achieve with this approach. I haven't read any research papers yet, where such a challenge would have been mastered.\nIf you are less concerned with research but rather get this project to work, I would propose an approach similar to option 2. The OCR tools you mentioned were designed to identify symbols, even if distorted or otherwise hard to read. In your case, the numbers will always look the same and are most likely always in fixed positions. Therefore, using fancy OCR algorithms or neural nets are overkill for the problem at hand. You can write a simple algorithm that crops the speedometer, searches for the 10 possible patterns (0 to 9) with 10 specific kernels and calculate the speed from the result. This can be implemented efficiently without the need to train a CNN or some other complex algorithm.\n\nMethod for implementation:\nTo answer the first question in the comments. The algorithm for this is really easy. Your image will be represented by a 2D array (or maybe a 3D array if you are working with colors). All you need to do is find out, what your numbers look like and store the appropriate array. You should end up with 10 different arrays for 10 different numbers.\nTo find out if a number is in your current picture, you only have to check if the array for this number is a subset of the array of your current picture. The position, where the arrays are matching, will also indicate the position of the number. You do this for all 10 numbers and can calculate the actual value displayed on the speedometer afterwards from the results.\nThis is very easy if the numbers always look exactly identical. In this case you can simply look for an exact match. If there are slight variations, you can look for parts of the image array that is similar to your template arrays. Just sum the absolute values of the diffs of each pixel value and if it is below a certain threshold, you have your match. That's all there is to it.\nIn python I like working with PIL for this kind of image manipulation, but there are other frameworks out there as well.\n", "type": 2, "id": "4022", "date": "2017-09-13T05:56:12.957", "score": 0, "comment_count": 5, "parent_id": "4013"}}}
{"line": 3944, "body": "There is a popular story regarding the back-of-the-envelope calculation performed by a British physicist named G. I. Taylor. He used dimensional analysis to estimate the power released by the explosion of a nuclear bomb, simply by analyzing a picture that was released in a magazine at the time.\nI believe many of you know some nice back-of-the-envelope calculations performed in machine learning (more specifically neural networks). Can you please share them?\n", "type": 1, "id": "6082", "date": "2018-04-16T17:00:14.823", "score": 10, "comment_count": 2, "tags": ["neural-networks", "machine-learning", "artificial-neuron"], "title": "Back-of-the-envelope machine learning (specifically neural networks) calculations", "answer_count": 2, "views": 343, "accepted_answer": null, "answers": {"6090": {"line": 3948, "body": "I have one to share. This is no formula, but a general thing I have noticed. \nThe number of neurons + neurons should be proportionate, in some way, to the complexity of the classification.\nAlthough this is fairly basic and widely known, it has helped me in many times to consider one thing: how many at a minimum does it need?\n", "type": 2, "id": "6090", "date": "2018-04-17T15:00:49.500", "score": 0, "comment_count": 2, "parent_id": "6082"}, "20978": {"line": 13800, "body": "I think a nice back-of-the envelope calculation is the intuition for exploding/vanishing gradients in RNNs:\nSimplifications\n\ndiagonalisable weights $U$ and $W$\nno non-linearities\n1 layer\n\nThis gives a hidden state $h_t$ at timestep $t$ for input $x_t$:  $h_t = W\\cdot h_{t-1} + U\\cdot x_t$\nLet $L_t$ be the loss at timestep $t$ and the total loss $L = \\sum_t L_t$. Then (eq. 3 -> 5 in the paper)\n$$\n\\frac{\\partial L_t}{\\partial W} \\sim = \n\\sum_{k=1}^{t} \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W} = \\sum_{k=1}^{t}\\frac{\\partial h_t}{\\partial h_k}\\times\\alpha_{t, k}\n$$\nLet's not care about terms regrouped in $\\alpha_{t, k}$:\n$$\n\\frac{\\partial h_t}{\\partial h_k} = \\prod_{k<i\\leq t} \\frac{\\partial h_i}{\\partial h_{i-1}} =  \\prod_{k<i\\leq t}  W =  \\prod_{k<i\\leq t} PDP^\\top = PD^{t-k}P^{\\top}\n$$\nSo you can easily see$^1$ that if the eigen values of $W$ (in the diagonal matrix $D$) are larger than $1$, the gradient will explode with time, and if they are smaller than $1$, it will vanish.\nMore detailed derivations in On the difficulty of training recurrent neural networks\n\n$^1$ remember $\\lim_{n \\to +\\infty}|x^n| = +\\infty$ if $|x|>1$ and $=0$ for $|x| < 1$\n", "type": 2, "id": "20978", "date": "2020-05-06T23:39:03.320", "score": 0, "comment_count": 2, "parent_id": "6082"}}}
{"line": 3774, "body": "Deep networks notoriously take a long time to train. \nWhat is the most time-consuming aspect of training them? Is it the matrix multiplications? Is it the forward pass? Is it some component of the backward pass?\n", "type": 1, "id": "5861", "date": "2018-04-01T19:50:28.670", "score": 5, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "training", "deep-neural-networks"], "title": "What is the most time-consuming part of training deep networks?", "answer_count": 3, "views": 1151, "accepted_answer": null, "answers": {"6551": {"line": 4296, "body": "There is no such single hard and slow step in training neural networks ,  forward pass involves large number of matrix multiplications so does backward pass , even though there are highly optimized libraries for matrix multiplications neural networks act on very high dimensional (tensor) multiplications in both forward and backward passes which makes it difficult train . however backward pass would be even more slower or even intractable if we won't use backpropagation in case of large neural networks, since computing derivatives is time-taking.\nrefer training results in for exact number https://github.com/baidu-research/DeepBench#types-of-operations\n", "type": 2, "id": "6551", "date": "2018-05-29T15:42:02.713", "score": 0, "comment_count": 0, "parent_id": "5861"}, "8150": {"line": 5484, "body": "Forward pass\nThe output of a layer can be calculated given the output of the previous layer. So the GPU can parallelize this computation for every layer and over the minibatch which is done by calculating a big matrix. But it needs to be sequential from layer to layer (earlier layers to higher layers). Regarding the layer type convolutions or especially fully connected layers can result in a big matrix calculation.\nBackward pass\nThe gradient of a layer with respect to the layer input (and layer parameters) can only be calculated given the gradient of the layer output (input gradient of a subsequent layer) and input to the layer (output of the previous layer). This again can be parallelized over a layer and minibatch but is sequential from higher layers to earlier layers. Moreover, since the backward pass relies on the outputs of the forward pass all intermediate layer outputs of the forward pass have to be cached for the backward pass which results in a high (GPU) memory usage.\nForward and backward pass take most of the time\nSo, these two steps take a long time for 1 training iteration, and (depending on your network) high GPU memory usage. But you should read and understand the backpropagation algorithm that basically explains everything.\nMoreover, to train a network from scratch, in general, takes lots of iterations because especially in the earlier layers training the parameters is based on gradients that are affected by lots of previous layers, which can result in noisy updates, etc., that do not always push the network parameters in the right direction directly. In contrast, e.g. fine-tuning a pre-trained network on some new task can for example already be done with much less training iterations.\n", "type": 2, "id": "8150", "date": "2018-09-26T19:21:09.223", "score": 1, "comment_count": 0, "parent_id": "5861"}, "23802": {"line": 15967, "body": "Check out Figure 6 in this paper: PyTorch Distributed: Experiences on Accelerating Data Parallel Training\nIt breaks down the latency of the forward pass, the backward pass, the communication step, and the optimization step for running both ResNet50 and BERT on a NVIDIA Tesla V100 GPUs.\nFrom measuring the pixels in the figure, I estimated the times for the forward, backward, and optimization steps as a percentage of their total time combined. (I ignored the communication step shown in the figure because that was only to show how long an unoptimized communication step would take when doing data-parallel training). Here are the estimates I got:\n\nForward: 23%\nBackward: 74%\nOptimization: 3%\n\nSo the backward pass takes about 3x as long as the forward pass, and the optimization step is relatively fast.\n", "type": 2, "id": "23802", "date": "2020-09-28T09:46:25.133", "score": 2, "comment_count": 0, "parent_id": "5861"}}}
{"line": 4567, "body": "Let's suppose I have an image with 16 channels that goes to a convolutional layer, which has 3 trainable $7 \\times 7$ filters, so the output of this layer has depth 3.\nHow does the convolutional layer go from 16 to 3 channels? What mathematical operation is applied?\n", "type": 1, "id": "6934", "date": "2018-06-28T22:47:30.847", "score": 4, "comment_count": 1, "tags": ["convolutional-neural-networks", "convolution", "convolutional-layers", "convolution-arithmetic", "2d-convolution"], "title": "How is the depth of the input related to the depth of the output of a convolutional layer?", "answer_count": 3, "views": 1402, "accepted_answer": "6936", "answers": {"6936": {"line": 4569, "body": "The reason why you go from 16 to 3 channels is that, in a 2d convolution, filters span the entire depth of the input. Therefore, your filters would actually be $7 \\times 7 \\times 16$ in order to cover all channels of the input.\nDetailed procedure\nThe output of the convolution automatically has a depth equal to the number of filters (so in your case this is $3$) because you have an $m \\times k$ filter matrix, where $m$ is the number of filters and $k$ is the number of elements in the unrolled filter (in your case, $m = 3$ and $k = 7 \\times 7 \\times 16 = 784$, so the filter matrix is $3 \\times 784$).\nThe input is usually unrolled according to the im2col procedure, where each tile corresponding to a single filter location is stretched into a column equal to the unrolled filter size. This is repeated for each filter location, so you end up with a very large matrix of size $k \\times n$, where $k$ is the same as $k$ above in the filter matrix, and $n$ depends on your padding and stride.\nMultiplying the $m \\times k$ filter matrix with the $k \\times n$ input matrix gives you an $m \\times n$ output matrix, where $m$ is the number of filters.\nFurther reading\nYou can find some very nice visual explanations of the convolution procedure here and here.\n", "type": 2, "id": "6936", "date": "2018-06-29T01:01:34.533", "score": 3, "comment_count": 1, "parent_id": "6934"}, "6940": {"line": 4572, "body": "Your input is having 16 channels of each of dimension m x n and there are 3 filter namely f1, f2 and f3 of dimensions fm x fn. We say that a filter is applied to a channel when it is superimposed on the image starting left-most, performing the operation of multiplying the weights of filter with the corresponding value in the image and then summing up to a single value and moving the filter to right (then down when it reaches rightmost part) across the image according to the stride of the filter.\nAs mentioned when a filter say f1 is applied to a channel say c, there is a single value. Now applying them to all channels  we get 16 values and all of them are added up to a single value. f1 is moved according to the stride and the same operation is repeated to get an output with a single channel (number of rows and columns are determined by padding, stride, dilation and kernel size of the filers).\nThe aforesaid process is done by all the 3 filters giving rise to 3 channels. In this way the conv layer makes the input to go from 16 to 3 channels. More detailed explanations can be found here.\n", "type": 2, "id": "6940", "date": "2018-06-29T06:15:42.357", "score": 1, "comment_count": 0, "parent_id": "6934"}, "14060": {"line": 9762, "body": "In short, its basically a magic of matrix multiplications. The dimensions of the weight matrix are such that w.r.t the input layer dimensions, the desired output layer dimensions are taken care of.   \n", "type": 2, "id": "14060", "date": "2019-08-20T06:18:34.080", "score": 0, "comment_count": 0, "parent_id": "6934"}}}
{"line": 3732, "body": "Suppose one trains a CNN to determine if something was either a cat/dog or neither (2 classes), would it be a good idea to assign all cats and dogs to one class and everything else to another? Or would it be better to have a class for cats, a class for dogs, and a class for everything else (3 classes)? My colleague argues for 3 classes because dogs and cats have different features, but I wonder if he's right.\n", "type": 1, "id": "5801", "date": "2018-03-26T09:29:22.130", "score": 3, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "training", "datasets", "binary-classification"], "title": "If we want to classify something as either a cat/dog or neither, do we need 2 or 3 classes?", "answer_count": 4, "views": 177, "accepted_answer": "5813", "answers": {"5804": {"line": 3734, "body": "If you want to determine if something is either a\n\ncat/dog or neither\n\nyou need 2 classes:\n\none for dog or cat, and\none for anything else.\n\nHowever, if you assign all cats and dogs to the same class $A$, if an input is classified as $A$, then you won't be able to know whether it is a dog or a cat, you will just know that it is either a dog or a cat.\nIn case you wanted to distinguish between cats and dogs too (apart from neither of them), then you'll need $3$ classes.\nFinally, if you specify only 2 classes:\n\ndog, and\ncat,\n\nthen your CNN will try to classify any new input as either a dog or a cat, even though it is neither a dog nor a cat (e.g. maybe it is a horse).\n", "type": 2, "id": "5804", "date": "2018-03-26T14:34:52.437", "score": 2, "comment_count": 1, "parent_id": "5801"}, "5813": {"line": 3739, "body": "The best approach may be to have a cat, dog, and neither class (3 classes total) and go with a regression approach -- specifically, outputting the probabilities of each class for any given input. From there, you can always take the probabilities of each output and derive the probability of a cat and dog class or neither class. Also, make sure you use the right activation on the output layer and cost function so that you can interpret the outputs as probabilities (e.g. softmax activation and cross-entropy loss).\n", "type": 2, "id": "5813", "date": "2018-03-27T18:49:02.383", "score": 1, "comment_count": 2, "parent_id": "5801"}, "6285": {"line": 4085, "body": "As far as generalization error is concerned, you are better off by learning the data distribution of (A and B) classes using unsupervised criterion.   \nIf you capture the underlying factors that explain most of the variations belong to A and B classes, after that, fine-tune it using a supervised criterion. in this way if you used two classes one for (A or B) and the other for neither (A or B), you will not force the model to learn features don't belong to (A or B), because the model just checks if a new data point is probably likely drawn from the data distribution that resembles (A or B).\nSide note: you will never have the data necessary to explore the internal structure of the otherwise class (neither A nor B).\n", "type": 2, "id": "6285", "date": "2018-05-06T05:20:52.640", "score": 1, "comment_count": 0, "parent_id": "5801"}, "23761": {"line": 15941, "body": "Normally you only have two classes along with a threshold probability.\nIt's how systems like YOLO work.\n", "type": 2, "id": "23761", "date": "2020-09-25T19:48:52.140", "score": -1, "comment_count": 0, "parent_id": "5801"}}}
{"line": 3360, "body": "I have been trying to use CNN for a regression problem. I followed the standard recommendation of disabling dropout and overfitting a small training set prior to trying for generalization. With a 10 layer deep architecture, I could overfit a training set of about 3000 examples. However, on adding 50% dropout after the fully-connected layer just before the output layer, I find that my model can no longer overfit the training set. Validation loss also stopped decreasing after a few epochs. This is a substantially small training set, so overfitting should not have been a problem, even with dropout. So, does this indicate that my network is not complex enough to generalize in the presence of dropout? Adding additional convolutional layers didn't help either. What are the things to try in this situation? I will be thankful if someone can give me a clue or suggestion. \nPS: For reference, I am using the learned weights of the first 16 layers of Alexnet and have added 3 convolutional layers with ReLU non-linearity followed by a max pooling layer and 2 fully connected layers. I update weights of all layers during training using SGD with momentum. \n", "type": 1, "id": "5318", "date": "2018-02-15T05:55:24.767", "score": 3, "comment_count": 3, "tags": ["convolutional-neural-networks", "linear-regression", "dropout"], "title": "What to do if CNN cannot overfit a training set on adding dropout?", "answer_count": 3, "views": 1186, "accepted_answer": null, "answers": {"6440": {"line": 4209, "body": "I think that your misuse of the term over-fitting made the question vague. In layman terms, over-fitting means that a model fails to generalize to real-world scenarios, but is accurate with the training set.\nUsing a dropout layer means that the network cuts down on neurons that are used for training, in this case, 50%.\nRecommendations for improving training accuracy would be:\n\nTransfer learning\nAdding more layers to the network (also shifting number of neurons helps)\nAdding epochs\nChanging optimizer (Adam and RMSProp are some of my suggestions)\nAdding activation layers\n\n", "type": 2, "id": "6440", "date": "2018-05-17T14:39:52.947", "score": 0, "comment_count": 1, "parent_id": "5318"}, "23425": {"line": 15684, "body": "Let's start with understanding what over-fitting means. Your model is over-fitting if during training your training loss continues to decrease but (in the later epochs) your validation loss begins to increase. That means the model can not generalize well to images it has not previously encountered.\nNaturally, you do not want this situation. What you want is a high training accuracy and a very low validation loss, which implies a high validation accuracy.\nThe first task is to ensure that your model gets a high training accuracy. Once that is accomplished, you can work on getting a low validation loss.\nIf your model is overfitting, there are several ways to mitigate the problem. First, start out with a simple model. If you have a lot of dense layers with a lot of neurons, reduce the hidden dense layers to a minimum. Typical just leave the top dense layer used for final classification. Then see how the model trains. If it trains well, look at the validation loss and see if it is reducing in the later epochs. If the model does not train well, add a dense layer followed by a dropout layer. Use the level of dropout to adjust for overfitting. If it still trains poorly, increase the number of neurons, and train again. If that fails, add another dense hidden layer with fewer neurons than the previous layer followed by another dropout layer.\nAnother method to combat overfitting is to add regularizers to the dense layers. Documentation for that is here.\n", "type": 2, "id": "23425", "date": "2020-09-04T17:06:31.720", "score": 1, "comment_count": 0, "parent_id": "5318"}, "26500": {"line": 18101, "body": "Sorry if this is a bad use of answer to add comment but since my reputation is not high enough this is only way to leave a comment to OP's question.\nI think some of the answers misunderstood the OP's intention.\nOver fitting is used as a means to test the complexity of the model - if a model cannot overfit a small dataset then it's likely not able to generalize well.\nIt's not that OP misunderstood the meaning of over fitting.\nFor Instance, I think this discussion is relevant: https://stats.stackexchange.com/questions/492165/what-to-do-when-a-neural-network-cannot-overfit-one-training-sample\n", "type": 2, "id": "26500", "date": "2021-02-22T02:59:14.393", "score": 2, "comment_count": 0, "parent_id": "5318"}}}
{"line": 3759, "body": "How can I train a neural network to recognize sub-sequences in a sequence flow?\nFor example: Given the sequence 111100002222 as an input sample from a stream, the neural network would recognize that 1111 , 0000 , 2222 are sub sequences (so 111100 would not be a valid subsequence) and so on for ~ 50 to 100 different subsequences.\nThere is no particular order in which the subsequence would appear in the flow.\nNo network architecture restriction.\nSubsequences are of variable length.\nGeneral concepts, ideas, and theory are welcome. \n", "type": 1, "id": "5838", "date": "2018-03-30T13:12:22.373", "score": 3, "comment_count": 7, "tags": ["neural-networks", "algorithm"], "title": "Ideas on how to make a neural net learn how to split sequence into sub sequences", "answer_count": 5, "views": 458, "accepted_answer": "7104", "answers": {"5977": {"line": 3861, "body": "Another approach could be to predict the class of a sequence and not the break point. Assuming that each sequence is part of a class, you can use a LSTM. Inputing the multiple sequences (111100002222 ) and let predict the class for each sequence (c1,c1,c1,c1,c0,c0,c0,c0,c2,c2,c2,c2)\n", "type": 2, "id": "5977", "date": "2018-04-10T07:22:28.453", "score": 0, "comment_count": 2, "parent_id": "5838"}, "5985": {"line": 3868, "body": "How about this ?\n1 - Learn all the basic building blocks of possible sub-sequence\nIn our words sequence example, that would correspond to phonemes.\n(I'm guessing that this step can even be done using unsupervised learning.)\nSo in the following example :\nHello Laurie, we would have learned 3 phonemes : HE, LO, RI.\n2- Learn all subsequence as sequences of 'building blocks'\nUsing a ClockWorkRNN with timesteps of interval +1 with, let's say, 10-15 timestep (groups), that is fed the next 'phoneme id' in the sequence, we would have a space large enough to record most words (Obviously, the number of timesteps should be size of the biggest word).\nThis is the subsequences memory RNN.\nIts sole purpose is to remember subsequences.\nNow, i'm really brainstorming here , taking a very wild guess, but what if : \nAfter training this RNN to a satisfying error rate, we check if the output of the RNN is very different to the next input for a couple of timesteps.\nIn other word, we see if the neural network has been able to 'guess' the next building block of the subsequence.\nIf not, then its a point of interest , because there is not a lot of possibilities as of why this would happend : the only one I see is \n1 - The RNN is currently receiving another word, thus making this timestep a sub-sequence 'break point'\nDo you guys see any points that could prove this theory wrong ?\n", "type": 2, "id": "5985", "date": "2018-04-10T20:32:47.903", "score": 0, "comment_count": 0, "parent_id": "5838"}, "5973": {"line": 3857, "body": "I guess, supervised learning should work rather well: You'd feed the network with a fixed substring and it'd determine if the middle character is the first letter of a word, or a last one, or neither or both.\nSo 2*n+1 inputs (fed e.g., with the string \"ingsits\") should output a 1 on the output determining if the middle letter (here: \"s\") is the first one of a word and a 0 on the output determining if it's the last one (taken from \"Thekingsitsthere\"). Each input character should probably be 1 hot encoded.\nYou'd probably want to use more context characters than in my example. OTOH you can use a simple MLP with no temporal complications. It'll never get perfect as it's impossible, but it get pretty close.\nConcerning unsupervised learning I'm skeptical...\n", "type": 2, "id": "5973", "date": "2018-04-10T02:54:48.923", "score": 0, "comment_count": 0, "parent_id": "5838"}, "5974": {"line": 3858, "body": "(NOTE: I think it will be easier to do it without ANNs...)\nBut if you insist:\n\nconvert the sequence into a fixed-size vectors.\npush trough a 2-5 1D-convolution layer with 1 neuron dense layer at the end (sigmoid activation) and another K-points detector for getting the sequence breakage points\ncreate a training set - to find the break-points (12, 23, 34 ...) in the sequence.\ntrain a detector with SGD to find these break-points. - loss functions: cross_entropy. \n\nThen, it should learn to find the breakage points, and based on this you can easily split the sequence.\n", "type": 2, "id": "5974", "date": "2018-04-10T03:24:08.583", "score": 0, "comment_count": 2, "parent_id": "5838"}, "7104": {"line": 4702, "body": "The problem in the original question is akin to that of inducing a context-sensitive grammar (CSL), except that it is harder because a CSL is assumed to be composed of fixed-length subsequences. It is probably closer to the problem of inducing a Reber grammar, but that in turn seems like an overkill.\nLSTMs are known to be able to learn both CSL and Reber grammars. However, I doubt that this is what you really need because of the following comment:\n\n[...] given an entire book where there is NO spaces anywhere, only characters (including special characters, like commas), in what way can we make the network learn the 'word boudaries' of this book.\n\nThis is called morphology induction, and it is a much harder problem than that of simple Reber grammar induction. Note that finding word boundaries is a special case of the problem of finding morpheme boundaries. There have been many attempts to solve this (also see this survey paper for more details and references). \nMost approaches developed seem to rely on statistical principles (like MDL) and do not use neural networks (a counterexample using LSTMs). My intuition is that the extreme morphological variability across languages (ranging from Finno-Ugric languages with highly inflectional morphology to Sino-Tibetan languages with hardly any morphology at all) makes it hard to train neural networks in a language-agnostic way. However, you might have better luck if you focus on a single language.\nHope that helps.\n", "type": 2, "id": "7104", "date": "2018-07-10T13:38:29.800", "score": 2, "comment_count": 0, "parent_id": "5838"}}}
{"line": 3099, "body": "i'm quite new to neural network and i recently built neural network for number classification in vehicle license plate. It has 3 layers: 1 input layer for 16*24(382 neurons) number image with 150 dpi , 1 hidden layer(199 neurons) with sigmoid activation function, 1 softmax output layer(10 neurons) for each number 0 to 9. \nI'm trying to expand my neural network to also classify letters in license plate. But i'm worried if i just simply add more classes into output, for example add 10 letters into classification so total 20 classes, it would be hard for neural network to separate feature from each class. And also, i think it might cause problem when input is one of number and neural network wrongly classifies as one of letter with biggest probability, even though sum of probabilities of all number output exceeds that.\nSo i wonder if it is possible to build hierchical neural network in following manner:\nThere are 3 neural networks: 'Item', 'Number', 'Letter'\n\n'Item' neural network classifies whether input is numbers or letters.\nIf 'Item' neural network classifies input as numbers(letters), then input goes through 'Number'('Letter') neural network.\nReturn final output from Number(Letter) neural network.\n\nAnd learning mechanism for each network is below:\n\n'Item' neural network learns all images of numbers and letters. So there are 2 output.\n'Number'('Letter') neural network learns images of only numbers(letter).\n\nWhich method should i pick to have better classification? Just simply add 10 more classes or build hierchical neural networks with method above?\n", "type": 1, "id": "4988", "date": "2018-01-13T15:14:53.140", "score": 6, "comment_count": 0, "tags": ["neural-networks", "classification"], "title": "Is it better to make neural network to have hierchical output?", "answer_count": 3, "views": 175, "accepted_answer": "4992", "answers": {"4992": {"line": 3103, "body": "Item network should be recogniting regional register number standards and typical places of numbers on those standards. Otherwise your 'Item' phase has equal task than the whole system, because 'Letter' and 'Number' do not differ in plates so much you could right away choose between the two without first evaluating the exact object on each case.\nSide note: correct me if that is not true on your local plate system\n", "type": 2, "id": "4992", "date": "2018-01-13T20:25:24.450", "score": 0, "comment_count": 1, "parent_id": "4988"}, "4997": {"line": 3108, "body": "Just use one network with a larger Softmax output layer and more hidden units. If you have enough training data, it will work just fine. In fact it could emulate the architecture you propose.\n", "type": 2, "id": "4997", "date": "2018-01-14T00:49:38.627", "score": 3, "comment_count": 0, "parent_id": "4988"}, "5037": {"line": 3140, "body": "I agree with the above answer. If you want to research this more in-depth look at this paper: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf\n", "type": 2, "id": "5037", "date": "2018-01-17T03:25:18.633", "score": 0, "comment_count": 0, "parent_id": "4988"}}}
{"line": 4276, "body": "There are many books, courses, etc. out there, but not sure which path to take. \nSo what would be the most effective way (shortest) to learn natural language processing online?\np.s. I mean learning fundamentals, not how to use existing libraries or services.\n", "type": 1, "id": "6527", "date": "2018-05-27T16:33:37.543", "score": 3, "comment_count": 0, "tags": ["neural-networks", "natural-language-processing"], "title": "What is the most effective way to learn natural language processing online?", "answer_count": 3, "views": 542, "accepted_answer": "6569", "answers": {"6569": {"line": 4310, "body": "Start Without Code, Begin with Language\nThe best approach to learning the field of natural language processing is to first read about linguistics to understand phonetics and the other aspects of human language that underlies familiar things like grammar, spelling, interpersonal communication, listening skills, reading, writing, and publication.  Starting that way will help you weed through the garbage and identify the contributions of value in the natural language field.  (There is quite a high percentage of garbage in print.)\nLearning that vocal sound and the fundamental units of communication are central to natural language parsing and processing and language synthesis, not the abstract concepts we learned in school will give you a framework for more specific NLP education.\nBy fundamental unit, I mean units larger than one word like, \"slight of hand,\" and units smaller than one word like the prefix, \"sub-,\" which can be used to construct, \"substellar,\" the meaning of which can be accurately guessed even though it doesn't spell check.\nAvoiding Trendy Books and Courses\nI recommend avoiding books and courses that talk primarily about speech recognition, syntax, semantic trees, and speech synthesis.  Find materials that understand what language is.\nCentral Concepts to Guide Understanding\nThe process of language begins with ideas in one mind (whether that mind is human or synthetic).\nTo create speech, ideas must be serialized into a string of signs that conform, not to a set of rules, but a set of associations between ideas and the sounds of basic linguistic units.  (Writing is a later, less developed invention, both in human history and in a child's language development.)\nThe sounds of the basic linguistic units are performed by the lungs and vocal instrumentation and pass to the cochlear organs connected to the recipient mind.  There the serialized ideas are subjected (if the person or computer is hearing) to reassembly of the serialized stream of basic linguistic units.\nThe recipient can be hearing but not listening.  If the person is also listening, the serialized stream is then processed to attempt the cognitive and emotional reconstruction of the original ideas.\nNote that ideas are NOT serial data.  They are hugely associative and supported by layered models.  They are also story-centric: They depend largely on the recipient's understanding of culturally common stories and themes.  When a child says, \"Promise?\" there is an entire array of ideas about parental reliability and the value of the expectation to the child that goes with that single sign.\nAlso note that cognitive and emotional reconstruction are always processed subjectively first.  In other words, the ideas of the speaker (or writer) are reassembled from the serial stream of signs according to the attentions, interests, comprehension, and current state of mind of the listener (or reader).  If objectivity occurs at all, it is rare and usually subsequent.\nThe notion that artificial intelligence will be less subjective than natural intelligence is precarious at best and is likely to prove naive.\nScan Material, Pick Carefully, and Study\nIf the material you scan, explains any of the details of the above process and how experimenters and engineers have simulated any of this machinery in Python, Java, C++, other common languages or any of the frameworks built on these, then study that material and try out any code you can get your hands on that implements those successful language processing components.\n", "type": 2, "id": "6569", "date": "2018-05-30T08:02:41.030", "score": 1, "comment_count": 0, "parent_id": "6527"}, "6530": {"line": 4278, "body": "I think you could learn NLP ,by taking this course https://see.stanford.edu/Course/CS224N\nNatural Language Processing - Stanford University | Coursera and try your best with the homework.\nTo further study, you could learn some deep learning algorithms like RNN. And of course, it's better that you will be able to learn more a lot from university professors with minimal time period.\nHope it can help you.\n", "type": 2, "id": "6530", "date": "2018-05-28T12:24:36.340", "score": 2, "comment_count": 0, "parent_id": "6527"}, "11239": {"line": 7556, "body": "Your first stop should be Sebastian Ruder's NLP newsletter, a monthly publication that will keep you current with the cutting-edge research in this field - progress is so rapid that monthly is the right frequency.  Not saying the @FauChrisian approach is wrong, but it's not obviously connected to the research that I see.  The Stanford course that @quintumnia mentions is a good bet too - again, progress is so rapid that books (and most courses) can't keep up. Fast.ai is also highly recommended - the founder is very active in the field.\n", "type": 2, "id": "11239", "date": "2019-03-14T17:47:45.863", "score": 0, "comment_count": 0, "parent_id": "6527"}}}
{"line": 3846, "body": "The problem to solve is non-linear regression of a non-linear function. My actual problem is to model the function \"find the max over many quadratic forms\": max(w.H.T * Q * w), but to get started and to learn more about neural networks, I created a toy example for a non-linear regression task, using Pytorch. \nThe problem is that the network never learns the function in a satisfactory way, even though my model is quite large with multiple layers (see below). Or is it not large enough or too large? How can the network be improved or maybe even simplified to get a much smaller training error?\nI experimented with different network architectures, but the result is never satisfactory. Usually, the error is quite small within the input interval around 0, but the network is not able to get good weights for the regions at the boundary of the interval (see plots below). The loss does not improve after a certain number of epochs.  I could generate even more training data, but I have not yet understood completely, how the training can be improved (tuning parameters such as batch size, amount of data, number of layers, normalizing input (output?) data,  number of neurons, epochs, etc.)\nMy neural network has 8 layers with the following number of neurons: 1, 80, 70, 60, 40, 40, 20, 1.\nFor the moment, I do not care too much about overfitting, my goal is to understand, why a certain network architecture/certain hyperparameters need to be chosen. Of course, avoiding overfitting at the same time would be a bonus.\nI am especially interested in using neural networks for regression tasks or as function approximators. In principle, my problem should be able to be approximated to arbitrary accuracy by a single layer neural network, according to the universal approximation theorem, isn't this correct?\n\n\n\n", "type": 1, "id": "5960", "date": "2018-04-09T11:37:05.947", "score": 2, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "pytorch"], "title": "Why isn't my model learning satisfactorily?", "answer_count": 2, "views": 527, "accepted_answer": "5961", "answers": {"5961": {"line": 3847, "body": "Neural networks learn badly with large input ranges. Scale your inputs to a smaller range e.g. -2 to 2, and convert to/from this range to represent your function interval consistently.\n", "type": 2, "id": "5961", "date": "2018-04-09T11:54:37.387", "score": 1, "comment_count": 3, "parent_id": "5960"}, "23989": {"line": 16104, "body": "I only have one good news... There is nothing wrong with your code. Neural networks tend to do that. Especially with a really complex function.\n\nIncreasing the amount of neurons will not decrease how the error is distributed.\nThere are better loss functions for each case but is not a really effective solution.\nNeural networks are really good managing noise. So, they are good ignoring minorities. It's a common expression \"ANN are racist\".\n\nI recommend you to deploy a histogram DataSet vs Output Value. To see if you have too much more data from the central region than in the frontier.\nIf you can generate more data at will. Generate more values in the specific zones with more errors.\nThis will increase the error and force the backpropagation algorithm to improve in that area.\nMore information on your optimization algorithm may be useful. But, like I said, everything seems perfectly normal.\n", "type": 2, "id": "23989", "date": "2020-10-09T13:13:56.080", "score": 0, "comment_count": 0, "parent_id": "5960"}}}
{"line": 5733, "body": "I've created a neural net using the ConvNetSharp library which has 3 fully connected hidden layers. The first having 35 neurons and the other two having 25 neurons each, each layer with a ReLU layer as the activation function layer.\nI'm using this network for image classification - kinda. Basically it takes inputs as raw grayscale pixel values of the input image and guesses an output. I used stochastic gradient descent for the training of the model and a learning rate of 0.01. The input image is a row or column of OMR \"bubbles\" and the network has to guess which of the \"bubble\" is marked i.e filled and show the index of that bubble.\nI think it is because it's very hard for the network to recognize the single filled bubble among many.\nHere is an example image of OMR sections:\n\nUsing image-preprocessing The network is given a single row or column of the above image to evaluate the marked one.\nHere is an example of a preprocessed image which the network sees:\n\nHere is an example of a marked input:\n\nI've tried to use Convolutional networks but I'm not able to get them working with this.\nBasically, my question is that what type of neural network and network architecture should I use for this kind of task?. An example of such a network with code would be greatly appreciated.\nI have tried many preprocessing techniques such as background subtraction using the AbsDiff function in EmguCv and also using the MOG2 Algorithm and I've also tried threshold binary function but there still remains enough noise in the images which makes it difficult for the neural net to learn. \nI think this problem is not specific to using neural nets for OMR but for others too. It would be great if there could be a solution out there that could store a background/template using a camera and then when the camera sees that image again, it perspective transforms it to match exactly to the template\nI'm able to achieve this much - and then find their difference or do some kind of preprocessing so that a neural net could learn from it. If this is not quite possible, then is there a type of neural network out there which could detect very small features from an image and learn from it.  I have tried Convolutional Neural Network but that also isn't working very well or I'm not applying them efficiently.\n", "type": 1, "id": "8509", "date": "2018-10-19T07:15:05.473", "score": 5, "comment_count": 8, "tags": ["neural-networks", "machine-learning", "deep-learning", "convolutional-neural-networks", "computer-vision"], "title": "Neural Network for Optical Mark Recognition?", "answer_count": 3, "views": 1087, "accepted_answer": null, "answers": {"8512": {"line": 5736, "body": "I'm not familiar with ConvNetSharp library, and the tag convolutional-neural-networks is a bit confusing me, but from :\n\nSo I've created a neural net using the ConvNetSharp library which has 3 fully connected hidden layers. The first having 35 neurons and the other two having 25 neurons, each with a ReLU layer as the activation function layer.\n\nI assume you are building just a densely connected neural network. Correct me if I'm wrong.\n\nThe type of neural network you need is Convolutional Neural Network.\nFor image recognition (which is your case), convolutional network are almost always the answer.\nThere is plenty of type of CNN, just pick one that seems appropriate and try.\nIn my opinion, your task seems quite simple, you won't need really deep / complex architecture.\n\n\nIt would be great if there could be a solution out there that could store a background/template using a camera and then when the camera sees that image again\n\nI am not aware of a model that could do what you are asking.\nBut what you are asking is not really in the 'neural network' mindset. The goal of building a neural network is that you don't specify anything. The neural network will learn and find the features for you. So you just have to feed him a lot of data, and he will be able to recognize your pattern.\nTake a look at this visualization of CNN filters :\n\nHere, no one gave the neural network the template of a nose or the template of an eye or the template of a face. The CNN learned it over a lot of image.\n", "type": 2, "id": "8512", "date": "2018-10-19T08:31:48.033", "score": 0, "comment_count": 2, "parent_id": "8509"}, "21451": {"line": 14188, "body": "From what I understand, don't bother with a CNN, you have essentially perfectly structured images.\nYou can hand code detectors to measure how much filled in a circle is.\nBasically do template alignment and then search over the circles.\nEx a simple detector would measure the average blackness of the circle which you could then threshold.\n", "type": 2, "id": "21451", "date": "2020-05-25T04:50:46.797", "score": 1, "comment_count": 0, "parent_id": "8509"}, "20583": {"line": 13497, "body": "Is it the case that one of the numbers if filled in? If so a CNN with 10 output should work well. Just choose the output that has the highest probability. If your data allows no number to be filled in, then have 11 outputs where the eleventh output indicates none is filled in. I would recommend transfer learning using the MobileNet model. Documentation is here. Here is the code to adapt MobileNet to your problem:\nimage_size=128\nno_of_classes=10  # set to 11 if in some cases no numbers are filled in\nlr_rate=.001\ndropout=.4\nmobile = tf.keras.applications.mobilenet.MobileNet( include_top=False,\n                                                           input_shape=(image_size,image_size,3),\n                                                           pooling='avg', weights='imagenet',\n                                                           alpha=1, depth_multiplier=1)\nx=mobile.layers[-1].output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx=Dense(128, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=dropout, seed=128)(x)\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\npredictions=Dense (no_of_classes, activation='softmax')(x)\nmodel = Model(inputs=mobile.input, outputs=predictions)    \nfor layer in model.layers:\n    layer.trainable=True\nmodel.compile(Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy']) \n\nI would also create a training, test and validation set with about 150 images in the test set and 150 images in the validation set leaving 1700 images for training. I also recommend you use two useful callbacks. Documentation is here. Use the ReduceLROnPlateau callback to monitor the validation loss and adjust the learning rate downward by a factor. Use ModelCheckpoint to monitor the validation loss and save the model with the lowest validation loss to use to make predictions on the test set.\n", "type": 2, "id": "20583", "date": "2020-04-23T23:01:35.317", "score": 0, "comment_count": 0, "parent_id": "8509"}}}
{"line": 3519, "body": "I'm training Seq2Seq model on OpenSubtitles dialogs - Cornell-Movie-Dialogs-Corpus.\nMy work based on the following papers (but currently I'm not implemented Attention yet):\n\nSequence to Sequence Learning with Neural Networks, Sutskever et al. 2014\nA Neural Conversational Model, Vinyals, Le, 2015\n\nThe loss I received is quite high and sucked in variation ~6.4 after 3 epoches. The model predicts the most common words with some times other not significant words (but 99.99% is just 'you'):\n\nI've experimented with 128 - 2048 hidden units and with 1 or 2 or 3 LSTM layers per encoder and decoder. The outcomes are more or less the same.\n\n\nSEQ1: yeah man it means love respect community and the dollars too the package the unk end\nSEQ2: but how did you get unk 82 end\nPREDICTION: promoting 16th dashboard be of the the the you you you you you you you you you you you you you you you you you you you you you you you you\n\nI'm using here greedy prediction, meaning - after I receive logit I do argmax(..) on all its value for first-3 mini-batch-elements (here I present only first element). For convenient - SEQ1 and SEQ2 are also printed - to know the actual dialog which was presented to the model.\nThe pseudo-code of my architecture looks like this (I'm using Tensorflow 1.5):\nseq1 = tf.placeholder(...)\nseq2 = tf.placeholder(...)\n\nembeddings = tf.Variable(tf.random_uniform([vocab_size, 100],-1,1))\n\nseq1_emb = tf.nn.embedding_lookup(embeddings, seq1)\nseq2_emb = tf.nn.embedding_lookup(embeddings, seq1)\n\nencoder_out, state1 = tf.nn.static_rnn(BasicLSTMCell(), seq1_emb)\ndecoder_out, state2 = tf.nn.static_rnn(BasicLSTMCell(), seq2_emb,\n                                                        initial_state=state_1)\nlogit = Dense(decoder_out, use_bias=False)\n\ncrossent = tf.nn.saparse_softmax_cross_entropy_with_logits(logits=logit, \n                                                         labels=target)\ncrossent = mask_padded_zeros(crossent)\nloss = tf.reduce_sum(crossent) / number_of_words_in_batch\n\ntrain = tf.train.AdamOptimizer(learning_rate=0.00002).minimize(loss) \n\nI'm also wonder if I pass well state1 to decoder, which in general looks like this:\n# reshape in pseudocode: state1 = state[1:]\nnew_state1 = []\nfor lstm in state1:\n    new_lstm = []\n    for gate in lstm:\n        new_lstm.append(gate[1:])\n    new_state1.append(tuple(new_lstm))\nstate1 = tuple(new_state1)\n\n\nShould I use some projection layer between states of encoder and decoder ?\n\nSo if seq1 has 32 words, seq2 has 31 (since we will not predict nothing after the last word, which is the tag <END>).\n", "type": 1, "id": "5536", "date": "2018-03-06T10:35:34.890", "score": 3, "comment_count": 0, "tags": ["neural-networks", "natural-language-processing", "tensorflow", "recurrent-neural-networks", "long-short-term-memory"], "title": "Seq2Seq dialogs predicts only most common words like `you` after couple of epoches", "answer_count": 1, "views": 1484, "accepted_answer": "7554", "answers": {"7554": {"line": 5036, "body": "To answer my own question - it was because of 2 things: \n\nToo small number of batches - Model just started to gain statistical knowledge about language dialogs. I needed to train it longer.\nI masked sequence too early (wrongly removed the < END > tag) - Because on each sentence the last world is just < END > tag - I removed it on all training examples, which prevented Model from learn what does it mean \"the end of the sentence\".\n\nThe last condition probably caused that strange pattern even further, because if model doesn't know what word to put in (and because of the lack of the < END > tag) it must fill each sentence till the end of max_sequence_len. \nSo the Model inputed in loop-like-manner - one of the most common word (where there was no signal from target sentence, because it simply ended).\n", "type": 2, "id": "7554", "date": "2018-08-14T12:27:44.073", "score": 0, "comment_count": 0, "parent_id": "5536"}}}
{"line": 4086, "body": "I'm building a 5-class classifier with a private dataset. Each data sample has 67 features and there are about 40000 samples. Samples of a particular class were duplicated to overcome class imbalance problems (hence 40000 samples). \nWith a one-vs-one multi-class SVM, I am getting an accuracy of ~79% on the validation set. The features were standardized to get 79% accuracy. Without standardization, the accuracy I get is ~72%. Similar result when I tried 50-fold cross validation.\nNow moving on to MLP results,\nExp 1:\n\nNetwork Architecture: [67 40 5]\nOptimizer: Adam\nLearning Rate: exponential decay of base learning rate\nValidation Accuracy: ~45%\nObservation: Both training accuracy and validation accuracy stops improving.\n\nExp 2:\nRepeated Exp 1 with batchnorm layer\n\nValidation Accuracy: ~50%\nObservation: Got 5% increase in accuracy.\n\nExp 3:\nTo overfit, increased the depth of MLP. A deeper version of Exp 1 network\n\nNetwork Architecture: [67 40 40 40 40 40 40 5]\nOptimizer: Adam\nLearning Rate: exponential decay of base learning rate\nValidation Accuracy: ~55%\n\nThoughts on what might be happening? \n", "type": 1, "id": "6289", "date": "2018-05-06T14:37:08.953", "score": 3, "comment_count": 4, "tags": ["neural-networks", "deep-learning", "overfitting", "multilayer-perceptrons"], "title": "Unable to overfit using MLP", "answer_count": 2, "views": 413, "accepted_answer": null, "answers": {"6290": {"line": 4087, "body": "I guess you are using linear activation functions,  maybe you are not initializing your weights, or you are regularizing your model enough.  \nInitialize weights with glorot, insert dropout layers in between, use Relu as your activation function, stop the training process based on Early Stopping. and just experiment with one hidden layer.\nside note: if you use Adam, don't mess with the learning rate.\nin SGD optimizer you could use decay because there is a single learning rate for all weight updates and the learning rate does not change during training.\nIn Adam, the learning rate is maintained for each network weight (parameter) and separately adopted as learning unfolds. \n", "type": 2, "id": "6290", "date": "2018-05-06T16:00:36.640", "score": 0, "comment_count": 1, "parent_id": "6289"}, "27445": {"line": 18831, "body": "For the multiclass SVM, there will be an ensembling effect since you are learning 5*4=20 1vs1 classifiers. It could be an interesting experiment to try the same thing with simple neural networks. Also, since you are standardizing the inputs you could try tanh activations on the first layer after the input. I presume you are using softmax on the output layer.\n", "type": 2, "id": "27445", "date": "2021-04-21T04:54:42.600", "score": 0, "comment_count": 0, "parent_id": "6289"}}}
{"line": 5009, "body": "I've been wanting to make my own Neural Network in Python, in order to better understand how it works. I've been following this series of videos as a sort of guide, but it seems the backpropagation will get much more difficult when you use a larger network, which I plan to do. He doesn't really explain how to scale it to larger ones.\nCurrently, my network feeds forward, but I don't have much of an idea of where to start with backpropagation. My code is posted below, to show you where I'm currently at (I'm not asking for coding help, just for some pointers to good sources, and I figure knowing where I'm currently at might help):\nimport numpy\n\nclass NN:\n    prediction = []\n    def __init__(self,input_length):\n        self.layers = []\n        self.input_length = input_length\n    def addLayer(self, layer):\n        self.layers.append(layer)\n        if len(self.layers) >1:\n            self.layers[len(self.layers)-1].setWeights(len(self.layers[len(self.layers)-2].neurons))\n        else:\n            self.layers[0].setWeights(self.input_length)\n    def feedForward(self, inputs):\n        _inputs = inputs\n        for i in range(len(self.layers)):\n            self.layers[i].process(_inputs)\n            _inputs = self.layers[i].output\n        self.prediction = _inputs\n\n    def calculateErr(self, target):\n        out = []\n        for i in range(0,len(self.prediction)):\n            out.append(  (self.prediction[i] - target[i]) ** 2  )\n        return out\n        \n\nclass Layer:\n\n    neurons = []\n    weights = []\n    biases = []\n    output = []\n    \n    def __init__(self,length,function):\n        for i in range(0,length):\n            self.neurons.append(Neuron(function))\n            self.biases.append(numpy.random.randn())\n\n    def setWeights(self, inlength):\n        for i in range(0,inlength):\n            self.weights.append([])\n            for j in range(0, inlength):\n                self.weights[i].append(numpy.random.randn())\n    \n    def process(self,inputs):\n        for i in range(0, len(self.neurons)):\n            self.output.append(self.neurons[i].run(inputs,self.weights[i], self.biases[i]))\n    \n\nclass Neuron:\n    output = 0\n    def __init__(self, function):\n        self.function = function\n    def run(self, inputs, weights, bias):\n        self.output = self.function(inputs,weights,bias)\n        return self.output\n\ndef sigmoid(n):\n    return 1/(1+numpy.exp(n))\n\n\ndef inputlayer_func(inputs,weights,bias):\n    return inputs\n\ndef l2_func(inputs,weights,bias):\n    out = 0\n    \n    for i in range(0,len(inputs)):\n        out += weights[i] * inputs[i]\n    out += bias\n    \n    return sigmoid(out)\n\nNNet = NN(2)\n\n\nl2 = Layer(1,l2_func)\n\n\nNNet.addLayer(l2)\nNNet.feedForward([2.0,1.0])\nprint(NNet.prediction)\n\nSo, is there any resource that explains how to implement the back-propagation algorithm step-by-step?\n", "type": 1, "id": "7522", "date": "2018-08-11T23:41:14.547", "score": 4, "comment_count": 0, "tags": ["neural-networks", "reference-request", "backpropagation", "implementation", "resource-request"], "title": "How can I implement back-propagation for medium-sized neural networks?", "answer_count": 2, "views": 101, "accepted_answer": "7545", "answers": {"7545": {"line": 5028, "body": "Backpropagation isn't too much more complicated, but understanding it well will require a bit of mathematics.\nThis tutorial is my go-to resource when students want more detail, because it includes fully worked through examples.\nChapter 18 of Russell & Norvig's book includes pseudocode for this algorithm, as well as a derivation, but without good examples.\n", "type": 2, "id": "7545", "date": "2018-08-13T15:05:05.983", "score": 2, "comment_count": 1, "parent_id": "7522"}, "25343": {"line": 17152, "body": "Nowadays, there are many resources that cover the back-propagation algorithm and some of them provide step-by-step examples.\nHowever, in addition to the other answer, I would like to mention the online book Neural Networks and Deep Learning by Nielsen that covers the back-propagation algorithm (and other topics) in detail and, at the same, intuitively, although some could disagree. You can find the associated source code here (which I had consulted a few years ago when I was learning about the topic).\n", "type": 2, "id": "25343", "date": "2020-12-23T00:10:20.207", "score": 0, "comment_count": 0, "parent_id": "7522"}}}
{"line": 5749, "body": "Should the weights of a neural network be updated after each example or at the end of the batch? Do I need a normalization factor in the second case?\n", "type": 1, "id": "8527", "date": "2018-10-19T19:53:17.767", "score": 2, "comment_count": 1, "tags": ["neural-networks", "backpropagation", "gradient-descent", "feedforward-neural-networks"], "title": "Should the weights of a neural network be updated after each example or at the end of the batch?", "answer_count": 2, "views": 1145, "accepted_answer": null, "answers": {"8540": {"line": 5756, "body": "The backpropagation step is generally used to compute the gradients and update the weights. \nLet us say, you are implementing gradient descent, select the whole training batch, perform the forward propagation using the current set of neurons/weights to get the classification output. Then compute the loss/cost with respect to the actual labels, (Note: This step contains dividing the loss and divide it by the number of training example to get the cost function). Once you get the cost you backpropagate by computing the gradients at each layer with respect to cost from output to input and perform the weight updates. \nIf you are using mini-batch, you compute the cost for that mini-batch, divide by the number of examples in the mini-batch, then compute the gradients and perform the update in the backprop step. \n", "type": 2, "id": "8540", "date": "2018-10-20T21:35:35.787", "score": 1, "comment_count": 2, "parent_id": "8527"}, "8541": {"line": 5757, "body": "\nWhen am I supposed to update my weights? After each forward-, and backpropagation; and or after each completed batch?\n\n\nIn your example, you should update the weights after each back propagation.\nAlso, you can back propagate the result by adding each error/loss for each prediction/example in a batch and then update the weights. pseudocode- for batch size 3:\nw = backprop(Error(t1)+Error(t2)+Error(t3))\nupdate_wights(w*learning_rate)\nNow what happen If you back propagate every error for every prediction/example in a batch and store them(without updating), and after completion of that batch update the  weights by using stored calculated values, then there is a chance to stuck in the local minima also some research shows that it's actually slow training. research paper\n\nnote- all those techniques in 1,2,3 are different but 1 and 2 results same. Also in 2, adding errors of different training example makes a new graph of neural network and back propagating it means back propagating the resulted graph.\n", "type": 2, "id": "8541", "date": "2018-10-20T23:41:33.907", "score": 0, "comment_count": 0, "parent_id": "8527"}}}
{"line": 5167, "body": "I am trying to understant how it works.  How do you teach it say, to add 1 to each number it gets.  I am pretty new to the subject and I learned how it works when you teach it to identify a picture of a number.  I can understand how it identifies a number but I cant get it how would it study to perform addition? I can understand that it can  identify a number or picture using the pixels and assigning weights and then learning to measure whether a picture of a number resembling the weight is assigned to each pixel. But i can't logically understand how would it learn the concept  of adding a number by one.  Suppose I showed it thousands of examples of 7 turning to 8 152 turning into 153 would it get it that every number in the world has to be added by one? How would it get it having no such operation of + ? Since addition does not exist to its proposal then how can it realize that it has to add one in every number? Even by seeing thousands of examples but having no such operation of plus I cant understand it.    I could understand identifying pixels and such but such an operation I cant get the theoretical logic behind it. Can you explain the logic in layman terms?\n", "type": 1, "id": "7727", "date": "2018-08-27T10:18:17.893", "score": 5, "comment_count": 2, "tags": ["neural-networks", "deep-learning"], "title": "How is it possible to teach a neural network to perform addition?", "answer_count": 2, "views": 1395, "accepted_answer": null, "answers": {"7732": {"line": 5171, "body": "This is what we call a regression problem. Although @John has provided a novel method I do not think it will work since you are decomposing the number into its minimal representation, so teaching it will be quite tough due to long term dependencies like 0111111 will change to 100000, so you have to train on almost all examples, with 0 actual learning.\nLet's see your problem from a different viewpoint. Why are you thinking only of integers? Your problem can be generalised to approximating this curve:\n\nThis is clearly an example of $W.T * X + B$. Single input node will feed into 2 output nodes of Leaky ReLu activation, and a bias node. The bias node will be your $c$ and the 2-3 Leaky ReLu's will adjust their weights to create a straight line. Training this might be a problem (with co-adaptation between nodes), but mathematically a solution will be achieved by this Neural Net structure.\nAlso it is better to train on real values also, for better and finer weight adjustment (although theoretically for a single independent variable $x$ you should just need 3-4 values to make this Neural Net learn, but who knows?)\nNOTE: The approximation in the negative region might not be that great.\n", "type": 2, "id": "7732", "date": "2018-08-27T13:47:39.293", "score": 0, "comment_count": 0, "parent_id": "7727"}, "7728": {"line": 5168, "body": "Welcome to AI.SE @bilanush.\nHere's an example approach that might make things clearer. There are other ways to train a neural network to do this however.\nIn your earlier example with an image, you probably noticed that the network receives the image as a series of values, representing each pixel in the image. The network then learns which of a series of output neurons should be active in response to a given set of pixel values. Those output neurons, when read in an appropriate way, correspond to the correct label for the image. The difference between the set of outputs that should have been active, and the set that were active, forms the basis of the error signal that allows the network to learn.\nYou've probably heard that computers represent numbers with binary digits. So you could think of the number 16 as being: 00010000 in \"8-bit binary\". In 16-bit binary, this number would be 0000000000010000, and so on.\nSo one way of viewing your problem is a function mapping binary inputs to binary outputs (very similar to labelling a black-and-white image). For instance, the input 00010000 (16) should produce the output 00010001 (17). The input 00100011 (35) should produce the output 00100100 (36), and so on.\nAs before, you will have a set of output neurons. In this case, it should be wide as the set of input neurons. As before, the error signal is the difference between the expected inputs and outputs.\nAs to the question of how they can learn this function \"without plus\", in fact the individual neurons in a network perform just two operations: addition of their inputs, and a non-linear transformation of the sum. It has been proven that these are sufficient to learn any function from inputs to outputs, as long as the network contains 3 layers or more, and the as long as the middle layer is wide enough, but here it should be easy to see how addition might emerge.\n", "type": 2, "id": "7728", "date": "2018-08-27T12:27:25.177", "score": 1, "comment_count": 2, "parent_id": "7727"}}}
{"line": 4788, "body": "Is it possible to make a neural network that uses only integers by scaling input and output of each function to [-INT_MAX, INT_MAX]? Is there any drawbacks?\n", "type": 1, "id": "7247", "date": "2018-07-22T14:12:51.643", "score": 13, "comment_count": 0, "tags": ["neural-networks", "machine-learning"], "title": "Why do we need floats for using neural networks?", "answer_count": 4, "views": 8173, "accepted_answer": "7323", "answers": {"7250": {"line": 4791, "body": "Some people might argue we can use int instead of float in NN's as float can easily be represented as anint / k where k is a multiplying factor say 10 ^ 9 e.g 0.00005 can be converted to 50000 by multiplying with 10 ^ 9..\nFrom a purely theoretical viewpoint: This is definitely possible, but it will result in a loss of precision since int falls in the INTEGER set of number whereas floats fall in the REAL NUMBER set. Converting real numbers to int's will result in high precision loss if you are using very high precisions e.g. float64. Real numbers have an uncountable infinity, whereas integers have countable infinity and there is a well known argument called Cantor's diagonalization argument which proves this. Here is a beautiful illustration of the same. After understanding the difference you will intuitionally gain an understanding why converting int's to float is not tenable.\nFrom a practical viewpoint: The most well known activation activation function is sigmoid activation (tanh is also very similar). The main property of these activatons are they squash numbers to between 0 and 1 or -1 and 1. If you convert floating point to a integer by multiplying with a large factor, which will result in a large number almost always, and pass it to any such function the resulting output will always almost be either of the extremities (i.e 1 or 0).\nComing to algorithms, algorithms which are similar to backpropagation with momentum cannot run on int. This is because,s since you will be scaling the int to a large number, and momentum algorithms typically use some sort of momentum_factor^n formulae, where n is the number of examples iterated already, you can imagine the result if momentum_factor = 100 and n = 10.\nOnly possible place where scaling might work is for relu activation. the problems with this approach will be if the data will probably not fit very good in this model, there will be relatively high errors.\nFinally: All NN's do is approximate a real valued function. You can try to magnify this function by multiplying it with a factor, but whenever you are switching from, real valued function to integers you are basically representing the function as a series of steps. Something like this happens (Image only for representation purposes):\n\nYou can clearly see the problem here, each binary number represent a step, to have better accuracy you have to increase the binary steps within a given length, which in your problem will translate to having very high value of bounds [-INT_MAX, INT_MAX].\n", "type": 2, "id": "7250", "date": "2018-07-22T16:57:06.117", "score": 2, "comment_count": 10, "parent_id": "7247"}, "7323": {"line": 4849, "body": "TL;DR\nNot only it's possible, it even gets done and is commercially available. It's just impractical on a commodity HW which is pretty good at FP arithmetic.\nDetails\nIt is definitely possible, and you might get some speed for a lot of troubles.\nThe nice thing about floating point is that it works without you knowing the exact range. You can scale your inputs to the range (-1, +1) (such a scaling is pretty commonplace as it speeds up the convergence) and multiply them by 2**31, so they used the range of signed 32-bit integers. That's fine.\nYou can't do the same to your weights as there's no limit on them. You can assume them to lie in the interval (-128, +128) and scale them accordingly.\nIf your assumption was wrong, you get an overflow and a huge negative weight where a huge positive weight should be or the other way round. In any case, a disaster.\nYou could check for overflow, but this is too expensive. Your arithmetic gets slower than FP.\nYou could check for possible overflow from time to time and take a corrective action. The details may get complicated.\nYou could use saturation arithmetic, but it's implemented only in some specialized CPUs, not in your PC.\nNow, there's a multiplication. With use of 64-bit integers, it goes well, but you need to compute a sum (with a possible overflow) and scale the output back to same sane range (another problem).\nAll in all, with fast FP arithmetic available, it's not worth the hassle.\nIt might be a good idea for a custom chip, which could do saturation integer arithmetic with much less hardware and much faster then FP.\n\nDepending on what integer types you use, there may be a precision loss when compared to the floating point, which may or may not matter. Note that TPU (used in AlphaZero) has 8-bit precision only.\n", "type": 2, "id": "7323", "date": "2018-07-28T22:42:14.073", "score": 6, "comment_count": 0, "parent_id": "7247"}, "7249": {"line": 4790, "body": "It is possible in principle, but you will end up emulating floating point arithmetic using integers in multiple places, so it is unlikely to be efficient for general use. Training is likely to be an issue.\nIf the output of a layer is scaled to [-INT_MAX, INT_MAX], then you need to multiply those values by weights (which you will also need to be integers with a large enough range that learning can progress smoothly) and sum them up, and then feed them into a non-linear function. \nIf you are restricting yourself to integer operations only, this will involve handling multiple integers to represent high/low words in a larger int type, that you then must scale (which introduces a multi-word integer division). By the time you have done this, it is unlikely there will be much benefit to using integer arithmetic. Although perhaps it is still possible, depending on the problem, network architecture and your target CPU/GPU etc.\nThere are instances of working neural networks used in computer vision with only 8-bit floating point (downsampled after training). So it is definitely possible to simplify and approximate some NNs. An integer-based NN derived from a pre-trained 32-bit fp one could potentially offer good performance in certain embedded environments. I found this experiment on a PC AMD chip which showed a marginal improvement even on PC architecture. On devices without dedicated fp processing, the relative improvement should be even better.\n", "type": 2, "id": "7249", "date": "2018-07-22T16:41:10.763", "score": 0, "comment_count": 2, "parent_id": "7247"}, "7326": {"line": 4850, "body": "Floating Point Hardware\nThere are three common floating point formats used to approximate real numbers used in digital arithmetic circuitry.  These are defined in IEEE 754, a standard that was adopted in 1985 with a revision in 2008.  These mappings of bit-wise layouts real number representations are designed into CPUs, FPUs, DSPs, and GPUs either in gate level hardware, firmware, or libraries1.\n\nbinary 32 has a 24 bit mantissa and an 8 bit exponent\nbinary 64 has a 53 bit mantissa and an 11 bit exponent\nbinary 128 has a 113 bit mantissa and a 15 bit exponent2\n\nFactors in Choosing Numerical Representations\nAny of these can represent signals in signal processing, and all have been experimented with in AI for various purposes related to three things:\n\nValue range \u2014 not a concern in ML applications where the signal is properly normalized\nAverting saturation of the signal with rounding noise \u2014 a key issue in parameter adjustment\nTime required to execute an algorithm on a given target architecture\n\nThe balance in the best designed AI is between these last two items.  In the case of back-propagation in neural nets, the gradient-based signal that approximates the desired corrective action to apply to the attenuation parameters of each layer must not become saturated with rounding noise.3\nHardware Trends Favor Floating-point\nBecause of the demand of certain markets and common uses, scalar, vector, or matrix operations using these standards may, in certain cases, be faster than integer arithmetic.  These markets include ...\n\nClosed loop control (piloting, targeting, countermeasures)\nCode breaking (Fourier, finite, convergence, fractal)\nVideo rendering (movie watching, animation, gaming, VR)\nScientific computing (particle physics, astrodynamics)\n\nFirst Degree Transforms to Integers\nOn the opposing end of numerical range, one can represent signals as integers (signed) or non-negative integers (unsigned).\nIn this case, the transformation between the set of real numbers, vectors, matrices, cubes, and hyper-cubes in the world of calculus4 and the integers that approximate them is a first degree polynomial.\nThe polynomial can be represented as $n = r(as + b)$, where $a \\ne 0$, $n$ is the binary number approximation, $s$ is the scalar real, and $r$ is the function that rounds real numbers to the nearest integer.  This defines a super-set of the concept of fixed point arithmetic because of $b$.\nInteger based calculations have also been examined experimentally for many AI applications.  This gives more options:\n\ntwo's complement 16 bit integer\n16 bit non-negative integer\ntwo's complement 32 bit integer\n32 bit non-negative integer\ntwo's complement 64 bit integer\n64 bit non-negative integer\ntwo's complement 128 bit integer\n128 bit non-negative integer\n\nExample Case\nFor instance, if your theory indicates the need to represent the real numbers in the range $[-\\pi, \\pi]$ in some algorithm, then you might represent this range as a 64 bit non-negative integer (if that works to the advantage of speed optimization for some reason that is algorithm and possibly hardware specific).\nYou know that $[-\\pi, \\pi]$ in the closed form (algebraic relation) developed from the calculus needs to be represented in the range $[0, 2^64 - 1]$, so in $n = r(as + b)$, $a = 2^61$ and $b = 2^60$. Choosing $a = \\frac {2^64} {\\pi}$ would likely create the need for more lost cycles in multiplication when a simple manipulation of the base two exponent is much more efficient.\nThe range of values for that real number would then be [0, 1100,1001,0000,1111,1101,1010,1010,0010,0010,0001,0110,1000,1100,0010,0011,0101] and the number of bits wasted by keeping the relationship based on powers of two will be $\\log_2 4 - log_2 \\pi$, which is approximately 0.3485 bits.  That's better than 99% conservation of information.\nBack to the Question\nThe question is a good one, and is hardware and domain dependent.\nAs mentioned above hardware is continuously developing in the direction to work with IEEE floating point vector and matrix arithmetic, particularly in 32 and 64 bit floating point multiplication.  For some domains and execution targets (hardware, bus architecture, firmware, and kernel), the floating point arithmetic may grossly out perform what performance gains can be obtained in 20th century CPUs by applying the above first degree polynomial transformation.\nWhy the Question is Relevant\nIn contract, if the product manufacturing price, the power consumption, and PC board size and weight must be kept low to enter certain consumer, aeronautic, and industrial markets, the low cost CPUs may be demanded. By design, these smaller CPU architectures do not have DSPs and the FPU capabilities don't usually have hardware realization of 64 bit floating point multiplication.5\nHandling Number Ranges\nCare in normalizing signals and picking the right values for a and b are essential, as mentioned, more so than with floating point, where the diminution of the exponent can eliminate many cases where saturation would be an issue with integers6.  Augmentation of the exponent can avert overflow automatically too, up to a point, of course.\nIn either type of numeric representation, normalizing is part of what improves convergence rate and reliability anyway, so it should always be addressed.\nThe only way to deal with saturation in applications requiring small signals (such as with gradient descent in back-propagation, especially when the data set ranges are over an order of magnitude) is to carefully work out the mathematics to avoid it.\nThis is a science by itself, and only a few people have the scope of knowledge to handle hardware manipulation at the circuitry and assembly language level along with the linear algebra, calculus, and machine learning comprehension.  The interdisciplinary skill set is rare and valuable.\n\nNotes\n[1] Libraries for low level hardware operations such as 128 bit multiplication are written in cross assembly language or in C with the -S option turned on so the developer can check the machine instructions.\n[2] Unless you are calculating the number of atoms in the universe, the number of permutations in possible game-play for the game Go, the course to a landing pad in a crater on Mars, or the distance in meters to reach a potentially habitable planet revolving around Proxima Centauri, you will likely not need larger representations than the three common IEEE floating point representations.\n[3] Rounding noise naturally arises when digital signals approach zero and the rounding of the least significant bit in the digital representation begins to produce chaotic noise of a magnitude that approaches that of the signal.  When this happens, the signal is saturated in that noise and cannot be used to reliably communicate the signal.\n[4] The closed forms (algebraic formulae) to be realized in software driven algorithms arise out of the solution of equations, usually involving differentials.\n[5] Daughter boards with GPUs are often too pricey, power hungry, hot, heavy, and/or packaging unfriendly in non-terrestrial and consumer markets.\n[6] The zeroing of feedback is skipped in this answer because it points to either one of two things: (A) Perfect convergence or (B) Poorly resolved mathematics.\n", "type": 2, "id": "7326", "date": "2018-07-29T01:26:28.747", "score": 2, "comment_count": 0, "parent_id": "7247"}}}
{"line": 3957, "body": "Does the human brain use a specific activation function? I've tried doing some research, and as it's a treshold for whether the signal is sent through a neuron or not, it sounds a lot like ReLU. However, I can't find a single article confirming this. Or is it more like a step function (it sends 1 if it's above the treshold, instead of the input value).\n", "type": 1, "id": "6099", "date": "2018-04-18T10:36:13.770", "score": 15, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "brain"], "title": "What activation function does the human brain use?", "answer_count": 4, "views": 3359, "accepted_answer": "6104", "answers": {"6101": {"line": 3958, "body": "The answer is We do not know. Odds are, we will not know for quite a while. The reason for this is we cannot understand the \"code\" of the human brain, nor can we simply feed it values and get results. This limits us to measuring currents of the input and output on test subjects, and we have had few such test subjects that are human. Thus, we know almost nothing about the human brain, including the activation function.\n", "type": 2, "id": "6101", "date": "2018-04-18T11:58:55.093", "score": 1, "comment_count": 0, "parent_id": "6099"}, "6104": {"line": 3960, "body": "The thing you were reading about is known as the action potential. It is a mechanism that governs how information flows within a neuron.\nIt works like this: Neurons have an electrical potential, which is a voltage difference inside and outside the cell. They also have a default resting potential, and an activation potential. The neuron tends to move towards the resting potential if it is left alone, but incoming electric activations from dendrites can shift its electric potential. \nIf the neuron reaches a certain threshold in electric potential (the activation potential), the entire neuron and its connecting axons goes through a chain reaction of ionic exchange inside/outside the cell that results in a \"wave of propagation\" through the axon.\nTL;DR: Once a neuron reaches a certain activation potential, it electrically discharges. But if the electric potential of the neuron doesn't reach that value then the neuron does not activate.\n\nDoes the human brain use a specific activation function? \n\nIIRC neurons in different parts of the brain behave a bit differently, and the way this question is phrased sounds as if you are asking if there is a specific implementation of neuronal activation (as opposed to us modelling it). \nBut in general behave relatively similar to each other (Neurons communicate with each other via neurochemicals, information propagates inside a neuron via a mechanism known as the action potential...) But the details and the differences they cause could be significant.\nThere are various biological neuron models, but the Hodgkin-Huxley Model is the most notable. \nAlso note that a general description of neurons don't give you a general description of neuronal dynamics a la cognition (understanding a tree doesn't give you complete understanding of a forest)\nBut, the method of which information propagates inside a neuron is in general quite well understood as sodium / potassium ionic exchange.\n\nIt (activation potential) sounds a lot like ReLU...\n\nIt's only like ReLU in the sense that they require a threshold before anything happens. But ReLU can have variable output while neurons are all-or-nothing. \nAlso ReLU (and other activation functions in general) are differentiable with respect to input space. This is very important for backprop.\nThis is a ReLU function, with the X-axis being input value and Y-axis being output value.\n\nAnd this is the action potential with the X-axis being time, and Y being output value.\n", "type": 2, "id": "6104", "date": "2018-04-18T14:21:39.670", "score": 14, "comment_count": 3, "parent_id": "6099"}, "7538": {"line": 5022, "body": "The brains of mammals do not use an activation function.  Only machine learning designs based on the perceptron multiply the vector of outputs from a prior layer by a parameter matrix and pass the result statelessly into a mathematical function.\nAlthough the spike aggregation behavior has been partly modeled, and in far more detail than the 1952 Hodgkin and Huxley model, all the models require statefulness to functionally approximate biological neurons.  RNNs and their derivatives are an attempt to correct that shortcoming in the perceptron design.\nIn addition to that distinction, although the signal strength summing into activation functions are parametrized, traditional ANNs, CNNs, and RNNs, are statically connected, something Intel claims they will correct with the Nirvana architecture in 2019 (which places into silicon that which we would call layer set up in Python or Java now.\nThere are at least three important biological neuron features that make the activation mechanism more than a function of a scalar input producing a scalar output, which renders questionable any algebraic comparison.\n\nState held as neuroplastic (changing) connectivity, and this is not just how many neurons in a layer but also the direction of signal propagation in three dimensions and the topology of the network, which is organized, but chaotically so\nThe state held within the cytoplasm and its organelles, which is only partly understood as of 2018\nThe fact that there is a temporal alignment factor, that pulses through a biological circuit may arrive via synapses in such a way that they aggregate but the peaks of the pulses are not coincident in time, so the activation probability is not as high as if they were temporally aligned.\n\nThe decision about what activation function to use has largely been based on the analysis of convergence on a theoretical level combined with testing permutations to see which ones show the most desirable combinations of speed, accuracy, and reliability in ctheonvergence.  By reliability is meant that convergence on the global optimum (not some local minimum of the error function) is reached at all for the majority of input cases.\nThis bifurcated research between the forks of practical machine learning and biological simulations and modeling.  The two branches may rejoin at some point with the emergence of spiking - Accuracy\n - Reliability (completes) networks.  The machine learning branch may borrow inspiration from the biological, such as the case of visual and auditory pathways in brains.\nThey have parallels and relationships that may be exploited to aid in progress along both forks, but gaining knowledge by comparing the shapes of activation functions is confounded by the above three differences, especially the temporal alignment factor and the entire timing of brain circuits which cannot be modeled using iterations.  The brain is a true parallel computing architecture, not reliant on loops or even time sharing in the CPU and data buses.\n", "type": 2, "id": "7538", "date": "2018-08-13T00:50:50.507", "score": 4, "comment_count": 0, "parent_id": "6099"}, "13096": {"line": 8985, "body": "My interpretation of the question was 'what activation function in an artificial neural network (ANN) is closest to that found in the brain?'\nWhilst I agree with the selected answer above, that a single neuron outputs a dirac, if you think of a neuron in an ANN as modelling the output firing rate, rather than the current output, then I believe ReLU might be closest?\nhttp://jackterwilliger.com/biological-neural-networks-part-i-spiking-neurons/\n", "type": 2, "id": "13096", "date": "2019-06-27T16:58:59.240", "score": -1, "comment_count": 0, "parent_id": "6099"}}}
{"line": 4258, "body": "How do I decrease the accuracy value when training a model using Keras; which parameters can I change to decrease the value?\nMy objective is not to actually decrease it, but just to know which parameters influence the accuracy\nsgd = optimizers.SGD(lr=1e-2)\n\n", "type": 1, "id": "6503", "date": "2018-05-24T10:26:14.820", "score": 1, "comment_count": 3, "tags": ["neural-networks", "python", "keras"], "title": "How to decrease accuracy from 99% to 80%~85% using keras for training a model", "answer_count": 2, "views": 730, "accepted_answer": "6506", "answers": {"6506": {"line": 4261, "body": "There are many things affecting accuracy. I'm gonna assume a lot here because you don't say anything about the model, what you're trying to achieve or how many classes you have. You're not even saying whether you're classifying or not. Also, you're not saying which accuracy you're using (classification, AUC, F1 etc.).\nI'm gonna assume here that you have some classification problem.\nAccuracy is the measure of how many classifications you got correct. In my experience 99% is a warning sign because it's too good to be true, and a result like that is often due to overfitting. Since this, in my experience, is the main reason you'd actually want the accuracy down, this is what I'm going to assume is your problem.\nOverfitting occurs when you train \"too much\" and the model only learns things that are within your training set, and fails on everything else. That is: it generalized bad.\nTo prevent this there are a number of things you could do;\n1) Data segmentation\nThe most common is to split your data into three bulks: training (~70% of the data), validation(~20%) test (~10%). These percentages are indications and would vary depending on how much data you have, and the balancing. \nThe idea is that you train on the training data, you run the validation through the network, and calculate the accuracy. When this accuracy, call it validation accuracy, is satisfactory, then you stop the training and run the test data through it. The latter accuracy (test accuracy) is the one that most papers publish (combined with AUC and F1 score.\nImportant: When you have split the data into these bulks you should put away the test, and not use it during training at all. You only use this at the very end to do an extra check that you haven't overfitted.\n2) Regularization\nThere are many types of regularization. Two very popular regularization methods for preventing overfitting are the L2-regularization (see previous link) and the dropout methods.\nWithout going into detail, these methods prevent the model weights from becoming too large. This is a good thing since the model won't rely too much on one feature, which in turn attenuates overfitting.\n\nI hope you learned something, and the most important lesson is that you should know what you're doing. If not, you could end up with a model that is not behaving like you thought. In the case of overfitting, you'd end up with a model that only works on your training data, which doesn't really do much good.\nI really recommend the book by Goodfellow: deeplearningbook.org.\n", "type": 2, "id": "6506", "date": "2018-05-24T18:22:37.393", "score": 1, "comment_count": 0, "parent_id": "6503"}, "18050": {"line": 12239, "body": "If you want to decrease the accuracy given the same optimizer/epochs/batch, you could add more layers and increase the number of parameters, it should now take a bit longer to converge. Hence for the same epochs you would get lower accuracy. You could also initialise your parameters in a non sensical way. \n", "type": 2, "id": "18050", "date": "2020-02-16T17:40:27.673", "score": 0, "comment_count": 0, "parent_id": "6503"}}}
{"line": 6260, "body": "I am wondering how the output of randomly initialized MLPs and ConvNets behave with respect to their inputs.  Can anyone point to some analysis or explanation of this?\nI am curious about this because in the Random Network Distillation work from OpenAI, they use the output of randomly initialized network to generate intrinsic reward for exploration.  It seems that this assumes that similar states will produce similar outputs of the random network.  Is this generally the case?  \nDo small changes in input yield small changes in output, or is it more chaotic?  Do they have other interesting properties?\n", "type": 1, "id": "9354", "date": "2018-12-06T00:27:48.983", "score": 4, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks"], "title": "How do randomly initialized neural networks behave?", "answer_count": 2, "views": 191, "accepted_answer": null, "answers": {"9779": {"line": 6517, "body": "In the Large-Scale Study of Curiosity-Driven Learning paper (the prequel to the Random Network Distillation work), in their discussion of Random Features, they reference 3 papers that discuss this:\n\nK. Jarrett, K. Kavukcuoglu, Y. LeCun, et al. What is the Best Multi-Stage Architecture for Object Recognition?\nA. M. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. Suresh, and A. Y. Ng. On Random Weights and Unsupervised Feature Learning\nZ. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song, and Z. Wang. Deep Fried Convnets\n\nI just briefly glanced over these.  For now, one interesting idea from [2] is to use randomly initialized networks for architecture search.  To evaluate the architecture for the task, you don't have to train it; you can just randomly initialize it and measure its performance.\n", "type": 2, "id": "9779", "date": "2019-01-01T18:42:39.130", "score": 0, "comment_count": 0, "parent_id": "9354"}, "9384": {"line": 6277, "body": "The output of randomly initialised neural networks will be random. I am not sure if I understand your question correctly but I would like to try and explain what OpenAI is trying to achieve with their RND Distillation approach.\nAny re-enforcement or supervised \"learning\" algorithm such as a neural network (NN) with back-propagation, genetic algorithm, partial swarm etc requires a fitness function. The fitness function \"directs\" the algorithm to a solution. \nThe basic idea is that the algorithm is randomly initialised. The algorithm is run, and then parts of the algorithm are changed. In this case the weights of the NN.\nA number of algorithms exist to update the weights, the most common back propagation / gradient decent as used in the paper. To update the weights the output of the NN is compared to the optimum output. Sometimes the optimum is known. Other times it is not. In the case of a game the optimum output is not known. It is assumed that the more points gained the better the NN does. In this case the fitness function or policy is the sum of points gained when playing the game. But gradient descent needs an error (i.e difference between output and expected output), which does not exist when playing a game, because you don't know what the points should be. However the NN could play one round of the game score some points, then run the NN again on the next move to determine a prediction of future points. The difference between the points gained and the prediction is then the error, which gradient descent can use. [Hope this makes sense]\nThis is fine for games like pong, a NN could play the game pong randomly and score point at each prediction, which will direct gradient descent into updating the weights to maximise points.\nHowever, games such as Montezuma's Revenge, requires a lot of complex actions before points are gained. If the points stay 0, how does gradient descent determine if the weights should be decreased or increased?\nInstead, the authors initialise a second random neural network which remains the same for the duration of training. As mentioned above the output of a randomly initialised neural network is random. \nThe output of the actual NN, and world state is then passed through the random NN (RND), which will return a random value. The NN is then required to not only determine a game action but also determine the result of the RND. \nThe policy function is extended from just points to also predict the result of the RND. The greater the prediction error the more bonus points are allocated to the policy function. This encourages gradient descent to change the weights of the NN to explore unseen parts of the game as this increases the prediction error of the RND which increases points. At the same time gradient descent must update the weights of the NN to predict the RND.\nI hope this answers your question.\nIn summary randomly initialised NN include multilayered perceptrons or convolutional networks will behave randomly with respect to their inputs. The RND network is randomly set once at the star of training, and used to basically remind the NN undergoing training that it has done this before do not do this again - i.e. rewards the NN for doing something new (exploration).\n", "type": 2, "id": "9384", "date": "2018-12-07T18:38:43.093", "score": 5, "comment_count": 5, "parent_id": "9354"}}}
{"line": 3199, "body": "Below is a quote from CS231n:\n\nPrefer a stack of small filter CONV to one large receptive field CONV layer. Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages\n\nMy visualized interpretation:\n\nHow can you see through the first CNN layer from the second CNN layer and see a 5x5 sized receptive field?\nThere were no previous comments stating all the other hyperparameters, like input size, steps, padding, etc. which made this very confusing to visualize.\n\nEdited:\nI think I found the answer. BUT I still don't understand it. In fact, I am more confused than ever.\n", "type": 1, "id": "5107", "date": "2018-01-23T18:45:30.723", "score": 7, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "convolution-arithmetic", "receptive-field"], "title": "How can 3 same size CNN layers in different ordering output different receptive field from the input layer?", "answer_count": 3, "views": 4545, "accepted_answer": "5740", "answers": {"5978": {"line": 3862, "body": "The problem is in your diagram. Here are the steps to get to a 5x5 receptive field. Here is your diagram, redone slightly:\n\nNotice that the new unit takes a weighted sum of the 9 pixels in the input, and then applies a rectified linear nonlinearity. Now, there are more of these, creating three new numbers computed from that part of the image. Each one slides over by one pixel:\n\nWe repeat this process going down three pixels as well, and then finally, we have a new 3x3 input field:\n\nNotice that the new unit on the right now gets input from a 5x5 input field. I hope this helps!\n", "type": 2, "id": "5978", "date": "2018-04-10T07:46:51.280", "score": 2, "comment_count": 0, "parent_id": "5107"}, "5740": {"line": 3683, "body": "It is really easy to visualize the growth in the receptive field of the input as you go deep into the CNN layers if you consider a small example.\nLet's take a simple example:\nThe dimensions are in the form of $\\text{channels} \\times \\text{height} \\times \\text{width}$.\n\nThe input image $I$ is a $3 \\times 5 \\times 5$ matrix\nThe first convolutional layer's kernel $K_1$ has shape $3 \\times 2 \\times 2$ (we consider only 1 filter for simplicity)\nThe second convolutional layer's kernel $K_2$ has shape $1 \\times 2 \\times 2$\nPadding $P = 0$\nStride $S = 1$\n\nThe output dimensions $O$ are calculated by the following formula taken from the lecture CS231n.\n$$O= (I - K + 2P)/S + 1$$\nWhen you do a convolution of the input image with the first filter $K_1$, you get an output of shape $1 \\times 4 \\times 4$ (this is the activation of the CONV1 layer). The receptive field for this is the same as the kernel size ($K_1$), that is, $2 \\times 2$.\nWhen this layer (of shape $1 \\times 4 \\times 4$) is convolved with the second kernel (CONV2) $K_2$ ($1 \\times 2 \\times 2$), the output would be $1 \\times 3 \\times 3$. The receptive field for this would be the $3 \\times 3$ window of the input because you have already accumulated the sum of the $2 \\times 2$ window in the previous layer.\nConsidering your example of three CONV layers with $3 \\times 3$ kernels is also similar. The first layer activation accumulates the sum of all the neurons in the $3 \\times 3$ window of the input. When you further convolve this output with a kernel of $3 \\times 3$, it will accumulate all the outputs of the previous layers covering a bigger receptive field of the input.\nThis observation comes in line with the argument that deeper layers learn more intricate features like facial expressions, abstract concepts, etc. because they cover a larger receptive field of our original input image.\n", "type": 2, "id": "5740", "date": "2018-03-19T07:16:50.670", "score": 4, "comment_count": 0, "parent_id": "5107"}, "7069": {"line": 4672, "body": "The intention of the referred text is to reason out the disadvantage of equivalent-merged-single-convolution-layer over multiple [CONV -> RELU]*N layers.\nIn the given scenario, if 2 layers of 3x3 filters were to be replaced by an equivalent single layer then this equivalent layer would need a filter with a receptive field of size 5x5.\nSimilarly, an equivalent layer filter would need its receptive field to be of size 7x7 to compress 3 layers of 3x3 filters. Note that the most obvious disadvantage would be missing out on modeling non-linearity.\n", "type": 2, "id": "7069", "date": "2018-07-07T15:26:04.627", "score": 0, "comment_count": 0, "parent_id": "5107"}}}
{"line": 3740, "body": "Suppose a CNN is trained to detect bounding box of a certain type of object (people, cars, houses, etc.)\nIf each image in the training set contains just one object (and its corresponding bounding box), how well can a CNN generalize to pick up all objects if the input for prediction contains multiple objects? \nShould the training images be downsampled in order for the CNN to pick out multiple objects in the prediction?\nI don't have a specific one in mind. I was just curious about the general behavior.\n", "type": 1, "id": "5814", "date": "2018-03-27T19:14:04.540", "score": 4, "comment_count": 1, "tags": ["neural-networks", "convolutional-neural-networks"], "title": "How well can CNN for bounding box detection generalise?", "answer_count": 2, "views": 902, "accepted_answer": null, "answers": {"6558": {"line": 4301, "body": "I suggest you to go through the r-cnn paper or go through a tutorial on it . CNNs transform the image into high dimensional vector in their last layer , in case of classification this vector is sent to a \"softmax\" layer , in case of bounding box regression , four values :length , breadth , location of one of the points of the bounding box , are regressed from this vector , so if you use a cnn with one regression head you end up with one bounding box irrespective of the training set. \n", "type": 2, "id": "6558", "date": "2018-05-29T20:01:29.337", "score": 1, "comment_count": 0, "parent_id": "5814"}, "21680": {"line": 14368, "body": "Typically, there are mainly three steps in an object bounding box detection.\nFirst, a model or algo is used to generate ROI(region of interest) or region proposals. These region proposals are an all large set of bounding boxes spanning the full images. (that is, an object localization component).\nIn the second step, visual features(Face, Person, etc... using Convolution) are extracted for each of the bounding boxes, they are evaluated. It is determined whether and which objects are present in the interested area based on visual features (i.e. an object classification component).\nIn the final post-processing step, overlapped boxes are combined into a single bounding box (that is, non-maximum suppression).\n", "type": 2, "id": "21680", "date": "2020-06-06T05:47:31.500", "score": 0, "comment_count": 0, "parent_id": "5814"}}}
{"line": 4460, "body": "I understand how Neural Networks work and have studied its theory well. My question is at the intricacies of Deep Neural networks and perhaps is a bit beyond common understanding (as I have been told (or misled) from discussions).\nMy question is: On the whole, is there a clear understanding of how mutation occurs within a neural network from the input layer to the output layer, for both supervised and unsupervised cases? \nAny neural network is a set of neurons + the connection weights. With each successive layer, there is a change in the input. Say I have a Neural Network that does movie recommendation on n parameters. \nSay if X is a parameter that stands for the movie rating on IMDB. In each successive stage, there is a mutation of input X to X' and further X'' and so on. \nWhile of course, we know how to mathematically talk about X' and X'', do we at all have a conceptual understanding as to what this variable is in its corresponding neural n-dimension? The neural weights which to the human eye might be set of random numbers but may mean something profound if we could ever understand what the neural weights 'represent'. \n\nWhat is the nature of neural weights such that despite decades worth of research and use, there is no clear understanding of what these connection weights represent? Or rather, why has there been so little effort in understanding the nature of neural weights, in a non-mathematical sense given the huge impetus in going beyond the black box notion of AI.  \n", "type": 1, "id": "6776", "date": "2018-06-16T21:03:09.043", "score": 5, "comment_count": 2, "tags": ["neural-networks", "deep-learning", "deep-neural-networks", "artificial-neuron", "concepts"], "title": "What do neural connection weights represent 'conceptually'?", "answer_count": 2, "views": 389, "accepted_answer": null, "answers": {"6791": {"line": 4471, "body": "I don't know if my intuition is correct but I will give it a try. \nYou could see weights as how much important one thing is, the problem is to understand what that thing represents. When I say thing I'm referring to the output of a specific neuron. I don't think that we can say what the output of a neuron represents in the real world unless we directly relate it through an error function or if the function used to compute that particular value have some meaning in the real world.\nEdit:\nIf you want, you could actually build your neural network such that its neurons represent something. It's also very simple. you have only to write down all the equations relative to that particular topic. You could put them in a big system or, and this is better, you could put them in several systems such that the outputs of system 1 are the input of system 2 and so on. You could convert each system into a layer where each neuron represents an equation. Note that in this case, you would have the classical neuron with\n\nz = dot(w.T,x) + b\na = g(x)\n\nbut a more complex equation for z (but still based on weights) and a linear activation function for a. In this case, you could name each neuron and say what they represent in the real world. \nHowever, this isn't the purpose of a neural network. A neural network should have neurons with simple equations to be fast thus the linear interpolating function dot(w.T,x) + b is the best choice (the fact that the activation function is almost always non linear and in some cases a non-banal function is due to other thing and could be an interesting question). A neural network should also be as general as possible because usually is build upon a system that you don't know completely.\nSo I modify slightly my answer: is not simply that you don't know what a neuron represent, excluding the ones of the output layer, you don't want that they have a meaning in the real world.\n", "type": 2, "id": "6791", "date": "2018-06-18T21:17:20.867", "score": 2, "comment_count": 2, "parent_id": "6776"}, "6818": {"line": 4491, "body": "It's a bit of a challenge to answer your question, since you appear to be not really familiar with the basics. You're talking about mutations, and changes to the input. \nNo. The input is a vector of data, which initializes the value of the input nodes. The first layer of weights is then used to calculate the values for the next layer of nodes. This next layer is not a \"mutation\" of the input layer; that suggest the second layer of nodes is similar but not exactly identical to the first layer. \nIn reality, it's very common that the second layer of nodes does not even have the same shape as the first layer. \nYou are even wondering if certain weights have a certain meaning. That's even easier to answer. We know these networks are quite robust. We can ignore a significant percentage of the weights, and the classifications will change only a little. This shows that no individual weight represents a specific aspect of the network. \n", "type": 2, "id": "6818", "date": "2018-06-20T09:47:14.427", "score": 0, "comment_count": 1, "parent_id": "6776"}}}
{"line": 4067, "body": "Is it possible to use a VAE to reconstruct an image starting from an initial image instead of using K.random_normal, as shown in the \"sampling\" function of this example?\nI have used a sample image with the VAE encoder to get z_mean and z_logvar.\nI have been given 1000 pixels in an otherwise blank image (with nothing in it).\nNow, I want to reconstruct the sample image using the decoder with a given constraint that the 1000 pixels in the otherwise blank image remain the same. The remaining pixels can be reconstructed so they are as close to the initial sample image as possible. In other words, my starting point for the decoder is a blank image with some pixels that don't change.\nHow can I modify the decoder to generate an image based on this constraint? Is it possible? Are there variations of VAE that might make this possible? So we can predict the latent variables by starting from an initial point(s)?\n", "type": 1, "id": "6246", "date": "2018-05-03T16:36:13.410", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "computer-vision", "variational-autoencoder", "constrained-optimization"], "title": "How to use a VAE to reconstruct an image starting from an initial image instead of starting from a random vector?", "answer_count": 2, "views": 670, "accepted_answer": null, "answers": {"6263": {"line": 4075, "body": "You could use VAE as previously answered though it will not work well in practice.\nI think denoising auto-encoder is suitable for your problem because during training, the input is corrupted stochastically, thus it must learn to guess the distribution of the missing information (reconstruct the clean original input)\nWe could argue that VAE is better than DAE at modeling p(x) because of the randomness introduced at the latent space layer while DAE like algorithm keeps putting noise starting from the input layer.\nsuppose your data is concentrated on this 1-D curved manifold, what VAE could do is just pick some random value and output p(X|Z) which is Gaussian by the way, while DAE would learn to map a corrupted data point x~ back to the original data point x.\n\n", "type": 2, "id": "6263", "date": "2018-05-04T07:32:14.870", "score": 0, "comment_count": 0, "parent_id": "6246"}, "6250": {"line": 4070, "body": "The thing is, the decoder samples from a latent mu and sigma, so you cant sample from a raw image directly. But if you're trying to put a random image into the encoder of a trained VAE to match it to some sample image (via reconstruction loss), then your random input image will converge to the target sample.\nThis will work when the following VAE architecture constraints are satisfied:\n\nThe target sample is contained in the previously used training distribution.\nThe parameters of the VAE are frozen after training.\nThe input image values are \"backpropagate-able\". (Interpret the input image as optimizable parameters.)\n\n", "type": 2, "id": "6250", "date": "2018-05-03T18:51:39.467", "score": 1, "comment_count": 2, "parent_id": "6246"}}}
{"line": 4399, "body": "I'm working with deep learning on some EEG data for classification, and I was wondering if there's any systematic/mathematical way to define the architecture of the networks, in order to compare their performance fairly.\nShould the comparison be at the level of neurons (e.g. number of neurons in each layer), or at the level of weights (e.g. number of parameters for training in each type of network), or maybe something else?\nOne idea that emerged was to construct one layer for the MLP for each corresponding convolutional layer, based on the number of neurons after the pooling and dropout layers.\nAny ideas? If there's any relative work or paper regarding this problem I would be very grateful to know.\nThank you for your time\nKonstantinos\n", "type": 1, "id": "6696", "date": "2018-06-08T19:48:10.853", "score": 3, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "comparison"], "title": "How to make a fair comparison of a convolutional neural network (cNN) vs a mutlilayer perceptron (MLP)?", "answer_count": 2, "views": 573, "accepted_answer": "6739", "answers": {"6717": {"line": 4411, "body": "The best best way to monitor an architectures performance would be comparing the resource utilization, model accuracy, value loss and confusion matrix.\n\ne.g VGG16 consumes less system resources in comparison to inception V3\n\n\nThere is an article that goes in depth.\n", "type": 2, "id": "6717", "date": "2018-06-12T08:16:11.630", "score": 0, "comment_count": 1, "parent_id": "6696"}, "6739": {"line": 4429, "body": "Konstantine, I assume you refer to plain MLP and CNN, without any modifications.\nI believe what you ask is how to set both of them up, in order to have the fairest comparison possible.\nThe way I would do it, is to use their plain implementations but both tuned as much as possible, in every hyperparameter. Both should work as black boxes that accept the same inputs and give the same outputs.\nThis will give you insight on the true raw performance of both algorithms.\nHope it helps :)\n", "type": 2, "id": "6739", "date": "2018-06-13T18:03:30.690", "score": 0, "comment_count": 2, "parent_id": "6696"}}}
{"line": 3528, "body": "I've seen these terms thrown around this site a lot, specifically in the tags convolutional-neural-networks and neural-networks.\nI know that a neural network is a system based loosely on the human brain. But what's the difference between a convolutional neural network and a regular neural network? Is one just a lot more complicated and, ahem, convoluted than the other? \n", "type": 1, "id": "5546", "date": "2018-03-06T21:01:55.610", "score": 36, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "comparison", "terminology", "definitions"], "title": "What is the difference between a convolutional neural network and a regular neural network?", "answer_count": 5, "views": 66294, "accepted_answer": null, "answers": {"5569": {"line": 3550, "body": "TLDR: \nThe convolutional-neural-network is a subclass of neural-networks which have at least one convolution layer. They are great for capturing local information (e.g. neighbor pixels in an image or surrounding words in a text) as well as reducing the complexity of the model (faster training, needs fewer samples, reduces the chance of overfitting). \nSee the following chart that depicts the several neural-networks architectures including deep-conventional-neural-networks: .\n\nNeural Networks (NN), or more precisely Artificial Neural Networks (ANN), is a class of Machine Learning algorithms that recently received a lot of attention (again!) due to the availability of Big Data and fast computing facilities (most of Deep Learning algorithms are essentially different variations of ANN).  \nThe class of ANN covers several architectures including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) eg LSTM and GRU, Autoencoders, and Deep Belief Networks. Therefore, CNN is just one kind of ANN.\nGenerally speaking, an ANN is a collection of connected and tunable units (a.k.a. nodes, neurons, and artificial neurons) which can pass a signal (usually a real-valued number) from a unit to another. The number of (layers of) units, their types, and the way they are connected to each other is called the network architecture.\nA CNN, in specific, has one or more layers of convolution units. A convolution unit receives its input from multiple units from the previous layer which together create a proximity. Therefore, the input units (that form a small neighborhood) share their weights. \nThe convolution units (as well as pooling units) are especially beneficial as:\n\nThey reduce the number of units in the network (since they are many-to-one mappings). This means, there are fewer parameters to learn which reduces the chance of overfitting as the model would be less complex than a fully connected network.\nThey consider the context/shared information in the small neighborhoods. This future is very important in many applications such as image, video, text, and speech processing/mining as the neighboring inputs (eg pixels, frames, words, etc) usually carry related information.  \n\nRead the followings for more information about (deep) CNNs:\n\nImageNet Classification with Deep Convolutional Neural Networks\nGoing Deeper with Convolutions\n\n\np.s. ANN is not \"a system based loosely on the human brain\" but rather a class of systems inspired by the neuron connections exist in animal brains.\n", "type": 2, "id": "5569", "date": "2018-03-08T01:49:51.950", "score": 38, "comment_count": 0, "parent_id": "5546"}, "5550": {"line": 3532, "body": "A convolutional neural network is one that has convolutional layers. If a general neural network is, loosely speaking, inspired by a human brain (which isn't very much accurate), the convolutional neural network is inspired by the visual cortex system, in humans and other animals (which is closer to the truth). As the name suggests, this layer applies the convolution with a learnable filter (a.k.a. kernel), as a result the network learns the patterns in the images: edges, corners, arcs, then more complex figures. Convolutional neural network may contain other layers as well, commonly pooling and dense layers.\nHighly recommend CS231n tutorial on this matter: it's very detailed and contains a lot of very nice visualizations.\n", "type": 2, "id": "5550", "date": "2018-03-06T23:01:38.160", "score": 6, "comment_count": 0, "parent_id": "5546"}, "5566": {"line": 3547, "body": "Convolutional Neural Networks (CNNs) are neural networks with architectural constraints to reduce computational complexity and ensure translational invariance (the network interprets input patterns the same regardless of translation-- in terms of image recognition: a banana is a banana regardless of where it is in the image). Convolutional Neural Networks have three important architectural features.\nLocal Connectivity: Neurons in one layer are only connected to neurons in the next layer that are spatially close to them. This design trims the vast majority of connections between consecutive layers, but keeps the ones that carry the most useful information. The assumption made here is that the input data has spatial significance, or in the example of computer vision, the relationship between two distant pixels is probably less significant than two close neighbors.\nShared Weights: This is the concept that makes CNNs \"convolutional.\" By forcing the neurons of one layer to share weights, the forward pass (feeding data through the network) becomes the equivalent of convolving a filter over the image to produce a new image. The training of CNNs then becomes the task of learning filters (deciding what features you should look for in the data.)\nPooling and ReLU: CNNs have two non-linearities: pooling layers and ReLU functions. Pooling layers consider a block of input data and simply pass on the maximum value. Doing this reduces the size of the output and requires no added parameters to learn, so pooling layers are often used to regulate the size of the network and keep the system below a computational limit. The ReLU function takes one input, x, and returns the maximum of {0, x}. ReLU(x) = argmax(x, 0). This introduces a similar effect to tanh(x) or sigmoid(x) as non-linearities to increase the model's expressive power.\n\nFurther Reading\nAs another answer mentioned, Stanford's CS 231n course covers this in detail. Check out this written guide and this lecture for more information. Blog posts like this one and this one are also very helpful.\nIf you're still curious why CNNs have the structure that they do, I suggest reading the paper that introduced them though this is quite long, and perhaps checking out this discussion between Yann Lecun and Christopher Manning about innate priors (the assumptions we make when we design the architecture of a model).\n", "type": 2, "id": "5566", "date": "2018-03-07T18:50:07.183", "score": 15, "comment_count": 1, "parent_id": "5546"}, "20633": {"line": 13536, "body": "The everyday definition of convolution comes from the Latin convolutus meaning 'to roll together'. Hence the meaning twisted or complicated.\nThe mathematical definition comes from the same root, with the interpretation of taking a \"rolling average\".\nHence in Machine Learning, a convolution is a sliding window across an input creating one averaged output for each stride the window takes. I.e. the values covered by the window are convoluted to create one convoluted output. This is best demonstrated with an a diagram:\n\nThe convolution can be any function of the input, but some common ones are the max value, or the mean value.\nA convolutional neural network (CNN) is a neural network where one or more of the layers employs a convolution as the function applied to the output of the previous layer.\nIf the window is greater than size 1x1, the output will be necessarily smaller than the input (unless the input is artificially 'padded' with zeros), and hence CNN's often have a distinctive 'funnel' shape:\n\n", "type": 2, "id": "20633", "date": "2020-04-25T22:13:50.023", "score": 2, "comment_count": 0, "parent_id": "5546"}, "18623": {"line": 12728, "body": "It's a very simplified explantion. I am just talking about the core idea. \nA neural network is a combination of many layers.  \nA neural network (Multiple Layer Perceptron: Regular neural network ): It does a linear combination (a mathematical operation) between the previous layer's output and the current layer's weights(vectors) and then it passes data to the next layer by passing through an activation function. The picture shows a unit of a layer.\n\nA neural network (Convolutional Neural Network):  It does convolution (In signal processing it's known as Correlation) (Its a mathematical operation) between the previous layer's output and the current layer's kernel ( a small matrix ) and then it passes data to the next layer by passing through an activation function. The picture shows a Convolution operation. Each layer may have many convolution operation \n\n", "type": 2, "id": "18623", "date": "2020-03-13T18:08:45.177", "score": 0, "comment_count": 0, "parent_id": "5546"}}}
{"line": 6376, "body": "I have multiple pictures that look exactly like the one below this text. I'm trying to train CNN to read the digits for me. Problem is isolating the digits. They could be written in any shape, way, and position that person who is writing them wanted to. I thought of maybe training another CNN to recognize the position/location of the digits, but I'm not sure how to approach the problem. But, I need to get rid of that string and underline. Any clue would be a great one. Btw. I would love to get the 28x28 format just like the one in MNIST.\nThanks up front.\n\n", "type": 1, "id": "9545", "date": "2018-12-16T08:06:37.283", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "image-recognition", "computer-vision", "handwritten-characters"], "title": "How to approach this handwritten digit recognition?", "answer_count": 2, "views": 137, "accepted_answer": null, "answers": {"9565": {"line": 6388, "body": "I think one approach you can try to segment the digits and Connected Components Labeling (https://en.wikipedia.org/wiki/Connected-component_labeling).\nWith it, you'll end up with a label for each letter and then you can try to find the convex hull of the letter.\nAfter that, just crop a square for each convex hull and input it to your CNN.\nNotice that it will only if there is at least one pixel between the letters...\n", "type": 2, "id": "9565", "date": "2018-12-17T11:38:49.787", "score": 1, "comment_count": 1, "parent_id": "9545"}, "9711": {"line": 6476, "body": "Use of CNNs to recognize digits is a reasonable approach as of this answer's writing, the effectiveness of which can be enhanced via Sensitive Error Correcting Output Codes, (2005, John Langford, Alina Beygelzimer) according to Shuo Yang et. al. in Deep Representation Learning with Target Coding.\nGiven a sufficient CNN depth, it can be trained to recognize the digits with the field name and underscore intact. Automatically removing them would generally only be possible after they are recognized using the CNN approach anyway.\nIsolating the digits from the field name and the underscore would be accomplished via the same approach used to isolate the digits from one another. There is no reason to perform these two conceptual tasks in series and consume additional development time and resources, when a single deep CNN can locate the digits and dismiss the field name and underscore. This is similar to animal vision. A mosquito's visual pathway distinguishes an oncoming object from the background to avoid a swat using the same network it uses to recognize the object.\nWhat would be helpful would be to normalize the input by finding a two dimensional plateau (much like a geological one) representing the distribution of black pixels in horizontal and vertical dimensions. Summing rows and columns of pixels and using a heuristic algorithm to find a rectangle outside of which there is only noise may be sufficient. Then trim and scale, which removes redundancy and may improve CNN training speed, accuracy, and/or reliability by distributing the image more widely over the input layer and likely reducing the layer count by at least one.\nAnother approach is to use overall features of the form in which the field resides to rotate, position, and scale the image so that the field name and underscore is within a fraction of a pixel from an expected location on the form. In this case training a separate CNN is less redundant and has a higher return on development investment. In such a design, the field name and underscore can be removed by subtraction, however this may disturb field value recognition because the handwriting may overlap items blanked. It will require experimentation to determine the accuracy and reliability hit (diminution) from such disturbance.\nAddendum in Response to Comment\nFor the first approach, a Field Name and Underscore Superimposer will need to be designed and implemented to transform the training example set features, the pixel arrays, but preserve the labels corresponding to each example. The hyper-parameter values that work best without the superimposition may need modification to work best with it.\n", "type": 2, "id": "9711", "date": "2018-12-27T18:51:09.460", "score": 0, "comment_count": 3, "parent_id": "9545"}}}
{"line": 4530, "body": "I have an image dataset where objects may belong to one of the hundred thousand classes.\nWhat kind of neural network architecture should I use in order to achieve this?\n", "type": 1, "id": "6880", "date": "2018-06-25T11:27:35.883", "score": 5, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "convolutional-neural-networks", "classification"], "title": "What kind of neural network architecture do I use to classify images into one hundred thousand classes?", "answer_count": 4, "views": 449, "accepted_answer": null, "answers": {"6883": {"line": 4532, "body": "As you can imagine and as it has already been said, a large one for your network to tune weights and biases. But I wanted to nuance this statement with two points\nFirst :  you can use an Autoencodeur to pre-process your images. It can reduce dimensionality and so improve the learning capability and efficiency (in a generalization point of view). \nThis kind of NN takes your images as inputs, encode and then decode them to provie new representation of your initial images. Dealing with the decoded dataset can allow you to consider less hidden layer with less hidden nodes and then speed your work up.\nSecond : architecture is sure a thing to deal with image recognition, but you can also play on the input representation (that is what the autoencoder aforesaid is about). You can look at PCA (Principal Component Analysis). It allows to reduce dimensionality to a certain number of components (that you specify). It is often used in face recognition where inputs and targets are various. \nAll that to say that architecture is sure a thing when dealing with large datasets, but there also few tools to reshape the inputs so that then can be more easily learnt. \nAnd by doing so you can improve the capability of your network as much in term of time computation as in quality and accuracy of prediction\n", "type": 2, "id": "6883", "date": "2018-06-25T14:08:22.557", "score": 1, "comment_count": 0, "parent_id": "6880"}, "6882": {"line": 4531, "body": "A large one! \nIn all seriousness, imagenet had roughly 1000 classes and did not require anything special from the top submissions. Depending on how deep(contextually) these classes are, you may want to do something like multi-label classification. Your biggest problems will likely be differentiating between classes, as well as class distribution.\nGood luck!\n", "type": 2, "id": "6882", "date": "2018-06-25T13:35:18.567", "score": 1, "comment_count": 1, "parent_id": "6880"}, "6886": {"line": 4535, "body": "Classification tasks with a large number of classes are usually handled with hierarchical softmax to reduce the complexity of the final layer. This is useful, for example, in applications such as word embedding where you have hundreds of thousands of classes (words), like in your case.\n", "type": 2, "id": "6886", "date": "2018-06-25T23:52:06.340", "score": 2, "comment_count": 0, "parent_id": "6880"}, "17811": {"line": 12039, "body": "Alexnet (2012), Overfeat (2013), VGG (2014) and ResNet (2016) are cited in many image recognition or segmentation applications. There is also GoogleLeNet (2015). \nThe lastest is the publication the denser is the network.\nThe ResNet publication comments on how the network density affects accuracy depending on the image data set size. The article tends to give a motivated answer to the question \n\nIs learning better networks as easy as stacking more layers?\n\nYou might consider the training time since you have your own image data set depending on the kind of hardware you can you use ( see this benchmark for instance ). The denser the more time it will takes.\nYou also have to consider the size of the traning data set w/r to the expected accuracy. If the set is too small the net will probably overfeat. In that case you migh consider a data augmentation strategy (one of the answers mentions auto encoding, I m not sure but this might help for this purpose).\nAll these publications refer to the ImageNet data base and the associated image classification/detection contest which has 1000 classes.\n", "type": 2, "id": "17811", "date": "2020-02-03T09:14:33.997", "score": 0, "comment_count": 0, "parent_id": "6880"}}}
{"line": 4470, "body": "Premise\nOk, I know that this question was asked before on ai.SE, on stats.SE and also on SO. So I did my homework in checking before posting my question, but none of them has an answer that fully satisfies me.\nSummary of my knowledge up to now\nSuppose you have a layer that is fully connected, and that each neuron performs an operation like\na = g(w^T * x + b)\n\nwere a is the output of the neuron, x the input, g our generic activation function, and finally w and b our parameters. \nIf both w and b are initialized with all elements equal to each other, then a is equal for each unit of that layer. \nThis means that we have symmetry, thus at each iteration of whichever algorithm we choose to update our parameters, they will update in the same way, thus there is no need for multiple units since they all behave as a single one. \nIn order to break the symmetry we could randomly initialize the matrix w and initialize b to zero (this is the setup that I've seen more often). \nThis way a is different for each unit so that all neurons behave differently.\nOf course randomly initializing both w and b would be also okay even if not necessary.\nThe actual question\nIs randomly initializing w the only choice? \nCould we randomly initialize b instead of w in order to break the symmetry?\nIs the answer dependent on the choice of the activation function and/or the cost function?\nMy thinking is that we could break the symmetry by randomly initializing b, since in this way a would be different for each unit and, since in the backward propagation the derivatives of both w and b depend on a(at least this should be true for all the activation functions that I have seen so far), each unit would behave differently. Obviously, this is only a thought, and I'm not sure that is absolutely true.  \nRemark\nNote that this is a theoretical question, not a practical one. \nThis means that I'm not really interested in answers that involves performance, but you're welcome to include them as corollary to the real answer.\n", "type": 1, "id": "6789", "date": "2018-06-18T19:32:27.573", "score": 5, "comment_count": 2, "tags": ["neural-networks", "training", "concepts"], "title": "Why should weights of Neural Networks be initialized to random numbers?", "answer_count": 3, "views": 693, "accepted_answer": "6794", "answers": {"6794": {"line": 4474, "body": "Randomising just b sort of works, but setting w to all zero causes severe problems with vanishing gradients, especially at the start of learning. \nUsing backpropagation, the gradient at the outputs of a layer L involves a sum multiplying the gradient of the inputs to layer L+1 by the weights (and not the biases) between the layers. This will be zero if the weights are all zero.\nA gradient of zero at L's output will further cause all earlier layers(L-1, L-2 etc all the way back to layer 1) to receive zero gradients, and thus not update either weights or bias at the update step. So the first time you run an update, it will only affect the last layer. Then the next time, it will affect the two layers closest to the output (but only marginally at the penultimate layer) and so on.\nA related issue is that with weights all zero, or all the same, maps all inputs, no matter how they vary, onto the same output. This also can adversely affect the gradient signal that you are using to drive learning - for a balanced data set you have a good chance of starting learning close to a local minimum in the cost function.\nFor deep networks especially, to fight vanishing (or exploding) gradients, you should initialise weights from a distribution that has an expected magnitude (after multiplying the inputs) and gradient magnitude that neither vanishes nor explodes. Analysis of values that work best in deep networks is how Xavier/Glorot initialisation were discovered. Without careful initialisation along these lines, deep networks take much longer to learn, or in worst cases never recover from a poor start and fail to learn effectively.\nPotentially to avoid these problems you could try to find a good non-zero fixed value for weights, as an alternative to Xavier initialisation, along with a good magnitude/distribution for bias initialisation. These would both vary according to size of the layer and possibly by the activation function. However, I would suspect this could suffer from other issues such sampling bias issues - there are more weights, therefore you get a better fit to desired aggregate behaviour when setting all the weight values randomly than you would for setting biases randomly.\n", "type": 2, "id": "6794", "date": "2018-06-19T07:25:48.457", "score": 5, "comment_count": 10, "parent_id": "6789"}, "6793": {"line": 4473, "body": "Most of the explanations given for choosing something or not choosing something (like hyper-parameter tuning) in deep learning, are based on empirical studies like analysing the error over a number of iterations. So the answer by @Joe S is what people in deep learning side give.\nSince you have asked for a mathematical explanation, I suggest you to read this paper published recently in NIPS. It talks about the convergence of SGD to global minima subject to weight initialisation being Gaussian using ReLU as activation function. The paper considers a neural net with no hidden layer, just input and output layers.\nThe very fact that analysis on such 'simple' network gets published in a very reputed and top conference itself suggests that the explanation you are seeking is not very easy and very few people work on the theoretical aspects of neural nets. IMHO after some years as the research progresses, I might be able to edit this answer and give the necessary explanation that you sought for. Till then this is the best I could do. Cheers!!!\n", "type": 2, "id": "6793", "date": "2018-06-19T05:54:03.153", "score": 2, "comment_count": 5, "parent_id": "6789"}, "6792": {"line": 4472, "body": "w should be randomized to small (nonzero) numbers so that the adjustments made by the backpropagation are more meaningful and each value in the matrix is updated a different amount. If you start with all zeros it will still work, but take longer to get to a meaningful result. AFAIK, this was found empirically by various researchers and became common practice. \nrandomizing b does not have the same effect of helping, therefore most people do not bother. \nThis choice is one of many that is made by the architect of the network and theoretically you could use an infinite number of w matrix initializations. The one commonly used just happens to be tested and generally works. \nThis video is better at explaining than I am: \nhttps://www.youtube.com/watch?v=iPNN805konI\n", "type": 2, "id": "6792", "date": "2018-06-18T23:28:47.237", "score": 0, "comment_count": 3, "parent_id": "6789"}}}
{"line": 3705, "body": "My understanding is that the convolutional layer of a convolutional neural network has four dimensions: input_channels, filter_height, filter_width, number_of_filters.  Furthermore, it is my understanding that each new filter just gets convoluted over ALL of the input_channels (or feature/activation maps from the previous layer).\nHOWEVER, the graphic below from CS231 shows each filter (in red) being applied to a SINGLE CHANNEL, rather than the same filter being used across channels.  This seems to indicate that there is a separate filter for EACH channel (in this case I'm assuming they're the three color channels of an input image, but the same would apply for all input channels).\nThis is confusing - is there a different unique filter for each input channel?\n\nThis is the source.\nThe above image seems contradictory to an excerpt from O'reilly's \"Fundamentals of Deep Learning\":\n\n\"...filters don't just operate on a single feature map. They operate\non the entire volume of feature maps that have been generated at a\nparticular layer...As a result, feature maps must be able to\noperate over volumes, not just areas\"\n\n...Also, it is my understanding that these images below are indicating a THE SAME filter is just convolved over all three input channels (contradictory to what's shown in the CS231 graphic above):\n\n\n", "type": 1, "id": "5769", "date": "2018-03-22T02:36:20.950", "score": 58, "comment_count": 2, "tags": ["deep-learning", "convolutional-neural-networks", "image-recognition", "weights"], "title": "In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?", "answer_count": 9, "views": 43413, "accepted_answer": null, "answers": {"5779": {"line": 3714, "body": "There are only restriction in 2D. Why?\nImagine a fully connected layer.\nIt'd be awfully huge, each neuron would be connected to maybe 1000x1000x3 inputs neurons. But we know that processing nearby pixel makes sense, therefore we limit ourselves to a small 2D-neighborhood, so each neuron is connected to only a 3x3 near neurons in 2D. We don't know such a thing about channels, so we connect to all channels.\nStill, there would be too many weights. But because of the translation invariance, a filter working well in one area is most probably useful in a different area. So we use the same set of weights across 2D. Again, there's no such translation invariance between channels, so there's no such restriction there.\n", "type": 2, "id": "5779", "date": "2018-03-23T04:39:52.307", "score": 0, "comment_count": 0, "parent_id": "5769"}, "5778": {"line": 3713, "body": "The following picture that you used in your question, very accurately describes what is happening. Remember that each element of the 3D filter (grey cube) is made up of a different value (3x3x3=27 values). So, three different 2D filters of size 3x3 can be concatenated to form this one 3D filter of size 3x3x3.\n\nThe 3x3x3 RGB chunk from the picture is multiplied elementwise by a 3D filter (shown as grey). In this case, the filter has 3x3x3=27 weights. When these weights are multiplied element wise and then summed, it gives one value.\n\nSo, is there a separate filter for each input channel?\nYES, there are as many 2D filters as number of input channels in the image. However, it helps if you think that for input matrices with more than one channel, there is only one 3D filter (as shown in the image above).\n\nThen why is this called 2D convolution (if filter is 3D and input matrix is 3D)?\nThis is 2D convolution because the strides of the filter is along the height and width dimensions only (NOT depth) and therefore, the output produced by this convolution is also a 2D matrix. The number of movement directions of the filter determine the dimensions of convolution.\nNote: If you build up your understanding by visualizing a single 3D filter instead of multiple 2D filters (one for each layer), then you will have an easy time understanding advanced CNN architectures like Resnet, InceptionV3, etc.\n", "type": 2, "id": "5778", "date": "2018-03-22T19:41:57.567", "score": 25, "comment_count": 7, "parent_id": "5769"}, "5771": {"line": 3707, "body": "\nIn a convolutional neural network, is there a unique filter for each input channel or are the same new filters used across all input channels?\n\nThe former. In fact there is a separate kernel defined for each input channel / output channel combination. \nTypically for a CNN architecture, in a single filter as described by your number_of_filters parameter, there is one 2D kernel per input channel. There are input_channels * number_of_filters sets of weights, each of which describe a convolution kernel. So the diagrams showing one set of weights per input channel for each filter are correct. The first diagram also shows clearly that the results of applying those kernels are combined by summing them up and adding bias for each output channel.\nThis can also be viewed as using a 3D convolution for each output channel, that happens to have the same depth as the input. Which is what your second diagram is showing, and also what many libraries will do internally. Mathematically this is the same result (provided the depths match exactly), although the layer type is typically labelled as \"Conv2D\" or similar. Similarly if your input type is inherently 3D, such as voxels or a video, then you might use a \"Conv3D\" layer, but internally it could well be implemented as a 4D convolution.\n", "type": 2, "id": "5771", "date": "2018-03-22T08:24:51.310", "score": 16, "comment_count": 5, "parent_id": "5769"}, "7067": {"line": 4671, "body": "Refer to \"Local Connectivity\" section in here and slide 7-18.\n\"Receptive Field\" hyperparameter of filter is defined by height & width only, as depth is fixed by preceding layer's depth.\nNOTE that \"The extent of the connectivity along the depth axis is always equal to the DEPTH of the input volume\" -or- DEPTH of activation map (in case of later layers).\nIntuitively, this must be due to the fact that image channels data are interleaved, not planar. This way, applying filter can be achieved simply by column vectors multiplication.\nNOTE that Convolutional Network learns all the filter parameters (including the depth dimension) and they are total \"hwinput_layer_depth + 1 (bias)\".\n", "type": 2, "id": "7067", "date": "2018-07-07T14:41:49.480", "score": 0, "comment_count": 0, "parent_id": "5769"}, "9181": {"line": 6152, "body": "I recommend chapter 2.2.1 of my masters thesis as an answer. To add to the remaining answers:\nKeras is your friend to understand what happens:\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\n\nmodel = Sequential()\nmodel.add(Conv2D(32, input_shape=(28, 28, 3),\n          kernel_size=(5, 5),\n          padding='same',\n          use_bias=False))\nmodel.add(Conv2D(17, (3, 3), padding='same', use_bias=False))\nmodel.add(Conv2D(13, (3, 3), padding='same', use_bias=False))\nmodel.add(Conv2D(7, (3, 3), padding='same', use_bias=False))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nprint(model.summary())\n\ngives\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 28, 28, 32)        2400      \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 28, 28, 17)        4896      \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 28, 28, 13)        1989      \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 28, 28, 7)         819       \n=================================================================\nTotal params: 10,104\n\nTry to formulate your options. What would that mean for the parameters if something else would be the case?\nHint: $2400 = 32 \\cdot (3 \\cdot 5 \\cdot 5)$\nThis approach also helps you with other layer types, not only convolutional layers.\nPlease also note that you are free to implement different solutions, that might have other numbers of parameters. \n", "type": 2, "id": "9181", "date": "2018-11-26T14:43:53.190", "score": 0, "comment_count": 0, "parent_id": "5769"}, "9161": {"line": 6134, "body": "I'm following up on the answers above with a concrete example in the hope to further clarify how the convolution works with respect to the input and output channels and the weights, respectively:\nLet the example be as follows (wrt to 1 convolutional layer):\n\nthe input tensor is 9x9x5, i.e. 5 input channels, so input_channels=5\nthe filter/kernel size is 4x4 and the stride is 1\nthe output tensor is 6x6x56, i.e. 56 output channels, so output_channels=56\nthe padding type is 'VALID' (i.e. no padding)\n\nWe note that:\n\nsince the input has 5 channels, the filter dimension becomes 4x4x5, i.e. there are 5 separate, unique 2D filters of size 4x4 (i.e. each has 16 weights); in order to convolve over the input of size 9x9x5 the filter becomes 3D and must be of size 4x4x5\ntherefore: for each input channel, there exists a distinct 2D filter with 16 different weights each. In other words, the number of 2D filters matches the number of input channels\nsince there are 56 output channels, there must be 56 3-dimensional filters W0, W1, ..., W55 of size 4x4x5 (cf. in the CS231 graphic there are 2 3-dimensional filters W0, W1 to account for the 2 output channels), where the 3rd dimension of size 5 represents the link to the 5 input channels (cf. in the CS231 graphic each 3D filter W0, W1 has the 3rd dimension 3, which matches the 3 input channels)\ntherefore: the number of 3D filters equals the number of output channels\n\nThat convolutional layer thus contains: \n56 3-dimensional filters of size 4x4x5 (= 80 different weights each) to account for the 56 output channels where each has a value for the 3rd dimension of 5 to match the 5 input channels. In total there are \nnumber_of_filters=input_channel*output_channels=5*56=280 \n2D filters of size 4x4 (i.e. 280x16 different weights in total).\n", "type": 2, "id": "9161", "date": "2018-11-25T16:46:23.593", "score": 8, "comment_count": 0, "parent_id": "5769"}, "10896": {"line": 7296, "body": "Just to make two details absolutely clear:\nSay you have $N$ 2D input channels going to $N$ 2D output channels.  The total number of 2D $3\\times3$ filter weights is actually $N^2$.  But how is the 3D convolution affected, i.e., if every input channel contributes one 2D layer to every output channel, then each output channel is composed initially of $N$ 2D layers, how are they combined?\nThis tends to be glossed over in almost every publication I've seen, but the key concept is the $N^2$ 2D output channels are interleaved with each other to form the $N$ output channels, like shuffled card decks, before being summed together.  This is all logical when you realize that along the channel dimensions of a convolution (which is never illustrated), you actually have a fully connected layer!  Every input 2D channel, multiplied by a unique $3\\times 3$ filter, yields a 2D output layer contribution to a single output channel.  Once combined, every output layer is a combination of every input layer $\\times$ a unique filter.  It's an all to all contribution.\nThe easiest way to convince yourself of this is to imagine what happens in other scenarios and see that the computation becomes degenerate - that is, if you don't interleave and recombine the results, then the different outputs wouldn't actually do anything - they'd have the same effect as a single output with combined weights.\n", "type": 2, "id": "10896", "date": "2019-02-26T15:25:34.630", "score": 0, "comment_count": 0, "parent_id": "5769"}, "13302": {"line": 9155, "body": "For anyone trying to understand how convolutions are calculated, here is a useful code snippet in Pytorch:\nbatch_size = 1\nheight = 3 \nwidth = 3\nconv1_in_channels = 2\nconv1_out_channels = 2\nconv2_out_channels = 2\nkernel_size = 2\n# (N, C_in, H, W) is shape of all tensors. (batch_size, channels, height, width)\ninput = torch.Tensor(np.arange(0, batch_size*height*width*in_channels).reshape(batch_size, in_channels, height, width))\nconv1 = nn.Conv2d(in_channels, conv1_out_channels, kernel_size, bias=False) # no bias to make calculations easier\n# set the weights of the convolutions to make the convolutions easier to follow\nnn.init.constant_(conv1.weight[0][0], 0.25)\nnn.init.constant_(conv1.weight[0][1], 0.5)\nnn.init.constant_(conv1.weight[1][0], 1) \nnn.init.constant_(conv1.weight[1][1], 2) \nout1 = conv1(input) # compute the convolution\n\nconv2 = nn.Conv2d(conv1_out_channels, conv2_out_channels, kernel_size, bias=False)\nnn.init.constant_(conv2.weight[0][0], 0.25)\nnn.init.constant_(conv2.weight[0][1], 0.5)\nnn.init.constant_(conv2.weight[1][0], 1) \nnn.init.constant_(conv2.weight[1][1], 2) \nout2 = conv2(out1) # compute the convolution\n\nfor tensor, name in zip([input, conv1.weight, out1, conv2.weight, out2], ['input', 'conv1', 'out1', 'conv2', 'out2']):\n    print('{}: {}'.format(name, tensor))\n    print('{} shape: {}'.format(name, tensor.shape))\n\nRunning this gives the following output:\ninput: tensor([[[[ 0.,  1.,  2.],\n          [ 3.,  4.,  5.],\n          [ 6.,  7.,  8.]],\n\n         [[ 9., 10., 11.],\n          [12., 13., 14.],\n          [15., 16., 17.]]]])\ninput shape: torch.Size([1, 2, 3, 3])\nconv1: Parameter containing:\ntensor([[[[0.2500, 0.2500],\n          [0.2500, 0.2500]],\n\n         [[0.5000, 0.5000],\n          [0.5000, 0.5000]]],\n\n\n        [[[1.0000, 1.0000],\n          [1.0000, 1.0000]],\n\n         [[2.0000, 2.0000],\n          [2.0000, 2.0000]]]], requires_grad=True)\nconv1 shape: torch.Size([2, 2, 2, 2])\nout1: tensor([[[[ 24.,  27.],\n          [ 33.,  36.]],\n\n         [[ 96., 108.],\n          [132., 144.]]]], grad_fn=<MkldnnConvolutionBackward>)\nout1 shape: torch.Size([1, 2, 2, 2])\nconv2: Parameter containing:\ntensor([[[[0.2500, 0.2500],\n          [0.2500, 0.2500]],\n\n         [[0.5000, 0.5000],\n          [0.5000, 0.5000]]],\n\n\n        [[[1.0000, 1.0000],\n          [1.0000, 1.0000]],\n\n         [[2.0000, 2.0000],\n          [2.0000, 2.0000]]]], requires_grad=True)\nconv2 shape: torch.Size([2, 2, 2, 2])\nout2: tensor([[[[ 270.]],\n\n         [[1080.]]]], grad_fn=<MkldnnConvolutionBackward>)\nout2 shape: torch.Size([1, 2, 1, 1])\n\nNotice how the each channel of the convolution sums over all previous channels outputs.\n", "type": 2, "id": "13302", "date": "2019-07-10T19:19:26.140", "score": 0, "comment_count": 0, "parent_id": "5769"}, "32496": {"line": 21366, "body": "My understanding from this paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/TASLP2339736-proof.pdf was that the filters are used on each input channels (i.e input feature map in the paper) separately and the result is summed, as described in eq. 8. Here they use different filters for each channel, but you could totally use the same filter.\nAs to knowing the software implementation in a particular ML library such as Tensorflow or PyTorch, this requires inspection as suggested by @Mohsin Bukhari\n", "type": 2, "id": "32496", "date": "2021-11-22T15:09:37.653", "score": 0, "comment_count": 0, "parent_id": "5769"}}}
{"line": 3761, "body": "There are problems (e.g. this one or this other one) that could potentially be solved easily using traditional algorithmic techniques. I think that training a neural network (or any other machine learning model) for such sorts of problems will be more time consuming, resource-intensive, and pointless.\nIf I want to solve a problem, how to decide whether it is better to solve algorithmically or by using NN/ML techniques? What are the pros and cons? How can this be done in a systematic way? And if I have to answer someone why I chose a particular domain, how should I answer?\nExample problems are appreciated.\n", "type": 1, "id": "5840", "date": "2018-03-30T19:07:54.197", "score": 4, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "comparison"], "title": "How to decide whether a problem needs to be solved algorithmically or with machine learning techniques?", "answer_count": 4, "views": 413, "accepted_answer": null, "answers": {"5986": {"line": 3869, "body": "fwiw, with the basic, non-trivial M-game, I have no doubt that ALphaZero could tear through any human player alive in very short order. I hope that people will start experimenting with that, especially on m^n(m^n) where m > 3 and n > 2 to see how they hold up.  Problem is, once you expand past n > 3 it gets very difficult for humans to play. This leads to a condition where performance of an NN on higher order M can only realistically be evaluated against other algorithms.  In this context, it seems worthwhile to develop a general, classical algorithm that can evaluate any order M, regardless of efficacy of tree search in relation to the problem size, with the understanding that decision making is never presumed optimal until the gametree becomes tractable.  This carries an an assumption of the same general strength across all M for the classical algorithm, because the expansion of m or n do alter the core heuristics. \nFrom the practical standpoint, as a product designed for mobile with no assumption of connectivity, it doesn't make sense to start integrating NNs until lowest-common-denominator mobile devices have sufficient resources.  The issue of package size is also important in this context--the classical algorithms require a trivial amount of code and volume. Most importantly, using classical algorithms formed of sets of heuristics and parameters allows recombination of functions to produce myriad automata of varying degrees of strength. (This can be easily accomplished by altering the size of tree search algorithms, but may only be relevant in determining which heuristics perform better under tree search restrictions.) \nFinally, because M-games provide an array precise metrics, it may be worthwhile to develop core heuristic function based on human reasoning.\n", "type": 2, "id": "5986", "date": "2018-04-10T21:09:39.480", "score": 1, "comment_count": 2, "parent_id": "5840"}, "5851": {"line": 3766, "body": "When we apply supervised learning to a problem, we are already systematizing the approach. A human has decided that a function exists (mapping from inputs to unique output) and that the offered features are the only ones that need be considered. The learning then goes ahead to find the best solution given those constraints. Unsupervised learning is a bit more general, searching for associations or relations that might not necessarily be functions. A neural net is not yet capable of generalizing and asking for more information, it can only become more specific unless a human intervenes.\nEverything depends on the detail of the problem. If it is clear that a function must exist then we can set a NN to find that function. Many other problems are more difficult - a company is losing money and you have data but halfway there was a change in CEO, so human reasoning has to be mixed in to deal with the situation. The human can modify the architecture of the NN to introduce dummy variables, but the NN cannot do this by itself.\nSo your answer really is \"I chose this method because of the (lack of) need for me to artificially constrain the approach to the problem.\"\n", "type": 2, "id": "5851", "date": "2018-03-31T19:56:34.993", "score": 1, "comment_count": 2, "parent_id": "5840"}, "5972": {"line": 3856, "body": "There are two different problems described in the linked question and your question: optimization and learning.\nOptimization\nIf you are asking about optimization (the second linked question: Search minimum value with learning machine algorithm) you can have 3 different approaches:\n\nanalytical approach\nnumerial methods\nmetaheuristics\n\nAs you suggest, it is usually better to try them from the first to the last one. It is common that the first approach is unfeasible for optimizing for target function, but very often you can use either mathematical optimization for some specific classes of problems (e.g. linear/quadratic programming) or iterative methods (e.g. conjugate gradient method). Only after considering this approaches it makes sense for the third class of approaches, genetic algorithms being a notable example, which is often classified as an AI approach.\nLearning\nIf you are asking about learning, then the first linked question (Ideas on how to make a neural net learn how to split sequence into sub sequences) seems to be intended as an example. However it doesn't make clear what the problem is, as the target function seems to be obvious, so no learning is needed. \nIn this case it also makes sense to first try to pin down the problem mathematically and resort to machine learning if it is impossible and if you have the data (input/output examples).\n", "type": 2, "id": "5972", "date": "2018-04-09T21:53:49.080", "score": 2, "comment_count": 6, "parent_id": "5840"}, "21528": {"line": 14250, "body": "Note that \"algorithmically\" can refer to anything that uses an algorithm. Currently, ML systems are trained with algorithms and neural networks can be seen as algorithms (although black-box ones), so ML is also algorithmic. Everything that runs on a computer (a concrete version of a Turing machine) can be seen as an algorithm (or program)! In fact, computers were invented exactly for this purpose: to perform some algorithmic operation (i.e. a set of instructions, like a recipe). \nSo, by algorithmic, I assume you're referring to techniques that are typically taught in an \"Algorithms and Data Structures\" course for a computer science student, such as the binary search (one of the most simple and yet beautiful and useful algorithms!), which is an algorithm that, given some constraints (a sorted array), gives you an exact correct solution in $\\mathcal{O}(\\log n)$ time. However, I think that you are also referring to every program that is primarily based on if-then statements and loops (e.g. desktop applications, websites, etc.)\nTo answer your question, you first need to understand the scope of the machine learning field. \nMachine learning (like statistics) is a set of techniques that attempt to learn from data. So, every problem where data is available (and you can get insight from) can potentially be solved with a machine learning technique.  ML techniques typically produce approximative solutions and are typically used to solve problems where an exact solution is infeasible. However, note that machine learning isn't the only approach to solve hard problems (e.g. you can also use meta-heuristics, e.g. ant colony optimization algorithms).\nIf you have an algorithm that produces an exact solution (without requiring data) in polynomial time (preferably, in $\\mathcal{O}(n^2)$ time), then machine learning (or any other technique that produces approximative solutions, e.g. heuristics) is quite useless. \n", "type": 2, "id": "21528", "date": "2020-05-29T22:32:38.743", "score": 0, "comment_count": 0, "parent_id": "5840"}}}
{"line": 4689, "body": "I choose the activation function for the output layer depending on the output that I need and the properties of the activation function that I know. For example, I choose the sigmoid function when I'm dealing with probabilities, a ReLU when I'm dealing with positive values, and a linear function when I'm dealing with general values.\nIn hidden layers, I use a leaky ReLU to avoid dead neurons instead of the ReLU, and the tanh instead of the sigmoid. Of course, I don't use a linear function in hidden units.\nHowever, the choice for them in the hidden layer is mostly due to trial and error.\nIs there any rule of thumb of which activation function is likely to work well in some situations?\nTake the term situations as general as possible: it could be referring to the depth of the layer, to the depth of the NN, to the number of neurons for that layer, to the optimizer that we chose, to the number of input features of that layer, to the application of this NN, etc.\nThe more activation functions I discover the more I'm confused in the choice of the function to use in hidden layers. I don't think that flipping a coin is a good way of choosing an activation function.\n", "type": 1, "id": "7088", "date": "2018-07-09T00:06:57.810", "score": 22, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "activation-function", "hyperparameter-optimization", "hyper-parameters"], "title": "How to choose an activation function for the hidden layers?", "answer_count": 3, "views": 9727, "accepted_answer": "7089", "answers": {"7089": {"line": 4690, "body": "It seems to me that you already understand the shortcomings of ReLUs and sigmoids (like dead neurons in the case of plain ReLU).\nYou may want to look at ELU (exponential linear units) and SELU (self-normalising version of ELU). Under some mild assumptions, the latter has the nice property of self-normalisation, which mitigates the problem of vanishing and exploding gradients. In addition, they propagate normalisation - i.e., they guarantee that the input to the next layer will have zero mean and unit variance.\nHowever, it would be incredibly difficult to recommend an activation function (AF) that works for all use cases, although I believe that SELU was designed so that it would do the right thing with pretty much any input.\nThere are many considerations - how difficult it is to compute the derivative (if it is differentiable at all!), how quickly a NN with your chosen AF converges, how smooth it is, whether it satisfies the conditions of the universal approximation theorem, whether it preserves normalisation, and so on. You may or may not care about some or any of those.\nThe bottom line is that there is no universal rule for choosing an activation function for hidden layers. Personally, I like to use sigmoids (especially tanh) because they are nicely bounded and very fast to compute, but most importantly because they work for my use cases. Others recommend leaky ReLU for the input and hidden layers as a go-to function if your NN fails to learn. You can even mix and match activation functions to evolve NNs for fancy applications.\nAt the end of the day, you are probably going to get as many opinions as there are people about the right choice of activation function, so the short answer should probably be: start with the AF of the day (leaky ReLU / SELU?) and work your way through other AFs in order of decreasing popularity if your NN struggles to learn anything.\n", "type": 2, "id": "7089", "date": "2018-07-09T01:44:31.187", "score": 15, "comment_count": 1, "parent_id": "7088"}, "7095": {"line": 4696, "body": "I don't know what kind of neural networks you are working on. But one should also consider tanh activation functions when dealing with recurrent neural network. The why is to avoid exploding gradient issues since the tanh function is bounded at the difference of the RELU function for instance.\n", "type": 2, "id": "7095", "date": "2018-07-09T13:51:27.067", "score": 0, "comment_count": 1, "parent_id": "7088"}, "21801": {"line": 14448, "body": "***Take my answer as a side note to that given by cantordust:\nIf one can verify that an activation function perform well in some cases, that good behavior often extrapolates to other problems.\nThus, by testing activation functions on a few different problems, one can often infer how well (or badly) it will perform on most problems.\nThe following video shows how different activation functions perform in different problems:\nhttps://www.youtube.com/watch?v=Hb3vIYUQ_I8\nOne can verify that an activation function usually perform well in all cases, or the other way around: it does it poorly in all cases.\nAs cantordust says, I would recommend always starting with leaky ReLU: it is simple, efficient, and generally produces nice results in a wide variety of problems.\nIt also evades the dying ReLU problem, and does not suffer from the vanishing gradient problem.\nThe only thing to keep in mind is the exploding gradient problem if the neural network is too deep, or if it is a recurrent neural network, which are essentially the same concept.\nThe video shows that other activation functions worth trying (in addition to leaky ReLU) are Gaussian, Sinusoid, or Tanh.\n", "type": 2, "id": "21801", "date": "2020-06-11T12:25:13.987", "score": 3, "comment_count": 3, "parent_id": "7088"}}}
{"line": 6241, "body": "We have convolutional neural networks and recurrent neural networks for analysing  respectively images and sequential data.\nHow do I determine which neural network architecture is more appropriate to approximate a certain function? For example, suppose I want to approximate the function $f(x,y) = \\sin(2\\pi x)\\sin(2\\pi y)$ with domain $\\Omega = [0,1]\\times [0,1]$, that is, $x$ and $y$ can be between $0$ and $1$ (inclusive). For example, which kind of activation functions would be better suited for approximating this specific function?\n", "type": 1, "id": "9319", "date": "2018-12-03T16:28:25.627", "score": 3, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "math", "activation-function", "function-approximation"], "title": "Which neural network should I use to approximate a specific function?", "answer_count": 2, "views": 533, "accepted_answer": null, "answers": {"9451": {"line": 6314, "body": "If the concept class specified is\n$$f(x, y) = k \\, \\sin(2 \\pi f_x x) \\, sin(2 \\pi f_y y) \\\\ \\land 0 < x < 1 \\\\ \\land 0 < y < 1 \\; \\text{,}$$\nand the optimum fit to example data is expected occur when $k \\approx 1 \\land f_x \\approx 1 \\land f_y \\approx 1$, then it is not an AI problem. It is a problem that can be solved with a least squares convergence, probably in conjunction with a Fourier transform.\nIf nothing is known about $f(x, y)$ except continuity and that it is single valued with respect to $(x, y)$, then few conclusions can be drawn about best approach. In such a case, the domain of $x$ and $y$ are irrelevant because they can be normalized. Furthermore, the tree of operations, such as $\\sin()$ and multiplication, are irrelevant too, because the function could just as easily be\n$$f(x, y) = \\ln(x) + \\Gamma(y) - k \\, \\text{.}$$\nThe question indicates the design involves CNN and RNN components for analyzing images and sequential data. It is not clear whether the CNN is for the discovery of objects or waves (given the $\\sin()$ in the function mentioned) and whether those objects move between frames so that the RNN must detect motion.\nNothing is given about the pool of example data available or planned to be available or the expected outputs of the system. If data is sequential, where is $t$ in the function? What is the objective of image analysis?\nAlthough a deep MLP (multilayer perceptron) with SGD can learn an arbitrary function, it is hardly an architecture, the mention of images, CNN, RNN, and sequential data, MLP with SGD does not seem to match.\nRegarding activation functions, the inner layer functions would depend on the higher level design requirements. The activation functions of the last layer of a single artificial network is usually chosen to match the data type and range of desired output for each output channel (dimension).\nIf the objective of this question is to take images and sequential data and produce something useful without a priori defining what useful means, then it is an unsolved AI problem thus far and no known topology comprised of artificial networks and other AI building blocks provide a solution. The autonomous development of internal concepts of usefulness would need to be developed mathematically and algorithmically and become practically speed optimized in hardware and software first.\n", "type": 2, "id": "9451", "date": "2018-12-11T08:53:52.680", "score": 1, "comment_count": 0, "parent_id": "9319"}, "12811": {"line": 8768, "body": "For example, suppose I want to approximate the function\nmashine learning is used to approximate an UNKNOWN function. If you do want to do this with NN, you will have a regression task with small full connected network with like:\n\nInput is 2 values of range 0 ..1\na few hidden layers, just start trying with one, of size like 3-6 neurons , activation  sigmoid or tanh\nneed a non-linear function here to approx non linear sin * sin, whatever \nlast layer with no activation function just making a difference from... known value f(x,y)\n\n", "type": 2, "id": "12811", "date": "2019-06-12T12:54:54.827", "score": 0, "comment_count": 0, "parent_id": "9319"}}}
{"line": 5218, "body": "I have been researching LSTM neural networks.  I have seen this diagram a lot and I have few questions about it. Firstly, is this diagram used for most LSTM neural networks?  \nSecondly, if it is, wouldn't only having single layers reduce it's usefulness?  \n", "type": 1, "id": "7785", "date": "2018-09-01T19:40:51.840", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "long-short-term-memory"], "title": "Need Help With LSTM Neural Networks", "answer_count": 2, "views": 77, "accepted_answer": "7790", "answers": {"7786": {"line": 5219, "body": "(1) Yes this is the diagram for a classical LSTM unit. Of cause there are some variants and those diagrams would look slightly different.\n(2) It is very common for researchers to use more than one layers of LSTM and achieves better performance than a single layer one. A common way to \"stack\" LSTMs is to use the previous layer's output ($h_t$ in your diagram) as the input to the next layer ($x_t$). However, I have seldom seen any successful application of 5+ layers of LSTMs, while for CNNs it is common to use tens or even hundreds of layers.\n", "type": 2, "id": "7786", "date": "2018-09-01T21:05:32.390", "score": 1, "comment_count": 0, "parent_id": "7785"}, "7790": {"line": 5222, "body": "Just to be 100% sure - the diagram you refer to is a diagram of an LSTM CELL, not NETWORK. The operands you see on the diagram are operations within a cell, not separate \"neurons\". I think it is quite obvious, however reading your questions I just wanted to be 100% sure we are on the same page.\nNow, about layers. RNN networks (LSTM in particular) are just like any other ANN structure. Theoretically, a 1-hidden layer network can do any computation of a \"deeper\" network. ANN is a universal approximate of math functions. Still, multi-layer ANNs typically work better on more complex problems. Multi-layer typically needs less total connections, learns better and is less resource-demanding.\nIn particular, multi-layer LSTMs are believed to be better at determining complex in-time patterns. I think there is no rigorous proof for this, however.\nAlso, in practical applications - I did not see much improvement in network capabilities by adding additional LSTM layers. Adding more dense layers before/ after LSTM seemed to have a much better effect.\n", "type": 2, "id": "7790", "date": "2018-09-02T09:02:51.330", "score": 0, "comment_count": 0, "parent_id": "7785"}}}
{"line": 5271, "body": "I have an application where I want to find the locations of objects on a simple, relatively constant background (fixed camera angle, etc). For investigative purposes, I've created a test dataset that displays many characteristics of the actual problem.\nHere's a sample from my test dataset.\n\nOur problem description is to find the bounding box of the single circle in the image. If there is more than one circle or no circles, we don't care about the bounding box (but we at least need to know that there is no valid single bounding box).\nFor my attempt to solve this, I built a CNN that would regress (min_x, min_y, max_y, max_y), as well as one more value that could indicate how many circles were in the image.\nI played with different architecture variations, but, in general, the architecture a was very standard CNN (3-4 ReLU convolutional layers with max-pooling in between, followed by a dense layer and an output layer with linear activation for the bounding box outputs, set to minimise the mean squared error between the outputs and the ground truth bounding boxes).\nRegardless of the architecture, hyperparameters, optimizers, etc, the result was always the same - the CNN could not even get close to building a model that was able to regress an accurate bounding box, even with over 50000 training examples to work with.\nWhat gives? Do I need to look at using another type of network as CNNs are more suited to classification rather than localisation tasks?\nObviously, there are computer vision techniques that could solve this easily, but due to the fact that the actual application is more involved, I want to know strictly about NN/AI approaches to this problem.\n", "type": 1, "id": "7867", "date": "2018-09-07T04:19:05.633", "score": 2, "comment_count": 0, "tags": ["convolutional-neural-networks", "computer-vision", "object-detection", "bounding-box"], "title": "How to architect a network to find bounding boxes in simple images?", "answer_count": 2, "views": 677, "accepted_answer": null, "answers": {"7868": {"line": 5272, "body": "There are some special architectures of CNNs which are designed exactly for the task you mention. The Detector library includes a collection of these architectures, this paper describes the Mask R-CNN network in detail, which is designed for image segmentation tasks.\n", "type": 2, "id": "7868", "date": "2018-09-07T06:34:42.487", "score": 1, "comment_count": 1, "parent_id": "7867"}, "8957": {"line": 6016, "body": "In the R-CNN family approach, the BBox regression is performed using a specific module providing region proposal: initially the Selective Search has been used then Region Proposal Network has been introduced, which has the advantage, being a NN, to be trainable. \nThis has not to be confused with the BBox Regressor which comes after in the architecture and is focused on refining the proposal bboxes for the \"Non Background\" proposal objects \nPlease consider that to perform object detection properly, at least using a region proposal based architecture, you need to perform the basic \"Background / Non Background\" classification (which is where CNN features help) so to filter out non relevant proposals \n", "type": 2, "id": "8957", "date": "2018-11-13T16:21:55.860", "score": 0, "comment_count": 0, "parent_id": "7867"}}}
{"line": 5134, "body": "Where can I find (more) pre-trained language models? I am especially interested in neural network-based models for English and German.\nI am aware only of Language Model on One Billion Word Benchmark and TF-LM: TensorFlow-based Language Modeling Toolkit.\nI am surprised not to find a greater wealth of models for different frameworks and languages.\n", "type": 1, "id": "7684", "date": "2018-08-23T07:17:27.470", "score": 5, "comment_count": 1, "tags": ["neural-networks", "natural-language-processing", "bert", "gpt", "language-model"], "title": "Where can I find pre-trained language models in English and German?", "answer_count": 2, "views": 1571, "accepted_answer": "7754", "answers": {"7712": {"line": 5156, "body": "This will depend to some extent on what you want to do with the language models.\nSome possible resources are:\nTensorFlow offers 3 pre-trained language models in the research package. \nCafe's ModelZoo has a single pre-trained model that does video -> captions.\nOther packages like Cafe2  offer pre-trained models, but the documentation does not suggest any of them are suitable for language.\nFailing this, a good approach might be to email the authors of a paper that adopts an approach you like. Some (but far from all) researchers will be happy to share their models, which you can then use as a starting point for your own.\n", "type": 2, "id": "7712", "date": "2018-08-25T13:08:58.910", "score": 0, "comment_count": 3, "parent_id": "7684"}, "7754": {"line": 5191, "body": "Of course now there has been a huge development:\nHuggingface published pytorch-transformers, a library for the so successful Transformer models (BERT and its variants, GPT-2, XLNet, etc.), including many pretrained (mostly English or multilingual) models (docs here). It also includes one German BERT model. SpaCy offers a convenient wrapper (blog post).\nUpdate: Now, Salesforce published the English model CTRL, which allows for use of \"control codes\" that influence the style, genre and content of the generated text.\nFor completeness, here is the old, now less relevant version of my answer:\n\nSince I posed the question, I found this pretrained German language model:\nhttps://lernapparat.de/german-lm/\nIt is an instance of a 3-layer \"averaged stochastic descent weight-dropped\" LSTM which was implemented based on an implementation by Salesforce.\n", "type": 2, "id": "7754", "date": "2018-08-29T13:52:59.557", "score": 1, "comment_count": 1, "parent_id": "7684"}}}
{"line": 5999, "body": "I have a question about the convolutional neural network.\nConsider this image:\n\nWe have a part of an input matrix and a filter. Now, we can do the convolution and the result is a scalar, if it is a large number the future was found otherwise. So, the feature map is a matrix where each number indicates the points where a feature was found. I understood this. The output of this convolution is a feature map (after activation function). My misunderstanding start here.\nThe next convolution will find another feature. I don't understand how this filter can find a new feature from a representation of where the previous feature was found. The feature maps output of the first convolution is a representation of indicates only where the features were found.\nShouldn't be the features found instead of a number that indicates how much was found in this feature? How exactly does this work?\n", "type": 1, "id": "8929", "date": "2018-11-11T13:47:45.863", "score": 2, "comment_count": 2, "tags": ["neural-networks", "convolutional-neural-networks", "feature-extraction", "features", "feature-map"], "title": "Features Map convolutional neural network", "answer_count": 3, "views": 118, "accepted_answer": null, "answers": {"8931": {"line": 6000, "body": "Since the kernel size is limited. A feature can encompass more than 3 pixels or say 5 pixels, for that we cannot find it in 1 layer. So by adding 2 layers and having an activation function that is not like a step function which you would prefer, we extend the feature to 5 or 9 pixels. And with n layers, a feature covering 2n+1, or 4n+1 can be seen.\nPS. You have some spelling errors. Take your time to frame well. Welcome!\n", "type": 2, "id": "8931", "date": "2018-11-11T14:20:27.923", "score": 2, "comment_count": 0, "parent_id": "8929"}, "8934": {"line": 6003, "body": "Three reasons:\n\nLimited field of view by one small kernel\nLayers of abstraction and combinations of intermediate features\nNon-linearity of decision boundary\n\nThe previous two answers by caissalover and Andrew got the first two points. Let's elaborate on the third:\nConvolution is a linear transformation of the input. But not everything can be properly expressed with linear operations. This is why we apply non-linear activation functions (e.g. ReLU) in between. \n", "type": 2, "id": "8934", "date": "2018-11-11T17:41:41.427", "score": -1, "comment_count": 0, "parent_id": "8929"}, "8933": {"line": 6002, "body": "Well, let's start from the beginning, convolution is an operation that is used not only in neural networks. Actually, people were using it in signal processing long before neural networks. Convolution is also used in image processing, in photoshop for example. So let's start from image processing. Take a look at this link. Here you can see how we can use conv op to process an image. If you choose an outline kernel you can basically extract edges from that image using 3x3 kernel and applying it on the image.\nSo when you train your network you change parameters of that kernel in a way that minimizes your loss function. However, it is a really hard question what representations your CNN will learn. But for the sake of the example, let's assume that in the first layer your network learned to extract edges from an input image. Then you feed that feature map to the second layer and the second layer now should learn to extract new features from those edges provided by the first layer. For example, it can learn to detect horizontal edges or vertical one, so if you trying to recognize cars vs pedestrians, for example, cars will have more horizontal edges when pedestrians will have more vertical edges. Also note, that we usually make several feature maps per layer, so in this example, 2nd layer can output 16 feature maps produced by different kernels with different parameters so it could recognize vertical edges, horizontal edges, round one and so on. After that you feed the output of the 2nd layer, let's say, to the fully connected layer and the fc layer will say \"ok, I see that previous layer recognized horizontal edges so I must output higher probability for the car, but if 2nd layer recognized vertical edges I would output higher probability for the pedestrian\". \nOf course, this is oversimplified and in reality, representations learned by the network will be way more abstract, but I think just for the sake of gaining intuition this is an acceptable example. But if you are looking to gain a deeper knowledge about what CNNs learn in their layers read this article, not an easy reading, especially if you don't know much about CNNs, but it is quite insightful. Also if you don't want to read that article here is a video presentation made by the authors of that paper on youtube.\n", "type": 2, "id": "8933", "date": "2018-11-11T15:17:52.770", "score": 0, "comment_count": 2, "parent_id": "8929"}}}
{"line": 6455, "body": "How can I use a 2-dimensional feature matrix, rather than a feature vector, as the input to a neural network?\nFor a WWII naval wargame, I have sorted out the features of interest to approximate the game state $S$ at a given time $t$.\n\nthey are a maximum of 50 naval task forces and airbases on the map\n\neach one has a lot of features (hex, weather, number of ships, type, distance to other task forces, distance to objective, cargo, intelligence, combat value, speed, damage, etc.)\n\n\nThe output would be the probability of winning and the level of victory.\n\n", "type": 1, "id": "9681", "date": "2018-12-25T20:30:47.270", "score": 1, "comment_count": 1, "tags": ["neural-networks", "game-ai"], "title": "How can I use a 2-dimensional feature matrix as the input to a neural network?", "answer_count": 2, "views": 632, "accepted_answer": null, "answers": {"9684": {"line": 6458, "body": "The most obvious way to do this would be to simply \"unroll\" your matrix into a vector. Your example input matrix would get turned into the following input vector:\n$$\\left( \\begin{array}{} a_1 & a_2 & \\dots & a_t & b_1 & b_2 & \\dots & b_t & c_1 & c_2 & \\dots & c_t \\end{array} \\right)$$\nI don't think there are any other clear ways to use an \"input matrix\" really. The only benefit I could see in using an input matrix rather than an unrolled vector (if it were possible to do so in whatever way) would be if doing so would somehow enable the learning algorithm to exploit the \"domain knowledge\" that certain input features are related to each other in special ways (i.e. features in the same row belong to the same unit, and features in the same column are the same \"type\" of feature, or other way around). Intuitively, I suspect something like this could be accomplished by restricting the number of connections you make to the next layer. For example, you could make a part of the next layer only be connected to all the $a_i$ features, a different part connected only to all the $b_i$ features, etc. Similarly, you could have a part that is connected only to the $a_1, b_1, c_1, \\dots$ features, a different part only connected to the $a_2, b_2, c_2, \\dots$ features, etc. I don't know for sure how well this would work though... just think that it could.\n", "type": 2, "id": "9684", "date": "2018-12-26T10:25:42.763", "score": 3, "comment_count": 2, "parent_id": "9681"}, "9687": {"line": 6460, "body": "(Split the answer from Dennis in two)\nA very different approach would be to use a network architecture with recurrence, for example an LSTM. You could treat your input as a \"sequence\" rather than a matrix (just like a sentence in language processing would be a sequence of inputs), providing your feature vectors for different units as inputs one at a time. This would remove the need of having a giant input layer (with support for 50 units) in cases where only a small portion of them would be used (e.g., if you only have 5 units). There is not really a concept of \"time\" in your inputs though... ideally the output of your network would be invariant to the order in which you provide it with the different inputs for the different units, but that would not typically be the case with these kinds of architectures in practice.\n", "type": 2, "id": "9687", "date": "2018-12-26T12:12:31.047", "score": 0, "comment_count": 2, "parent_id": "9681"}}}
{"line": 5803, "body": "I am training a Multilayer Neural Nets with 146 samples (97 for training set, 20 for validation set and 29 for testing set). I am using:\nautomatic differentiation,\nSGD method,\nfixed learning rate + momentum term,\nlogistic function,\nquadratic cost function,\nL1 and L2 regularization technique,\nadding some artificial noise 3%.\nWhen I used L1 or L2 regularization technique my problem (overfitting problem) got worst.\ni tried different values for lambdas (the penalty parameter  0.0001, 0.001, 0.01, 0.1, 1.0 and 5.0). After 0.1 i just killed my ANN. The best result that i took it was using 0.001 (but it is worst comparing the one that i didnt use regularization technique). The graph represent the error functions for different penalty parameters and also a case without using L1. \n\nand the accuracy\n\nWhat can be?\nThanks!!\n", "type": 1, "id": "8607", "date": "2018-10-24T20:05:07.130", "score": 6, "comment_count": 5, "tags": ["neural-networks", "deep-learning", "training", "overfitting", "regularization"], "title": "Why L1/L2 regularization technique did not improve my accuracy?", "answer_count": 3, "views": 6051, "accepted_answer": "8632", "answers": {"8632": {"line": 5825, "body": "Regularization is one of the important prerequisites for improving the reliability, speed, and accuracy of convergence, but it is not a solution to every problem.  Irregularity in data is only one of many root causes for slow or otherwise inadequate learning results, and as the results in the question indicates, it can reduce reliability, speed, or accuracy in some cases.\nFor those who are new to this topic, these are a few other root causes.\n\nComplexity beyond the capacity of the computing machinery to model adequately\nInsufficient number of examples for training\nPoor distribution alignment between data sets (training, testing, validation, production)\nSaturation of back propagation values in floating point\nOutliers cause by error in example creation or labeling\nLocal optimization minima in the loss function combined with insufficient stochastic injection in SGD\nLeaning to heavily on artificial network convergence and neglecting using other known algorithms as part of an overall system architecture\nUsing activation functions or hyper-parameters that are not well tuned to the example set or the model\n\nIn the case of regularization, L1 and L2 have known properties that have been proven in theorem form to guarantee convergence in fewer examples or epochs, but they rely on very specific models toward which convergence is targetted.  L1 and L2 are not always beneficial for polynomial models.  Trying to make them so tends to lead to over-fitting.\nThe resulting trained networks lack the needed generalization to produce reliable behavior upon receiving data outside the training set.  Irregularities might not always be caught during test and verification.  Deployed systems can exhibit the signs of over-fitting over time because the network learned time-specific features.\nOverfitting can be prevented using dropout regularization, another technique first proposed in Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Srivastava, et al. 2014.\nAnother two things to look at ...\n\nIs the logistic function is the best choice and for which layers?  It seems to be down-trending.  More people are using ReLU and its softer varieties because of higher performance shown in a number of problem domains and models.\nIs the noise injection clean in that it is a good pseudo noise source and is it configured properly with whatever SGD methods and libraries being used.\n\nI am training a Multilayer Neural Nets with 146 samples (97 for training set, 20 for validation set and 29 for testing set). I am using: automatic differentiation, SGD method, fixed learning rate + momentum term, logistic function, quadratic cost function, L1 and L2 regularization technique, adding some artificial noise 3%.\nThe question indicated the best results for the current training set and model.\n\nThe best result was obtained using a lambda of 0.001 without regularization.\n\nThat's the starting point for improving further.  Since complete automated convergence is not yet something libraries and frameworks provide, the engineer normally has to converge on the combination of techniques and settings that best help the network's convergence.\n", "type": 2, "id": "8632", "date": "2018-10-26T15:28:36.860", "score": 4, "comment_count": 4, "parent_id": "8607"}, "13474": {"line": 9280, "body": "You have a small dataset.  Should you even be using neural nets?  Have you done any diagnostics to see if you even have enough data?  Are you using the right metric?  Accuracy is not always the correct metric.  Which weights are you retaining?  You will overfit if you save the weights that produce the lowest training error.  Save the weights that produced the lowest validation error.  L1, L2, and dropout are all great.  So many things not described in the problem...\nhttp://www.ultravioletanalytics.com/blog/kaggle-titanic-competition-part-ix-bias-variance-and-learning-curves\nI'm wondering why you're not trying interpretable models to see if the resulting weights for the features make sense.  Also if your comparing all those models and parameters, set your random initial starting point to be the same by setting the seed.  I also hope you are using the same training set for each model.\nYou probably need more data...\n", "type": 2, "id": "13474", "date": "2019-07-20T21:44:31.957", "score": 1, "comment_count": 0, "parent_id": "8607"}, "21146": {"line": 13938, "body": "Your network without regularization does not appear to be over fitting but rather it appears to be converging to a minima. I am actually a bit surprised it is doing as well as it is given that your data set is small. So You don't need regularization. If you want to improve the accuracy you might try using an adjustable learning rate. The Keras call back ReduceLROnPlateau can be used for this. Documentation is here. Also use the callback ModelCheckpoint to save the model with the lowest validation loss. Documentation is here. It would help a lot if you posted your model code. I have found if you do encounter over fitting dropout works more effectively than regularization.\n", "type": 2, "id": "21146", "date": "2020-05-14T08:06:51.863", "score": 0, "comment_count": 0, "parent_id": "8607"}}}
{"line": 5741, "body": "What are some ways to design a neural network with the restriction that the $L_1$ norm of the output values must be less than or equal to 1? In particular, how would I go about performing back-propagation for this net?\nI was thinking there must be some \"penalty\" method just like how in the mathematical optimization problem, you can introduce a log barrier function as the \"penalty function\"\n", "type": 1, "id": "8518", "date": "2018-10-19T11:55:43.517", "score": 6, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "backpropagation", "objective-functions", "constrained-optimization"], "title": "How do we design a neural network such that the $L_1$ norm of the outputs is less than or equal to 1?", "answer_count": 2, "views": 1287, "accepted_answer": null, "answers": {"8565": {"line": 5776, "body": "The term 'size' isn't applicable to the tensor output of a network or network.  These are the qualities.\n\nRank $N$ that defines the rank of each tensor instance in $\\mathbb{R}^N$\nRanges of the indices to the dimensions from $1$ to $N$\nRanges of the scalar values that comprise the tensor instance \u2014 If they are discrete rather than real (approximated by floating point or fixed point numbers), then the range is the description of the permissible ordinal values.\n\nThe question may be referring to this last quality.\nThe imposition of a penalty for values that are in the range of the numeric type used as the output of the last activation function but not in the allowable range of output for the desired trained function works in a limited way.  It skews the output distribution with respect to the natural distribution of possible learning states and therefore can easily interfere with convergence quality or speed or both.\nThere are a number of techniques that map natural output distribution with constrained ranges, but it must be done without skewing the distribution upstream from the technique used, to avoid negatively impacting favorable convergence properties of the artificial network.\nOne simple case that can be described here is when the number of possible output states is in the set of $2^i$ where $i \\in$ { positive integers }.  In such a case, the final layer of the network can be $i$ threshold activation functions with 1 or -1 as possible output values.\nIn that case, the ordinal then becomes\n$$o = \\sum_{n=0}^i \\frac {y_n + 1} {2},$$\nwhere $y_n$ is the output from activation function index $n$.\n", "type": 2, "id": "8565", "date": "2018-10-21T19:37:14.457", "score": -3, "comment_count": 1, "parent_id": "8518"}, "14030": {"line": 9737, "body": "Penalty (barrier function) is perfectly valid and simplest method for simplex type constraint (L1 norm is simplex constraint on absolute values). Any type of barrier function may work, logarithmic, reciprocal or quadratic. All of them supported by any major framework(pytorch, tensorflow), just add them to loss function. You would need some hyperparameter tuning for the scale factor of penalty.\nThere is more efficient, though more complex way to do it. Instead of putting constraint you can automatically output value wich satisfy simplex constraint:\nAssume that L1 norm constraint is $\\left \\|v\\right \\|_1 \\leq 1$, $v \\in \\mathbb{R}^n$\n\nput $sigmoid(v_i)$ activation on output to norm elements to [-1, 1]\nadd slack (fake) variable element $v_{n+1} = 1 - \\sum_{1}^{n}  v_i $\nproject new $v{}'\\in \\mathbb{R}^{n+1}$, $v{}'_i = |v_i|,1\\leq  i  \\leq n+1$ onto unit simplex with standard algorithm (also here)\n\nBackpropagation of last step may require differentiable sorting, which is missing in most of frameworks, you may have to look for open sourced implementation, for example extract it from here or use some automatic differentiation package. Both require some substantial code reading/debugging. However in my experience assuming constant $\\Delta$ also works in many cases, in that case  differentiable sorting is not needed. Intuition behind constant $\\Delta$  is that  $\\Delta$  could be chosen such way that there is some interval on which it's value doesn't affect sorting order.\n", "type": 2, "id": "14030", "date": "2019-08-18T08:53:12.107", "score": 2, "comment_count": 0, "parent_id": "8518"}}}
{"line": 6296, "body": "Consider a perceptron where $w_0=1$ and $w_1=1$:\n\nNow, suppose that we use the following activation function\n\\begin{align}\nf(x)=\n\\begin{cases}\n1, \\text{ if }x =1\\\\\n0, \\text{ otherwise}\n\\end{cases}\n\\end{align}\nThe output is then summarised as:\n\\begin{array}{|c|c|c|c|}\n\\hline\nx_0 & x_1 & w_0x_0 + w_1x_1 & f( \\cdot )\\\\ \\hline\n0 & 0 & 0 & 0 \\\\ \\hline\n0 & 1 & 1 & 1 \\\\ \\hline\n1 & 0 & 1 & 1 \\\\ \\hline\n1 & 1 & 2 & 0 \\\\ \\hline\n\\end{array}\nIs there something wrong with the way I've defined the activation function?\n", "type": 1, "id": "9417", "date": "2018-12-08T21:09:21.717", "score": 5, "comment_count": 0, "tags": ["neural-networks", "activation-function", "perceptron", "xor-problem"], "title": "Why can't the XOR linear inseparability problem be solved with one perceptron like this?", "answer_count": 5, "views": 707, "accepted_answer": "9538", "answers": {"9538": {"line": 6373, "body": "It can be done.\nThe activation function of a neuron does not have to be monotonic. The activation that Rahul suggested can be implemented via a continuously differentiable function, for example $ f(s) = exp(-k(1-s)^2) $ which has a nice derivative $f'(s) = 2k~(1-s)f(s)$. Here, $s=w_0~x_0+w_1~x_1$. Therefore, standard gradient-based learning algorithms are applicable.\nThe neuron's error is $ E = \\frac{1}{2}(v-v_d)^2$,\nwhere $v_d$ - desired output, $v$ - actual output. The weights $w_i, ~i=0,1$ are initialized randomly and then updated during training as follows\n$$w_i \\to w_i - \\alpha\\frac{\\partial E}{\\partial w_i}$$\nwhere $\\alpha$ is a learning rate. We have\n$$\\frac{\\partial E}{\\partial w_i} = (v-v_d)\\frac{\\partial v}{\\partial w_i}=(f(s)-v_d)~\\frac{\\partial f}{\\partial s}\\frac{\\partial s}{\\partial w_i}=2k~(f(s)-v_d)(1-s)f(s)~x_i$$\nLet's test it in Python.\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nFor training, I take a few points randomly scattered around $[0, 0]$, $[0, 1]$, $[1, 0]$, and $[1, 1]$. \nn = 10\nsd = [0.05, 0.05]\n\nx00 = np.random.normal(loc=[0, 0], scale=sd, size=(n,2))\nx01 = np.random.normal(loc=[0, 1], scale=sd, size=(n,2))\nx10 = np.random.normal(loc=[1, 0], scale=sd, size=(n,2))\nx11 = np.random.normal(loc=[1, 1], scale=sd, size=(n,2))\n\nx = np.vstack((x00,x01,x10,x11))\ny = np.vstack((np.zeros((x00.shape[0],1)), \n               np.ones((x01.shape[0],1)), \n               np.ones((x10.shape[0],1)), \n               np.zeros((x11.shape[0],1)))).ravel()\n\nind = np.arange(len(y))\nnp.random.shuffle(ind)\n\nx = x[ind]\ny = y[ind]\nN = len(y)\n\nplt.scatter(*x00.T, label='00')\nplt.scatter(*x01.T, label='01')\nplt.scatter(*x10.T, label='10')\nplt.scatter(*x11.T, label='11')\nplt.legend()\nplt.show()\n\n\nActivation function:\nk = 10\n\ndef f(s):\n    return np.exp(-k*(s-1)**2)\n\nInitialize the weights, and train the network:\nw = np.random.uniform(low=0.25, high=1.75, size=(2))\n\nprint(\"Initial w:\", w)\n\nrate = 0.01\nn_epochs = 20\n\nerror = []\nfor _ in range(n_epochs):\n    err = 0\n    for i in range(N):\n        s = np.dot(x[i],w)\n        w -= rate * 2 * k * (f(s) - y[i]) * (1-s) * f(s) * x[i]\n        err += 0.5*(f(s) - y[i])**2\n    err /= N\n    error.append(err)\n\nprint('Final w:', w)\n\nThe weights have indeed converged to $w_0=1,~w_1=1$:\nInitial w: [1.5915165  0.27594833]\nFinal w: [1.03561356 0.96695205]\n\nThe training error is decreasing:\nplt.scatter(np.arange(n_epochs), error)\nplt.grid()\nplt.xticks(np.arange(0, n_epochs, step=1))\nplt.show()\n\n\nLet's test it. I create a testing set in the same way as the training set. My test data are different from my training data because I didn't fix the seed.\nx00 = np.random.normal(loc=[0, 0], scale=sd, size=(n,2))\nx01 = np.random.normal(loc=[0, 1], scale=sd, size=(n,2))\nx10 = np.random.normal(loc=[1, 0], scale=sd, size=(n,2))\nx11 = np.random.normal(loc=[1, 1], scale=sd, size=(n,2))\n\nx_test = np.vstack((x00,x01,x10,x11))\ny_test = np.vstack((np.zeros((x00.shape[0],1)), \n               np.ones((x01.shape[0],1)), \n               np.ones((x10.shape[0],1)), \n               np.zeros((x11.shape[0],1)))).ravel()\n\nI calculate the root mean squared error, and the coefficient of determination (R^2 score):\ndef fwd(x,w):\n    return f(np.dot(x,w))\n\nRMSE = 0\n\nfor i in range(N):\n    RMSE += (fwd(x_test[i],w) - y_test[i])**2\n\nRMSE = np.sqrt(RMSE/N)\n\nprint(\"RMSE\", RMSE)\n\nybar = np.mean(y)\n\nS = 0\nD = 0\nfor i in range(N):\n    S += (fwd(x_test[i],w) - y_test[i])**2\n    D += (fwd(x_test[i],w) - ybar)**2\n\nr2_score = 1 - S/D\nprint(\"r2_score\", r2_score)\n\nResult:\nRMSE 0.09199468888373698\nr2_score 0.9613632278609362\n\n... or I am doing something wrong? Please tell me.\n", "type": 2, "id": "9538", "date": "2018-12-15T18:14:13.280", "score": 4, "comment_count": 1, "parent_id": "9417"}, "9548": {"line": 6378, "body": "Another activation function that could be used for this problem:\n$$f(x) = \\underset{i}{max}(x_i) - \\underset{i}{min}(x_i)$$\nIt's not continuous, no backpropagation, sorry. Some other learning algorithm is required. \nHowever, this answers the question, if an XOR can be solved with one neuron. Maybe this function is a solution of some learning problem with weights. Something like\n$$f(x) = max(w_0x_0,w_1x_1) - min(w_0x_0,w_1x_1)$$\nI don't know how this creature is generalizable to other tasks, and how much learning can be done by just manipulating maxima and minima of weighted inputs. Any ideas?\n", "type": 2, "id": "9548", "date": "2018-12-16T11:34:00.113", "score": 0, "comment_count": 0, "parent_id": "9417"}, "9421": {"line": 6299, "body": "Indeed I think the problem is with the way you've defined the activation function. By selecting it arbitrarily, you could solve many specific problems. In practice, activation functions used are monotonic. It keeps the error function convex at a per-layer level. In theory though I'm not sure exactly what Rosenblatt has claimed so it might be worth calling him\n", "type": 2, "id": "9421", "date": "2018-12-09T01:44:54.313", "score": 2, "comment_count": 2, "parent_id": "9417"}, "9428": {"line": 6305, "body": "The main problems are that your activation function is not monotonic (as pointed out by csrev), and that it is not continuously differentiable. These make it very difficult / impossible to use standard gradient-based learning algorithms. \nSo yes, there may exist a good solution of weight values, but it is very difficult to find or approximate those weight values automatically through a learning algorithm. Also note that it completely breaks down as soon as you have a tiny error in one of the weights, even if it is approximate very closely; if one of the weights has a value of $0.999$ rather than $1.0$, the solution breaks down completely.\n", "type": 2, "id": "9428", "date": "2018-12-09T13:02:59.553", "score": 2, "comment_count": 0, "parent_id": "9417"}, "9540": {"line": 6374, "body": "I'm also working on a perceptron that is able to solve the XOR Problem, and I get interesting results using sine as an activation function, and using the derivative to make the perceptron learn. But you will need bias to make sure the perceptron is able to solve the problem.\n", "type": 2, "id": "9540", "date": "2018-12-15T20:29:43.417", "score": 0, "comment_count": 1, "parent_id": "9417"}}}
{"line": 5869, "body": "I have trained a convolutional neural network on images to detect emotions. Now I need to use the same network to extract features from the images and use them to train an LSTM. The problem is: the dimensions of the top layers are: [None, 4, 4, 512] or [None, 4, 4, 1024]. Therefore, extracting features from this layer will result in a 4 x 4 x 512 = 8192 or 4 x 4 x 1024 = 16384 dimensional vector for each image. Clearly, this is not what I want.\nTherefore, I would like to know what to do in this case and how to extract features that are of reasonable size. Should I apply global average pooling to the activation or what?\nAny help is much appreciated!\n", "type": 1, "id": "8716", "date": "2018-11-01T16:56:50.253", "score": 3, "comment_count": 0, "tags": ["convolutional-neural-networks", "long-short-term-memory", "feature-selection", "dimensionality"], "title": "Problem extracting features from convolutional layer where the dimensions are big for feature maps", "answer_count": 2, "views": 58, "accepted_answer": null, "answers": {"8734": {"line": 5884, "body": "I think it's okay to let CNN extract a big number of feature maps, then you can reduce the features from it. To reduce features from CNN result, you can do feature selection or feature extraction. Some people prefer to use feature selection and select the most significant feature maps, for examples:\n\nAction unit selective feature maps in deep networks for facial expression recognition\nConvolutional Neural Network Simplification Based on Feature Maps Selection\n\nHope it helps.\n", "type": 2, "id": "8734", "date": "2018-11-02T06:50:45.177", "score": 0, "comment_count": 0, "parent_id": "8716"}, "27438": {"line": 18828, "body": "In addition to the first answer about feature selection, you could also add a global max or average pooling layer at the end of your network. This would reduce the dimensionality to 512 or 1024. If that's still too much, another option would be to add an additional convolutional layer with reduced channels and then do the global pooling. You will have to experiment with which option is best for your data.\n", "type": 2, "id": "27438", "date": "2021-04-20T20:58:10.597", "score": 0, "comment_count": 0, "parent_id": "8716"}}}
{"line": 5259, "body": "I developed a CNN for image analysis. I've around 100K labeled images.  I'm getting a accuracy around 85% and a validation accuracy around 82%, so it looks like the model generalize better than fitting. So, I'm playing with different hyper-parameters: number of filters, number of layers, number of neurons in the dense layers, etc. \nFor every test, I'm using all the training data, and it is very slow and time consuming.  \nIs there a way to have an early idea about if a model will perform better than another?\n", "type": 1, "id": "7853", "date": "2018-09-06T12:37:06.217", "score": 2, "comment_count": 4, "tags": ["machine-learning", "training", "deep-neural-networks"], "title": "Is there a way of pre-determining whether a CNN model will perform better than another?", "answer_count": 4, "views": 147, "accepted_answer": null, "answers": {"7855": {"line": 5261, "body": "I would like you to try with following changes in the model. \n\nIntroduce batch normalization layer in the model. \nTry with the batch size of 32 - 64\nUse different architecture, Like VGG, Resnet, etc..\n\nThere is no full proof answer to this question but you can get best by trying some know strategies.\n", "type": 2, "id": "7855", "date": "2018-09-06T13:35:41.060", "score": 0, "comment_count": 1, "parent_id": "7853"}, "15687": {"line": 10237, "body": "I can't comment but here are a few suggestions:\n\nPlay with lr and lr finder\nUse a pretrained model\nUse architecture search or use something like efficientnet b6\nUse swish over relu\nTry different optimizers\nTry Bayesian Optimization\n\n", "type": 2, "id": "15687", "date": "2019-10-02T05:01:31.970", "score": 0, "comment_count": 0, "parent_id": "7853"}, "15684": {"line": 10235, "body": "Your description sounds like something similar to Imagenet dataset. According to this website, the state-of-the-art top-1 accuracy just as high as 86%, not much higher than yours. There are plenty of methods to improve accuracy. I would suggest you to read the paper or github listed in SOTA to find ideas that best fits your situation. \n", "type": 2, "id": "15684", "date": "2019-10-02T00:01:32.687", "score": 0, "comment_count": 0, "parent_id": "7853"}, "32133": {"line": 21067, "body": "The simple answer to your question is \"No\" with a caveat.\nThe caveat is that there are signs that your network is never going to perform well.  For example, the epoch accuracy fails to improve or even consistently declines over the first several epochs, or the validation accuracy is flat or declining.  It could be that the validation loss starts high and just keeps increasing from the beginning.  These are all bad signs.\nOutside of this, however, it's very tough to know the model won't work well in the long run.  For example, we have a model we built for solving a set of CAPTCHAs.  The regression portions of that converged very quickly, but the portions that solved the rest of the CAPTCHA took something like 18 hours before they converged.  Honestly, we only ran it that long because it was the end of the day and the regression piece looked so promising; there was nothing in the training behavior of the CAPTCHA solver that looked like it would work (even though our intuition was that it should.)\nIn the end, we have a 96%+ accuracy CAPTCHA solver that we likely would have killed if we had watched it train for more than 10 or 15 minutes.\n", "type": 2, "id": "32133", "date": "2021-10-21T14:14:24.250", "score": 1, "comment_count": 0, "parent_id": "7853"}}}
{"line": 6362, "body": "In this article here, the writer claims that a new type of neural net is required to deal with data that is both continuous, and also sparsely sampled. \nIt was my understanding that this was the entire purpose of techniques that use neural nets, to make assumptions about a system with a non-continuous data set.\nSo why do we need to switch to a non-layered design to deal with these data sets better?\n", "type": 1, "id": "9520", "date": "2018-12-14T15:08:08.750", "score": 7, "comment_count": 2, "tags": ["neural-networks"], "title": "Why do layered neural nets struggle with continous data?", "answer_count": 1, "views": 201, "accepted_answer": null, "answers": {"9634": {"line": 6427, "body": "They struggle because if your network have an inductive bias towards modeling datasets which are described with ODEs well, you will learn faster, and with smaller dataset. I think, this what the authors of the original article meant.\nIn a similar way, CNNs recognize images much better, because their features are translation invariant whereas fully connected net needs to learn to recognize a cat in each different position from scratch.\n", "type": 2, "id": "9634", "date": "2018-12-21T03:01:22.603", "score": 0, "comment_count": 0, "parent_id": "9520"}}}
{"line": 4580, "body": "In image classification we are generally told the main reason of using CNN's is that densely connected NN's cannot handle so many parameters (10 ^ 6 for a 1000 * 1000 image). My question is, is there any other reason why CNN's are used over DNN's (densely connected NN)? \nBasically if we have infinite resources will DNN trump CNN's or are CNN's inherently well suited for image classification as RNN's are for speech. Answers based either on mathematics or experience on the field is appreciated.\n", "type": 1, "id": "6952", "date": "2018-06-29T14:09:14.537", "score": 7, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks"], "title": "CNN's vs Densely Connected NN's", "answer_count": 3, "views": 484, "accepted_answer": null, "answers": {"6956": {"line": 4584, "body": "That is not the actual reason , \"convolution\" layers are inspired by cells in visual-system. This is derived from the work of hubel-wiesel.\nfor more information check hubel-wiesel experiment.\n", "type": 2, "id": "6956", "date": "2018-06-29T17:10:42.347", "score": 0, "comment_count": 6, "parent_id": "6952"}, "9269": {"line": 6209, "body": "The keyword here is Parameter Sharing or Weight Sharing across various image portions.\nIf we take a simple example of grayscale binary image of an alphabet 'F', it is a combination of multiple patterns. The patterns here are vertical lines and horizontal lines. These patterns are based on relation between intensities of contiguous cells. This relation between contiguous cells is established using a weight matrix.  \nAlso, for identifying multiple horizontal lines, we dont need multiple node-sets in hidden dense layer trying to identify different horizontal lines in the image. The pattern is same but present in different locations. Hence the sharing of weights comes into picture. \nIn the 1st hidden layer, encode the pattern horizontal line in a weight matrix(learnt during training and used in testing). Place it over small grid and check for presence. As this matrix is slided and tested across the image, the presence of horizontal lines is marked in various locations. This weight matrix is called a kernel. \nCombining the above points, kernel provides a way of handling parameter / weight sharing between contiguous cells to identify patterns. Dense layer instead of kernels would solve it eventually but start in a random manner. Since a efficient way was identified, it is being used. \nNext to identify vertical lines, another kernel needed and slide across.\nSuppose next we have dense layer as 2nd hidden layer. This layer looks for combination of patterns ('p' horizontal lines and 'q' vertical lines in this case for 'F') present and learns combinations to identify output. \nJust to compare with traditional programming, kernels are like regular expressions. dense layers are like loops. Just sharing my thoughts. Any better explanation is welcome.\n", "type": 2, "id": "9269", "date": "2018-11-30T05:41:19.523", "score": 2, "comment_count": 0, "parent_id": "6952"}, "9273": {"line": 6213, "body": "Convolution Neural Networks can detect more of the spatial features compared to Densely Connected Network. Consider this in any given real world image the pixel values of neighboring cells to do not vary  highly, But when this image are passed to a Densely Connected Neural network for training the spatial relations between neighboring pixels is lost as all other cells can heavily influence the training whereas in Convolutional networks due to operation of convolution by local information is preserved, it is called local connectivity.\n", "type": 2, "id": "9273", "date": "2018-11-30T07:03:11.833", "score": 2, "comment_count": 0, "parent_id": "6952"}}}
{"line": 6401, "body": "I've been given an assignment to create a neural network that will suggest a Croatian word for a word given in any other European language (out of those found here). The words are limited to drinks you can find on a bar menu.\nI've looked at many NN examples, both simple and complex, but I'm having trouble with understanding how to normalize the input.\nFor example, words \"beer\", \"birra\" and \"cervexa\" should all translate to \"pivo\". If I include those 3 in the training set, and after the network has finished training I input the word \"bier\", the output should be \"pivo\" again.\nI'm not looking for a working solution to this problem, I just need a nudge in the right direction regarding normalization.\n", "type": 1, "id": "9588", "date": "2018-12-18T16:34:16.317", "score": 3, "comment_count": 0, "tags": ["neural-networks"], "title": "Translating a single word Neural Networks", "answer_count": 2, "views": 84, "accepted_answer": null, "answers": {"9591": {"line": 6403, "body": "So, the normalization as you call it is just encoding the word into some vector of fixed length. After you encode it, you should be able to decode it and match the words. You should google a little, for a seq2seq problems and Encoder/Decoder structure. There are encoder-decoder frameworks out there and lots of resources. You don't care about a language so just pretend that words like \"beer\" and \"birra\" have the same meaning in Croatian and in this case, it is \"pivo\".\n", "type": 2, "id": "9591", "date": "2018-12-18T20:25:56.540", "score": 0, "comment_count": 0, "parent_id": "9588"}, "9716": {"line": 6478, "body": "The training examples are not mentioned, but, assuming the assignment involves supervised learning, normalization is, in this case, mapping the inputs and outputs to binary values.\nTranslation, in this simplified context, is a map of strings to strings, which isn't really an AI device by contemporary standards. Since an artificial network is given as a requirement for the solution, the goal is to train the network to act like a hash map.\n$$ \\Big\\{..., \\;\n\\big(\n[\\text{\"beer\", \"birra\", \"cervexa\"}]:\n\\text{\"pivo\"}\n\\big)\n, \\; ...\\Big\\} $$\n$$ \\Downarrow $$\n$$ \\Big\\{..., \\; \\big(\\text{\"beer\" : \"pivo\"}), (\\text{\"birra\" : \"pivo\"}), (\\text{\"cervexa\" : \"pivo\"}\\big), \\; ...\\Big\\} $$\nAssuming there is a list of pairs $(\\mathcal{X}, \\mathcal{Y})$, where $\\mathcal{X}$ is the word in any of the source languages and $\\mathcal{Y}$ is the word in the target language, and the vocabulary of both is known to have word counts of $X$ and $Y$ respectively, one can determine the input and output layer widths (cell counts) $w_1$ and $w_{\\ell}$, where $\\ell$ is the number of network layers.\n$$w_1 = \\text{ceiling} ( \\log_2{X} )$$\n$$w_{\\ell} = \\text{ceiling} ( \\log_2{Y} )$$\nThe assignment of source words to binary values is arbitrary. You can sort them by language and then by whatever common character encoding used (such as UTF-8) just for debugging purposes. The same is true of target words.\nA simple MLP (multi-layer perceptron) with mini-batch training using basic propagation map be appropriate.\n", "type": 2, "id": "9716", "date": "2018-12-27T23:19:10.430", "score": 1, "comment_count": 0, "parent_id": "9588"}}}
{"line": 6422, "body": "Problem: Fraud detection\nTask : classifying transactions as either fraud/ Non-fraud using GAN \n", "type": 1, "id": "9624", "date": "2018-12-20T17:30:40.910", "score": 1, "comment_count": 1, "tags": ["neural-networks", "generative-adversarial-networks"], "title": "Can we implement GAN (Generative adversarial neural networks) for classication problem like Fraud detecion?", "answer_count": 2, "views": 528, "accepted_answer": "9683", "answers": {"9626": {"line": 6423, "body": "Yes, although the traditional technique is to use Anomaly Detection for this sort of problem. The reason for that is that the space of possible frauds is very much larger than the space of legitimate transactions and so it is difficult to model the former. The usual idea is to thoroughly explore the smaller space of legitimate transactions and flag as fraudulent anything outside of that space.\nUsing the GAN method could be interesting as it might identify novel fraud techniques, and those are always useful to know. \n", "type": 2, "id": "9626", "date": "2018-12-20T21:07:17.450", "score": -1, "comment_count": 6, "parent_id": "9624"}, "9683": {"line": 6457, "body": "Generative networks was the first popular class of topologies that had compound feedback, corrective signaling at more than one layer.  In popular network designs such as MLPs (multilayer perceptrons), CNNs (convolutional neural networks), and LSTM (long short term memory) networks, the backpropagation is a single layer mechanism.  The mechanism distributes corrective a signal creating a closed loop that ideally converges to an optimal network behavior.\nThe speed, accuracy, and reliability of convergence of networks in an AI system depend on a number of engineering choices at a lower level of design and development.\n\nSelection of algorithm variants and extensions\nSelection of data set\nDrawing of the training samples\nData normalization\nInitialization\nAssignment of learning rate or learning rate policy\nOther hyper-parameters\n\nGenerative networks are designed to achieve a higher level balance between two networks, each of which have their own backpropagation, and thus have a compound feedback topology.\nConsidering the original GAN approach in the context of fraud detection, a discriminative network $D$, in a way, detects fraudulence originating from the generative network $G$, and, in a way, $G$ learns authenticity from the corrective signal provided by $D$, even if the authenticity learned is superficial.  However, in the case of credit card fraud detection from a numerical set of data, the fraudulence is not internal to the system to be designed as in the case of GAN.  Without both the $G$ and $D$ networks, the design is not a generative network.\nSince the comment stated, \"I have to explore GAN for this problem,\" the key question is this.\n\nIf this system is to benefit from a GAN, what would the GAN generate that could contribute to the solution?\n\nIf the answer is nothing, then a GAN is not the correct approach.  A discriminative network without a generative network is not a GAN.  If this question is from an assignment to train a network using a static and already existing data set, a MLP may suffice, depending on the expectations of the teaching staff.  If that is the case, the goal will be to analyze the data set using statistical tools to determine if it is already normalized well enough to lead to excellent convergence accuracy, speed, and reliability.  If normalization is indicated, find ways to present the data to the input layer of the MLP in such a way that each input is relevant and has a well distributed range of values across the data type.\nEnsure there is no data loss in this process, and remove redundancy where it can easily be removed.  By redundancy removal is meant, if one of the dimensions of data have only four possible values of between 10 and 1,000 bytes, encode the four possible values in two bits because to do so removes unnecessary burden from the training.\nSimilarly, the output layer must be chosen to produce an encoded set of possible output values and the decoding must be written to produce usable output.  In this case, it appears that the labels are, Fraudulent and Authentic.  Which means that the last layer would be a single binary threshold cell.\nFor this simple MLP approach and for many of the approaches mentioned next, there are examples available in Python, Java, C++ and other languages from which implementation can be initiated.  If the design must stand up to a more realistic fraud detection scenario, either for credit cards, junk mail, banking, log-in, or communications those experienced with fraud detection know that the above approach is entirely insufficient except as a starting point for learning.  In such a case, these considerations further complicate the design of the solution.\n\nFraudulence and authenticity is not necessarily sufficient information for investigators.\n    The likelihood of fraudulence and the reason why fraudulence is suspected is also pertinent for end users of production output,\n    so that security holes can be blocked and investigation may be directed toward prosecution or some other set of remedies.\nFraudulence is usually a security breach phenomena that involves continuous improvement in the attacks until the attackers\n    are located and their work is interrupted by arrest.\n    What initially works as fraud detection may not work a week, a day, an hour, or a minute later.\nFraud detection, if it to be useful in a real financial context, must be part of a fraud prevention system,\n    and training on a static data set may be meaningless in that larger usage environment.\n\nIn AI systems that work in real production environments, it may be worth investigating using an LSTM network in combination with a reinforcement learner.  Such may be over a beginner's level of design and implementation experience, but using either an LSTM network or a basic Q-learning approach instead of both of them together may be a reasonable starting point for dedicated students.  If the answer to the question about what a GAN could generate that would be of use must be yes, the following ideas may be helpful.\n\nA generative network could generate transaction data with a distribution that resembles authentic transactions, and such has been successful in the past.\nA generative network could generate transaction data with a distribution that resembles attacks on an account by a fraudulent user. That has been done too.\nA generative network could theoretically generate hacker agents, although this may not have yet been done.\nA generative network could be used to generate patterns of authentic transactions based on some authenticity measure other than the data set mentioned, and the trained discriminative network could then be employed in some way in a fraud detection system in production, although it is unclear what advantage would be gained.\n\nThe reason a GAN was suggested would need to be clarified before this answer could provide a more exact direction for AI system topology and project approach.\n", "type": 2, "id": "9683", "date": "2018-12-26T00:06:01.413", "score": 1, "comment_count": 0, "parent_id": "9624"}}}
{"line": 5405, "body": "I'm trying to use a CNN to analyse statistical images. These images are not 'natural' images (cats, dogs, etc) but images generated by visualising a dataset. The idea is that these datasets hopefully contain patterns in them that can be used as part of a classification problem.\n\nMost CNN examples I've seen have one of more pooling layers, and the explaination I've seen for them is to reduce the number of training elements, but also to allow for some locational independance of an element (e.g. I know this is an eye, and can appear anywhere in the image).\nIn my case location is important and I want my CNN to be aware of that. ie. the presence of a pattern at a specific location in the image means something very specific compared to if that feature or pattern appears elsewhere.\nAt the moment my network looks like this (taken from an example somewhere):\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 196, 178, 32)      896       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 196, 178, 32)      0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 98, 89, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 96, 87, 32)        9248      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 96, 87, 32)        0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 48, 43, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 46, 41, 64)        18496     \n_________________________________________________________________\nactivation_3 (Activation)    (None, 46, 41, 64)        0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 23, 20, 64)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 29440)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                942112    \n_________________________________________________________________\nactivation_4 (Activation)    (None, 32)                0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 99        \n_________________________________________________________________\nactivation_5 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 970,851\nTrainable params: 970,851\nNon-trainable params: 0\n_________________________________________________________________\n\nThe 'images' I'm training on are 180 x 180 x 3 pixels and each channel contains a different set of raw data.\nWhat strategies are there to improve my CNN to deal with this? I have tried simply removing some of the pooling layers, but that greatly increased memory and training time and didn't seem to really help.\n", "type": 1, "id": "8038", "date": "2018-09-19T11:22:14.860", "score": 1, "comment_count": 2, "tags": ["deep-learning", "convolutional-neural-networks", "keras"], "title": "CNN Pooling layers unhelpful when location important?", "answer_count": 2, "views": 68, "accepted_answer": null, "answers": {"8052": {"line": 5416, "body": "1X1 convolution might improve accuracy as they reduce dimension in filter space.\nGoogle inception V3 architecture would be a good starting point.\nhttps://arxiv.org/pdf/1512.00567.pdf\n", "type": 2, "id": "8052", "date": "2018-09-19T22:46:56.383", "score": -1, "comment_count": 0, "parent_id": "8038"}, "8040": {"line": 5407, "body": "Pooling doesn't completely remove information about location of features within your image. If you don't want to use pooling but want to reduce the size of your neural net layers you should try a value of stride greater than 1 in your convolutional layers. e.g:\nkeras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n", "type": 2, "id": "8040", "date": "2018-09-19T13:14:11.860", "score": -1, "comment_count": 2, "parent_id": "8038"}}}
{"line": 6297, "body": "I'm trying to train a Siamese network to check if two images are similar. My implementation is based on this. I find the Euclidian distance of the feature vectors(the final flattened layer of my CNN) of my two images and train the model using the contrastive loss function. \nMy question is, how do I get a binary output from the Siamese network for testing (1 if it two images are similar, 0 otherwise). Is it just by thresholding the Euclidian distance to check how similar the images are? If so, how do I go about selecting the threshold? If I wanted to measure the training and validation accuracies, the threshold would have to be increased as the network learns better. Is there a way to learn this threshold for a given dataset? \nI would appreciate any leads, thank you.\n", "type": 1, "id": "9418", "date": "2018-12-08T22:02:53.463", "score": 1, "comment_count": 0, "tags": ["machine-learning", "deep-learning", "convolutional-neural-networks"], "title": "How to get a binary output from a Siamese Neural Network", "answer_count": 2, "views": 980, "accepted_answer": null, "answers": {"9420": {"line": 6298, "body": "I am working on something rather similar in the sense that I get a continuous value (between 0 and 1) as output that I want to be binary. I think you should be using the ROC curve (Receiver-Operating-Characteristic) instead of the accuracy. If you want a single value take the area-under-the-curve. The ROC curve is basically true-prositive-rate vs false-positive-rate as you test for different values of threshold. You could also (or instead) use precision vs recall.\nAlso, the optimal threshold does not depend as much on the dataset as it depends on your neural network. As you said this value will change as your network learns. Simply use the outputted value and test different thresholds. \nIf do want to keep the accuracy, simply pick the threshold for which is it best (at the given iteration). Keep in mind that you should not define the threshold from the validation set as you would be cheating. \nps: from rereading it seems like you were implying to learn the threshold from inside the network. I believe it should just be a post-process\n", "type": 2, "id": "9420", "date": "2018-12-09T01:22:29.487", "score": 1, "comment_count": 2, "parent_id": "9418"}, "14201": {"line": 9867, "body": "Don't use the Euclidian distance as the similar/dissimilar factor, you'll get better results if you put a couple Dense layers at the top of your Siamese network.  You don't mention how large your feature vectors are, but if they are 128D face encodings, this is what I used:\n# dist_euclid_layer is a layers.Lambda that performs the euclid distance on the input\nfirst_dense = layers.Dense(512, activation='relu')(dist_euclid_layer)\ndrop_one = layers.Dropout(0.5)(first_dense)\noutput_dense = layers.Dense(2, activation='sigmoid')(drop_one)\nmodel = models.Model(inputs=[input_layer], outputs=output_dense)\nmodel.compile(loss=losses.binary_crossentropy, optimizer=opt, metrics=['accuracy'])\n\nIn training, use [0,1] or [1,0] for Y to indicate different / similar.\nThen in production, use the np.argmax to find the positive match:\npredictions_raw = model.predict(vstack_batch_input, batch_size=len(batch_input))\npredictions = np.argmax(predictions_raw,axis=1)\ntot_matches = np.sum(predictions)\nif tot_matches == 1:\n    # ... good times, whatever face[N] has prediction[N] == 1 is the matching face\n\n", "type": 2, "id": "14201", "date": "2019-08-29T19:37:56.087", "score": 0, "comment_count": 0, "parent_id": "9418"}}}
{"line": 6779, "body": "I'm quite new to the field of computer vision and was wondering what are the purposes of having the boundary boxes in object detection.\nObviously, it shows where the detected object is, and using a classifier can only classify one object per image, but my question is that\n\nIf I don't need to know 'where' an object is (or objects are) and just interested in the existence of them and how many there are, is it possible to just get rid of the boundary boxes?\n\nIf not, how does bounding boxes help detect objects? From what I have figured is that a network (if using neural network architectures) predicts the coordinates of the bounding boxes if there is something in the feature map. Doesn't this mean that the detector already knows where the object is (at least briefly)? So, continuing from question 1, if I'm not interested in the exact location, would training for bounding boxes be irrelevant?\n\nFinally, in architectures like YOLO, it seems that they predict the probability of each class on each grid (e.g. 7 x 7 for YOLO v1). What would be the purpose of bounding boxes in this architecture other than that it shows exactly where the object is? Obviously, the class has already been predicted, so I'm guessing that it doesn't help classify better.\n\n\n", "type": 1, "id": "10177", "date": "2019-01-25T06:39:39.863", "score": 4, "comment_count": 0, "tags": ["machine-learning", "convolutional-neural-networks", "object-detection", "yolo", "bounding-box"], "title": "What's the role of bounding boxes in object detection?", "answer_count": 2, "views": 1579, "accepted_answer": "10221", "answers": {"10221": {"line": 6813, "body": "A bounding box is a rectangle superimposed over an image within which all important features of a particular object is expected to reside. It's purpose is to reduce the range of search for those object features and thereby conserve computing resources: Allocation of memory, processors, cores, processing time, some other resource, or a combination of them. For instance, when a convolution kernel is used, the bounding box can significantly limit the range of the travel for the kernel in relation to the input frame.\nWhen an object is in the forefront of a scene and a surface of that object is faced front with respect to the camera, edge detection leads directly to that surface's outlines, which lead to object extent in the optical focal plane. When edges of object surfaces are partly obscured, the potential visual recognition value of modelling the object, depth of field, stereoscopy, or extrapolation of spin and trajectory increases to make up for the obscurity.\n\nA classifier can only classify one object per image\n\nA collection of objects is an object, and the objects in the collection or statistics about them can be characterized mathematically as attributes of the collection object. A classifier dealing with such a case can produce a multidimensional classification of that collection object, the dimensions of which can correspond to the objects in the collection. Because of that case, the statement is false.\n\n1) If I don't need to know 'where' an object is (or objects are) and just interested in the existence of them and how many there are, is it possible to just get rid of the boundary boxes?\n\nIf you have sufficient resources or patience to process portions of the frame that don't contain the objects, yes.\nQuestions (2) and (3) are already addressed above, but let's look at them in that context.\n\n2.a) If not, how does bounding boxes help detect objects?\n\nIt helps by fulfilling its purpose, to reduce the range of the search. If by thrifty method a bounding shape of any type can be created, then the narrowing of focus can be used to reduce the computing burden on the less thrifty method by eliminating pixels that are not necessary to peruse with more resource-consuming-per-pixel methods. These less thrifty methods may be necessary to recognize surfaces, motion, and obscured edges and reflections so that the detection of object trajectory can be obtained with reliability.\nThat these thrifty mechanisms to find the region of focus and these less thrifty mechanisms to use that information and then determine activity at higher levels of abstraction are artificial networks of this type or that or use algorithms of this type or that is not relevant yet. First understand the need to reduce computing cost in AI, which is a pervasive concept for anything more complex than tic-tac-toe, and then consider how bounding boxes help the AI engineer and the stakeholders of the engineering project to procure technology that is viable in the market.\n\n2.b) From what I have figured is that a network (if using neural network architectures) predicts the coordinates of the bounding boxes if there is something in the feature map. Doesn't this mean that the detector already knows where the object is (at least briefly)?\n2.c) So continuing from question 1, if I'm not interested in the exact location, would training for bounding boxes be irrelevant?\n\nCognition is something AI seeks to simulate, and many hope to have robots like in the movies that can help out and be invaluable friends, like TARS in the Nolan brothers 2014 film Interstellar. We're not there. The network knows nothing. It can train a complex connection between an input signal through a series of attenuation matrices and activation functions to produce an output signal statistically consistent with its loss function, value function, or some other criteria.\nThe inner layers of an artificial net may, if not directed to do so, produce something equivalent to a bounding region only if velocity of convergence is present as a factor in its loss or value function. Otherwise there is nothing in the Jacobian leading convergence to reduce its own time to completion. Therefore, the process may complete, but not as well as if cognition steps in and decides that the bounding region will be found first and then used to reduce the total burden of mechanical (arithmetic) operations to find the desired output signal as a function of input signal.\n\n3) Finally, in architectures like YOLO, it seems that they predict the probability of each class on each grid (e.g. 7 x 7 for YOLO v1). What would be the purpose of bounding boxes in this architecture other than that it shows exactly where the object is? Obviously, the class has already been predicted so I'm guessing that it doesn't help classify better.\n\nReading the section in A Real-Time Chinese Traffic Sign Detection Algorithm Based on Modified YOLOv2, J Zhang, M Huang, X Jin, X Li, 2017, may help further comprehension of these principles and their almost universal role in AI, especially the text around their statement, \"The Network Architecture of YOLO v2 YOLO employs a single neural network to predict bounding boxes and class probabilities directly from full images in one inference. It divides the input image into S x S grids.\" This way you can see the use of these principles in the achievement of specific research goals.\nOther such applications can be found by simply reading the article full text available on the right side of an academic search for yolo algorithm and using ctrl-f to find the word bound.\n", "type": 2, "id": "10221", "date": "2019-01-26T18:23:25.590", "score": 2, "comment_count": 2, "parent_id": "10177"}, "10186": {"line": 6784, "body": "In principle, you could train the model to output a sigmoid map of coarse object positions (0 -> no object, 1 -> an object center is located here). The map could be subjected to non-maximum suppression and such model could be trained end-to-end. That would be possible, if that's what you are asking.\n", "type": 2, "id": "10186", "date": "2019-01-25T12:52:43.070", "score": -1, "comment_count": 0, "parent_id": "10177"}}}
{"line": 5562, "body": "I'd like to implement a partially connected neural network with ~3 to 4 hidden layers (a sparse deep neural network?) where I can specify which node connects to which node from the previous/next layer. So I want the architecture to be highly specified/customized from the get-go and I want the neural network to optimize the weights of the specified connections, while keeping everything else 0 during the forward pass AND the backpropagation (connection does not ever exist).\nI am a complete beginner in neural networks. I have been recently working with tensorflow & keras to construct fully connected deep networks. Is there anything in tensorflow (or something else) that I should look into that might allow me to do this? I think with tf, I should be able to specify the computational graph such that only certain connections exist but I really have no idea yet where to start from to do this...\nI came across papers/posts on network pruning, but it doesn't seem really relevant to me. I don't want to go back and prune my network to make it less over-parameterized or eliminate insignificant connections. \nI want the connections to be specified and the network to be relatively sparse from the initialisation and stay that way during the back-propagation.\n", "type": 1, "id": "8270", "date": "2018-10-05T16:47:38.873", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "deep-neural-networks", "feedforward-neural-networks"], "title": "How to create Partially Connected NNs with prespecified connections using Tensorflow?", "answer_count": 3, "views": 1150, "accepted_answer": null, "answers": {"8663": {"line": 5841, "body": "So I've done some more research after posting this and it looks like treating things as separate neural nets and connecting them later in subsequent hidden layers is a good idea. If anyone has other ideas that might be helpful, I'd love to hear, though!\n", "type": 2, "id": "8663", "date": "2018-10-29T06:14:08.730", "score": 0, "comment_count": 0, "parent_id": "8270"}, "11707": {"line": 7893, "body": "In case that you want to connect a placeholder to a new layer you can do as below\nx = tf.placeholder(shape=[None, 784], dtype=tf.float32) # mnist for example\n\nx = tf.reshape(x, (-1, 784, 1)) # change to new shape (784 ,1)\n\nx = tf.unstack(self._input,axis=1) # getting a list with 784 elements of 1\n\ncon = tf.concat(x[1,2,3,4,5]) # for instance you want only from 1 to 5 inputs/neurons\n\nyou can feed your new layer with above con which is only corresponds from the input 1 to 5.\nyou can apply same technique to a layer instead of tf.placeholder\n", "type": 2, "id": "11707", "date": "2019-04-07T17:48:56.173", "score": 0, "comment_count": 0, "parent_id": "8270"}, "12583": {"line": 8589, "body": "I am working on something similar (if not the same thing) in JS right now.\nNetwork structure: The network is a 2d array. Each element in the 2d dimension is an object holding 2 arrays, starts and ends. The starts array holds the outgoing connections. The ends array holds the incoming connections. This way, an end on one node is a reference to a start on another node in the next layer. Each element in starts and ends has 2 variables, weight and value. I've tried setting the value on the node itself (array[x][x].value instead of array[x][y].starts[z].value) but it didn't work out. The value must be carried to the next activation somehow - so I put it directly on the \"synapse\". (Note that nodes, or neurons, are just conceptual here. I'm just referring to the cell in the array (array[x][y] <- node)).\nConnections:\nSo, I have an \"addLink\" function that sets the connections. It takes sr, sc, er, ec as arguments (start/end row, start/end column). It creates the \"Link\" object and places it into two arrays (array[sr][sc].starts and array[er][ec].ends). You would just need an array, or external files... some list of start and end coordinates for each link that you want to form.\nMy connection solution:\nI have implemented \"max outgoing connections\" for each node, as well as defining \"possible connection ranges\" for each node. Basically, I define a box size (yMin, yMax, xMin, xMax), it adds the node's coordinates (and inverts for a second box on the other side) to create an array of possible connections and \"randomly\" chooses a number of nodes from those connections up to max connections. I call it getReachableCells. This allows me to form connections across multiple layers. I also define the probabilities of which connections are chosen (further connections are less likely; one cell to the right is the most likely.) In the future, I plan to (rarely) make one of those reachable cells an input node on another network. I've made a 2d-array utility class that allows me to scale, consolidate, and prune the network.\nI plan on creating signalling events that triggers growth and death of nodes and connections. For example, if an input-values array's (x_data) length is greater than the number of input neurons, this will trigger a growth signal. If it happens several times (if the demand is high), the array will be scaled and a new node created. Synapses will have connection strengths that are strengthened with use and weakened with lack of use - eventually being removed if the connection strength hits 0. (I started as a game programmer.)\nAlso, I made a render class that shows all of the connections as lines on a canvas as well as functions that label inputs and outputs.\nThe problem that I'm currently facing: Backpropagation. Since I have multiple outputs, I need to adjust the weights associated with each output. I call this an output's \"chain\". I want to be able to grab the chain during the forward pass so that I don't have to loop over the entire network again for every output to find it, but it's looking like that's the only way. If anyone has any insight on this, please let me know.\n\n", "type": 2, "id": "12583", "date": "2019-05-29T13:00:34.010", "score": 0, "comment_count": 0, "parent_id": "8270"}}}
{"line": 5225, "body": "I want to explore and experiment the ways in which I could use a neural network to identify patterns in text. \nexamples:\n\nPrices of XYZ stock went down at 11:00 am today\nRetrieve a list of items exchanged on 03/04/2018\nShow error logs between 3 - 5 am yesterday.\nReserve a flight for 3rd October.\nDo I have any meetings this Friday?\nRemind to me wake up early tue, 4th sept\n\nThis is for a project so I am not using regular expressions. Papers, projects, ideas are all welcome but I want to approach feature extraction/pattern detection to have a model trained which can Identify patterns that it has already seen.\n", "type": 1, "id": "7794", "date": "2018-09-02T11:25:26.373", "score": 6, "comment_count": 3, "tags": ["neural-networks", "pattern-recognition"], "title": "How can I detect datetime patterns in text?", "answer_count": 3, "views": 2328, "accepted_answer": "7910", "answers": {"7910": {"line": 5307, "body": "Approaches\nThere are two main approaches to detecting any human readable representation of a discrete quantity within text.\n\nDetect well known and stable patterns in the input stream and by adjacency determine the output stream.\nWindowing through the text in the input stream and directly detect the quantities.\n\nThere are other approaches and there are hybrids of these two or one of these and the other approaches, but these two are the theoretically most straightforward and likely to produce both reliability and accuracy.\nRe-entrant Learning\nWhether the training involves re-entrant learning techniques, such as reinforcement, is a tangential issue that this answer will not address, but know that whether all training is solely a deployment component or whether adaptation and/or convergence occurs in real time is an architectural decision to be made.\nPractical Concerns\nPractically, the outputs of each recognition are as follows.\n\nStarting index\nEnding index\nInteger year or null\nInteger day of year or null\nInteger hour in military time or null\nMinute or null\nSecond or null\nTime zone or null\nProbability the recognition unit was correctly identified\nProbability the recognition produced accurate results\n\nAlso practically, the input must either be from within one particular locale's norms in terms of\n\nCalendar,\nTime,\nWritten language,\nCharacter encoding, and\nCollation,\n\n... or ...\n\nThe learning must occur using training sets that include the locales that will be encountered during system use\n\n... or ...\n\nMuch of the locale specific syntax must be normalized to a general date and time language such as this:\njnvrii --> D_01\nEnero --> D_01\nJanuar --> D_01\n\nso that Filipino and Icelandic names for the first month of the year enter the artificial network as the same binary pattern.\n**Date and Time Specifically*\nIn the case of 1. above, which is semi-heuristic in nature, and assuming that the locale is entirely en-US.utf-8, the CASE INSENSITIVE patterns for a PCRE library or equivalent to use as a search orientation heuristic include the following.\n(^|[^0-9a-z])(19|20|21)[0-9][0-9])([^0-9a-z]|$)\n(^|[^0-9a-z])(Mon|Monday|Tue|Tues|Tuesday|Wed|Wednesday|Thu|Thur|Thurs|Thursday|Fri|Friday|Sat|Saturday|Sun)([^0-9a-z]|$)\n(^|[^0-9a-z])(Jan|January|Feb|February|Mar|March|Apr|April|May|Jun|June|Jul|July|Aug|August|Sep|Sept|September|Oct|October|Nov|November|Dec|December)([^0-9a-z]|$)\n(^|[^0-9a-z])(Today|Yesterday|Tomorrow)([^0-9a-z]|$)\n(^|[^0-9])[AP]M|[AP][.]M[.]|Noon|Midnight)([^0-9a-z]|$)\n(^|[^a-z])(0?[1-9])(:[0-5][0-9]){1,2}([^a-z]|$)\n\nThere should be others for time, hyphenated or slash delimited dates, or time zone.\nThe positions and normalized encoding of these date and time artifacts are then substituted into the artificial network inputs instead of the original text in the stream, reducing redundancy and improving both the speed of training and the resulting accuracy and reliability of recognition.\nIn the case of 2. above, the entire burden of recognition is left to the artificial network.  The advantage is less reliance on date and time conventions.  The disadvantage is a much larger burden placed on training data variety and training epochs, meaning a much higher burden on computing resources and the pacience of the stake holder for the project.\nWindowing\nAn overlapping windowing strategy is necessary.  Unlike FFT spectral analysis in real time, the windowing must be rectangular, because the size of the window is the width of the input layer of the artificial network.  Experimenting with the normalization of input such that the encoding of text and data and time components entering the input layer could greatly vary the results in terms of training speed, recognition accuracy, reliability, and adaptability to varying statistical distributions of date and time instances and relationships.\n", "type": 2, "id": "7910", "date": "2018-09-10T20:27:36.370", "score": 2, "comment_count": 1, "parent_id": "7794"}, "7833": {"line": 5249, "body": "If want to use deep learning approaches, you should look to recurrent neural networks (RNN). Recurrent networks will take into account temporal dependencies and could detect thatn this in this Friday belong to datetime but not in this apple. \nAs a simple model, you could create a model with a bidirectional LSTM layer (a type of RNN):\n\nInput: the sequences of characters.\nOutput: whether the character belongs to datetime or not.\n\nThe longest part will gather many sentences with its corresponding solution to create a training/testing dataset. Keras might be a good framework to start playing around and with many examples. \n", "type": 2, "id": "7833", "date": "2018-09-05T15:59:43.343", "score": 1, "comment_count": 2, "parent_id": "7794"}, "7841": {"line": 5253, "body": "If you dont want to use machine learning you may use date time parser in python. Few examples are given below.  It will return you formatted date time from given string. It works with all languages.\n>>> import dateparser\n>>> dateparser.parse('12/12/12')\ndatetime.datetime(2012, 12, 12, 0, 0)\n>>> dateparser.parse(u'Fri, 12 Dec 2014 10:55:50')\ndatetime.datetime(2014, 12, 12, 10, 55, 50)\n>>> dateparser.parse(u'Martes 21 de Octubre de 2014')  # Spanish (Tuesday 21 October 2014)\ndatetime.datetime(2014, 10, 21, 0, 0)\n>>> dateparser.parse(u'Le 11 Decembre 2014 a 09:00')  # French (11 December 2014 at 09:00)\ndatetime.datetime(2014, 12, 11, 9, 0)\n>>> dateparser.parse(u'13 ianvaria 2015 g. v 13:34')  # Russian (13 January 2015 at 13:34)\ndatetime.datetime(2015, 1, 13, 13, 34)\n>>> dateparser.parse(u'1 eduue`ntulaakhm 2005, 1:00 AM')  # Thai (1 October 2005, 1:00 AM)\ndatetime.datetime(2005, 10, 1, 1, 0)\n\n", "type": 2, "id": "7841", "date": "2018-09-06T04:46:51.847", "score": 0, "comment_count": 1, "parent_id": "7794"}}}
{"line": 6648, "body": "I was trying to understand the loss function of GANs, while I found a little mis-match between different papers. \nThis is the screen-shot from the original paper of Goodfellow at https://arxiv.org/pdf/1406.2661.pdf: ,\nAnd equation (1) in this version of pix2pix paper at https://arxiv.org/pdf/1611.07004.pdf\n\nPutting aside the fact that pix2pix is using conditional GAN, which introduces a second term $y$, the 2 formulas are quite resemble, except that in the pix2pix paper, they try to get minimax of ${\\cal{L}}_{cGAN}(G, D)$, which is defined to be $E_{x,y}[...] + E_{x,z}[...]$, whereas in the original paper, they define $\\min\\max V(G, D) = E[...] + E[...]$. \nI am not coming from a good math background, so I am quite confused. I'm not sure where the mistake is, but assuming that $E$ is expectation (correct me if I'm wrong), the version in pix2pix makes more sense to me, although I think it's quite less likely that Goodfellow could make this mistake in his amazing paper. Maybe there's no mistake at all and it's me who do not understand them correctly.\n", "type": 1, "id": "9990", "date": "2019-01-14T18:11:35.290", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "generative-model"], "title": "Confusing on GAN loss function", "answer_count": 3, "views": 1967, "accepted_answer": null, "answers": {"10117": {"line": 6737, "body": "I'm not sure I understand your question. However responding to your question in the comments. The difference between the two objectives is that:\nIn an ordinary GAN, we want to push $p(G)$ to be as close as possible to $p(data)$\nIn a conditional GAN, we have a context $c$. If we imagine for ease of understanding that $c=[1,2,3]$ is a discrete variable where all the data can be categorised under one of these c values , then we want to:\n push $p(G|c=1)$ as close as possible to $p(data|c=1)$\npush $p(G|c=2)$ as close as possible to $p(data|c=2)$\npush $p(G|c=3)$ as close as possible to $p(data|c=3)$\n", "type": 2, "id": "10117", "date": "2019-01-21T17:36:39.007", "score": 0, "comment_count": 0, "parent_id": "9990"}, "9993": {"line": 6649, "body": "What is meant by both papers is that we have two agents (generator and discriminator) playing a game with the value function V defined as a sum of the expectations (i.e. an expectation of the outcome value defined as a sum of two terms, or actually a logarithm of a product...). The generator uses a strategy G encoded in the parameters of its neural network (thg), the discriminator uses a strategy D encoded in the parameters of its neural network (thd). Our goal is to (hopefully) find such a pair of strategies (a pair of parameter sets thgmin and thdmax) that produce the minimax value.\nWhile trying to find the (thgmin, thdmax) pair using gradient descent, we actually have two loss functions: one is the loss function for G, parameterized by thg, another is the loss function for D, parameterized by thd, and we train them alternatively on minibatches together.\nIf you look at the Algorithm 1 in the original paper, the loss function for the discriminator is -log(D(x; thd)) - log(1 - D(G(z); thd), and the loss function for the generator is log(1 - D(G(z; thg)) (in both cases, in the original paper, x is sampled from the reference data distribution and z is sampled from noise):\nThe ideal value for the loss function of the discriminator is 0, otherwise it's greater than 0. The \"loss\" function of the generator is actually negative, but, for better gradient descent behavior, can be replaced with -log(D(G(z; thg)), which also has the ideal value for the generator at 0. It is impossible to reach zero loss for both generator and discriminator in the same GAN at the same time. However, the idea of the GAN is not to reach zero loss for any of the game agents (this is actually counterproductive), but to use that \"double gradient descent\" to \"converge\" the distribution of G(z) to the distribution of x.\n", "type": 2, "id": "9993", "date": "2019-01-15T00:46:39.527", "score": 1, "comment_count": 4, "parent_id": "9990"}, "10192": {"line": 6788, "body": "The question is about a mismatch between the loss function in two papers on GANs. The first paper is Generative Adversarial Nets Ian J. Goodfellow et. al., 2014, and the excerpt image in the question is this.\n\nThe adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generator's distribution $p_g$ over data $x$, we define a prior on input noise variables $p_z (z)$, then represent a mapping to data space as $G (z; \\theta_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\\theta_g$. We also define a second multilayer perceptron $D (x; \\theta_d)$ that outputs a single scalar. $D (x)$ represents the probability that $x$ came from the data rather than pg. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\\log (1 - D(G(z)))$:\nIn other words, $D$ and $G$ play the following two-player minimax game with value function $V (G, D)$:\n$$ \\min_G \\, \\max_D V (D, G) = \\mathbb{E}_{x~p_{data}(x)} \\, [\\log \\, D(x)] \\\\\n\\quad\\quad\\quad\\quad\\quad\\quad\\quad  + \\, \\mathbb{E}_{z~p_z(z)} \\, [\\log \\, (1 - D(G(z)))] \\, \\text{.} \\quad \\text{(1)} $$\n\nThe second paper is Image-to-Image Translation with Conditional Adversarial Networks, Phillip Isola Jun-Yan Zhu Tinghui Zhou Alexei A. Efros, 2018, and the excerpt image in the question is this.\n\nThe objective of a conditional GAN can be expressed as\n$$ \\mathcal{L}_{cGAN} (G, D) = \\mathbb{E}_{x, y} \\, [\\log D(x, y)] \\\\ \\quad\\quad\\quad\\quad\\quad\\quad\\quad + \\mathbb{E}_{x, z} \\, [\\log \\, (1 - D(x, G(x, z))], \\quad \\text{(1)} $$\nwhere $G$ tries to minimize this objective against an adversarial $D$ that tries to maximize it, i.e.\n$$ G^{*} = \\arg \\, \\min_G \\, \\max_D \\mathcal{L}_{cGAN} (G, D) \\, \\text{.} $$\nTo test the importance of conditioning the discriminator, we also compare to an unconditional variant in which the discriminator does not observe $x$:\n$$ \\mathcal{L}_{GAN} (G, D) = \\mathbb{E}_y \\, [\\log \\, D(y)] \\\\\n\\quad\\quad\\quad\\quad\\quad\\quad\\quad + \\mathbb{E}_{x, z} \\, [\\log \\, (1 - D(G(x, z))] \\, \\text{.} \\quad \\text{(2)} $$\n\nIn the above $G$ refers to the generative network, $D$ refers to the discriminative network, and $G^{*}$ refers to the minimum with respect to $G$ of the maximum with respect to $D$. As the question author tentatively put forward, $\\mathbb{E}$ is the expectation with respect to its subscripts.\nThe question of discrepancy is that the right hand sides do not match between the first paper's equation (1) and the second paper's equation (2) which is absent of the condition involving $y$.\nFirst paper:\n$$ \\mathbb{E}_{x~p_{data}(x)} \\, [\\log \\, D(x)] \\\\\n\\quad\\quad\\quad\\quad\\quad\\quad\\quad  + \\, \\mathbb{E}_{z~p_z(z)} \\, [\\log \\, (1 - D(G(z)))] \\, \\text{.} \\quad \\text{(1)} $$\nSecond paper:\n$$ \\mathbb{E}_y \\, [\\log \\, D(y)] \\\\\n\\quad\\quad\\quad\\quad\\quad\\quad\\quad + \\mathbb{E}_{x, z} \\, [\\log \\, (1 - D(G(x, z))] \\, \\text{.} \\quad \\text{(2)} $$\nThe second later paper further states this.\n\nGANs are generative models that learn a mapping from random noise vector $z$ to output image $y, G : z \\rightarrow y$. In contrast, conditional GANs learn a mapping from observed image $x$ and random noise vector $z$, to $y, G : {x, z} \\rightarrow y$.\n\nNotice that there is no $y$ in the first paper and the removal of the condition in the second paper corresponds to the removal of $x$ as the first parameter of $D$. This is one of the causes of confusion when comparing the right hand sides. The others are use of variables and degree of explicitness in notation.\nThe tilda $~$ means drawn according to. The right hand side in the first paper indicates that the expectation involving $x$ is based on a drawing according to the probability distribution of the data with respect to $x$ and the expectation involving $z$ is based on a drawing according to the probability distribution of $z$ with respect to $z$.\nThe removal of the observation of $x$ from the second right hand term of the second paper's equation (2), which is the first parameter of $G$, the replacement of that equation's $y$ variable with the now freed up $x$ variable, and the acceptance of the abbreviation of the tilda notation used in the first paper then brings both papers into exact agreement.\n", "type": 2, "id": "10192", "date": "2019-01-25T14:40:13.367", "score": 1, "comment_count": 1, "parent_id": "9990"}}}
{"line": 6948, "body": "I have a dataset which I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 1972 columns (the features). The last column i.e. column 1972 has string type labels (14 different categories of target labels). I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!\n", "type": 1, "id": "10447", "date": "2019-02-07T22:33:27.293", "score": 7, "comment_count": 0, "tags": ["convolutional-neural-networks", "tensorflow"], "title": "How to use CNN for making predictions on non-image data?", "answer_count": 3, "views": 6752, "accepted_answer": "10449", "answers": {"10458": {"line": 6956, "body": "The convolutional models are a method of choice when your problem is translation invariant (or covariant). In image classification, the image should be classified into class 'cow' if a cow is present in any part of the image. In text classification, different orders of phrases and sentences result in related meaning. In speech recognition, the same syllable is used at different places to build different words.\nIn your problem, you should check whether some subsequences of your 1791 columns give rise to the same meaning although they are located at different places within the sample. If the answer is positive, then convolutional layers are likely going to improve the performance.\n", "type": 2, "id": "10458", "date": "2019-02-08T16:05:53.400", "score": 4, "comment_count": 0, "parent_id": "10447"}, "10449": {"line": 6949, "body": "You can use CNN on any data, but it's recommended to use CNN only on data that have spatial features (It might still work on data that doesn't have spatial features, see DuttaA's comment below).\nFor example, in the image, the connection between pixels in some area gives you another feature (e.g. edge) instead of a feature from one pixel (e.g. color). So, as long as you can shaping your data, and your data have spatial features, you can use CNN.\nFor Text classification, there are connections between characters (that form words) so you can use CNN for text classification in character level.\nFor Speech recognition, there is also a connection between frequencies from one frame with some previous and next frames, so you can also use CNN for speech recognition.\nIf your data have spatial features, just reshape it to a 1D array (for example in text) or 2D array (for example in Audio). Tensorflow's function conv1d and conv2d are general function that can be used on any data. It look the data as an array of floating-point, not as image/audio/text.\nBut if your data doesn't have spatial features, for example, your features are price, salary, status_marriage, etc. I think you don't need CNN, and using CNN won't help.\n", "type": 2, "id": "10449", "date": "2019-02-08T00:52:25.103", "score": 6, "comment_count": 5, "parent_id": "10447"}, "30220": {"line": 20292, "body": "DeepInsight method has been used for converting tabular data into corresponding images which are then processed by CNN. Here is the link https://alok-ai-lab.github.io/DeepInsight/\n", "type": 2, "id": "30220", "date": "2021-08-17T08:34:56.977", "score": 0, "comment_count": 0, "parent_id": "10447"}}}
{"line": 7429, "body": "I'm studying how SPP (Spatial, Pyramid, Pooling) works. SPP was invented to tackle the fix input image size in CNN. According to the original paper https://arxiv.org/pdf/1406.4729.pdf, the authors say: \n\nconvolutional layers do not require a fixed image size and can\n  generate feature maps of any sizes. On the other hand, the\n  fully-connected layers need to have fixed size/length input by their\n  definition. Hence, the fixed size constraint comes only from the\n  fully-connected layers, which exist at a deeper stage of the network.\n\nWhy does a fully connected layer only accepts a fixed input size (but convolutional layers don't)? What's the real reason behind this definition?\n", "type": 1, "id": "11074", "date": "2019-03-07T07:50:52.097", "score": 2, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "computer-vision"], "title": "Why does a fully connected layer only accept a fixed input size?", "answer_count": 2, "views": 1412, "accepted_answer": "11075", "answers": {"11076": {"line": 7431, "body": "It doesn't have to be so. Fully connected layer could be considered as convolutional layer with input image of 1 pixel and spatial kernel size of 1 pixel. So 1-pixel kernel convolutional layer is effectively the same as fully connected layer attached to each pixel. That is idea behind \"Fully Convolutional Networks\". If you want \"true\", 1-pixel fully connected layer after convolutional layer (with variable input size) all you have to do is to put average pooling layer (or other type of pooling layer)  before fully connected layer. That way fully connected layer can accept variable input size.\n", "type": 2, "id": "11076", "date": "2019-03-07T10:28:09.877", "score": 0, "comment_count": 0, "parent_id": "11074"}, "11075": {"line": 7430, "body": "A convolutional layer is a layer where you slide a kernel or filter (which you can think of as a small square matrix of weights, which need to be learned during the learning phase) over the input. In practice, when you need to slide this kernel, you will often need to specify the \"padding\" (around the input) and \"stride\" (with which you convolve the kernel on the input), in order to obtain the desired output (size). So, even if you receive inputs of different sizes, you can change these values, like the padding or the stride, in order to produce a valid output (size). In this sense, I think, we can say that convolutional layers accept inputs of (almost) any size.\nThe number of feature maps does not depend on the kernel or input (sizes). The number of features maps is determined by the number of different kernels that you will use to slide over the input. If you have $K$ different kernels, then you will have $K$ different feature maps. The number of kernels is often a hyper-parameter, so you can change it (as you please).\nA fully connected (FC) layer requires a fixed input size by design. The programmer decides the number of input units (or neurons) that the FC layer will have. This hyper-parameter often does not change during the learning phase. So, yes, FC often accept inputs of fixed size (also because they do not adopt techniques like \"padding\").\n", "type": 2, "id": "11075", "date": "2019-03-07T09:19:46.780", "score": 1, "comment_count": 0, "parent_id": "11074"}}}
{"line": 7232, "body": "I've been looking at various bounding box algorithms, like the three versions of RCNN, SSD and YOLO, and I have noticed that not even the original papers include pseudocode for their algorithms. I have built a CNN classifier and I am attempting to incorporate bounding box regression, though I am having difficulties in implementation. I was wondering if anyone can whip up some pseudocode for any bounding box classifier or a link to one (unsuccessful in my search) to aid my endeavor.\nNote: I do know that there are many pre-built and pre-trained versions of these object classifiers that I can download from various sources, I am interested in building it myself.\n", "type": 1, "id": "10806", "date": "2019-02-22T01:30:15.920", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "object-recognition", "pseudocode"], "title": "Pseudocode for CNN with Bounding Box and Classifier", "answer_count": 2, "views": 1785, "accepted_answer": null, "answers": {"10825": {"line": 7245, "body": "In General\nEach of those projects has open source code on github that you can look at.  If you do some quick googling you'll find the software for these basic regressors exist for different deep learning frameworks.  Usually the these types of projects only include pseduo code for the custom or complicated layers that are involved in the detector.  There isn't much of a need to include pseudo code for a simple convolution because it's a well established operation.  I just included a few links here, but if you look around there's a bunch of implementations in different frameworks that you'll find.\nSSD\nhttps://github.com/amdegroot/ssd.pytorch\nhttps://github.com/weiliu89/caffe/tree/ssd\nFasterRCNN (Better version of RCNN)\nhttps://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN\nYOLO V3\nV3 -- https://itnext.io/implementing-yolo-v3-in-tensorflow-tf-slim-c3c55ff59dbe\nV1 -- https://github.com/hizhangp/yolo_tensorflow\nAside\nAs an aside, unless you're trying to do novel research or fit a specific function usually these detectors work pretty well out of the box.  Running a base level detector and them modifying for your application is usually a safer route unless you have a deep understanding of how these networks work.\n", "type": 2, "id": "10825", "date": "2019-02-22T18:05:21.147", "score": 0, "comment_count": 0, "parent_id": "10806"}, "13213": {"line": 9080, "body": "The minimal algorithm for convolution in $\\mathbb{R}^2$ is a four dimensional iteration.\nfor all vertical kernel positions\n  for all horizontal kernel positions\n    initialize the value at the output position to the bias\n    for all vertical positions in the kernel\n      for all horizontal positions in the kernel\n        add the product of the input value to that of the output position\n\nIn $\\mathbb{R}^n$ it is a $2n$ dimensional iteration following this pattern.\nThe minimal algorithm for regression of bounding boxes orthogonal with respect to the image grid (no tilting) is this.\nuntil number of boxes reaches max\n  make first guess of two coordinates\n  until number of guesses reaches max or matching criteria is met\n    evaluate guess\n    remember guess and guess results\n    improve on guess based on evaluation results and\n          possibly injected randomness,\n          excluding locations already covered\n    if some intermediate criteria is met\n      change the nature of the guessing, evaluation, and improving\n            as is appropriate for the criteria match\n            (this covers approaches that have multiple phases)\n  if no guess matched criteria\n    break\n\nThat's approaching concepts from the top down. When approaching from the other direction, reverse engineer the best code. In the case of RCNN, it is unadvisable to find implementations following the first paper expressing the approach. Reading the first paper may be helpful to get the gist of the approach, but reverse engineer the best one, which, in this case, may be Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, 2016. Study the implementation they pushed to git at https://github.com/rbgirshick/py-faster-rcnn/tree/master/. The algorithm is in lib/fast_rcnn.\nThe reason this algorithm isn't spelled out in their paper or any paper from the first on down through the lineage to their paper is simple.\n\nThe pseudo-code above is universal across all convolutions and all bounding box regressions, so that doesn't need to be restated with each approach.\nThe main features of an approach like RCNN, SSD, or YOLO are not algorithmic. They are algebraic expressions of the guess, the evaluation, the improvement upon the guess, and the test for the criteria.\nThe use of objects and functional programming makes the implementation more readable, so it can be easier to read the implementation than read a huge chunk of the above pseudo-code with all the algebra and test branches plugged in.\nFor the above reasons, it is rare that pseudo-code would be used prior to the implementation when the paper is written.\nThe return on investment of reverse engineering from code to pseudo-code is only sufficient motivation if one is going to improve the algorithm and write another paper, and on the way to finishing the prior paper's pseudo-code, the new paper and the new code gets finished first.\n\nSince the author of this question seems interested in writing their own code, it may be reasonable to assume the same author may be interested in thinking their own thoughts, so I'll add this.\nNone of these algorithms are object recognition. Recognition has to do with cognition, and these approaches do not even touch upon cognitive processing, another branch of AI not related to convolution and probably not closely related to formal regression either. Additionally, bounding boxes are not the way animal vision systems work. Early gestalt experiments in vision indicate a complete independence of human vision from rectilinear formalities. In lay terms, humans and other organisms with vision systems don't have any conception of Cartesian coordinates. We can still read books if tilted slightly relative to the plane passing through our eyes. We don't zoom or tilt in Cartesian coordinates.\nThese facts may not be necessary to comprehend to create an automated vehicle driving system that produces a better safety record than average human drivers, but that is only because humans don't set that bar very high and because cars roll in the plane of the road. These facts are indeed necessary in aeronautic system used in military applications, where nothing is particularly Cartesian and the meaning of horizontal and vertical is ambiguous. For that reason, it is unlikely that bounding boxes will be the edge of vision technology for very long.\nIf one wishes to transcend current mediocrity, consider bounding circles with fuzzy boundaries, which would be more like the systems that evolved over millions of biological iterations. If the computer hardware is poorly fit to radial processing, design new hardware in which radial processing is native and in which Cartesian coordinates may be foreign and cumbersome.\nRegarding the classifier, the classifier papers do generally include the algorithm, so those can be found by doing an academic search for the original paper describing the classifier being used.\n", "type": 2, "id": "13213", "date": "2019-07-05T10:49:29.300", "score": 2, "comment_count": 0, "parent_id": "10806"}}}
{"line": 5906, "body": "With a team, we are studying how it is possible to predict the price movement with high-frequency. Instead of predicting the price directly, we have decided to try predicting price difference as well as the features. In other words, at time t+1, we predict the price difference and the features for time t+2. We use the predicted features from time t+1 to predict the price at time t+2. \nWe got very excited, because we thought getting good results with the following graph\n\nWe got problems in production and we wasn't known the problem till we plot the price difference.\n\nHere is the content of the config file\n{\n    \"data\": {\n        \"sequence_length\":30,\n        \"train_test_split\": 0.85,\n        \"normalise\": false,\n        \"num_steps\": 5\n    },\n    \"training\": {\n        \"epochs\":200,\n        \"batch_size\": 64\n    },\n    \"model\": {\n        \"loss\": \"mse\",\n        \"optimizer\": \"adam\",\n        \"layers\": [\n            {\n                \"type\": \"lstm\",\n                \"neurons\": 51,\n                \"input_timesteps\": 30,\n                \"input_dim\": 101,\n                \"return_seq\": true,\n                \"activation\": \"relu\"\n            },\n            {\n                \"type\": \"dropout\",\n                \"rate\": 0.1\n            },\n            {\n                \"type\": \"lstm\",\n                \"neurons\": 51,\n                \"activation\": \"relu\",\n                \"return_seq\": false\n            },\n            {\n                \"type\": \"dropout\",\n                \"rate\": 0.1\n            },\n            {\n                \"type\": \"dense\",\n                \"neurons\": 101,\n                \"activation\": \"relu\"\n            },\n            {\n                \"type\": \"dense\",\n                \"neurons\": 101,\n                \"activation\": \"linear\"\n            }\n        ]\n    }\n}\n\nPrices don't change very fast. Therefore, the next price is almost always very close to the last price. In other words, P_{t+1} - P_{t} is very often close to zero or zero directly. If there is too many zeros then the network will only recognize the zeros. The model has picked up on that.\nI guess the model learned almost nothing except the very simple relationship that the next price is close to the last price. There is not necessarily anything wrong with the model. Predicting stock prices should be a very hard problem. \nSo a straightforward improvement should be of taking the features as a whole instead of their difference. \nI want to keep working with price difference instead of the price in itself because we are making the series potential more stationary.\nWhat might be a good solution to deal with the repetitive zeros related to our \"price difference\" problem? Does applying the log-return is a better idea than applying price differences?\nDoes a zero inflated estimators is a good idea? First predict whether it's gonna be a zero. If not predict the value. https://gist.github.com/fonnesbeck/874808 ?\n", "type": 1, "id": "8778", "date": "2018-11-05T00:10:41.387", "score": 3, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "long-short-term-memory"], "title": "Price difference predictions curve almost vanished", "answer_count": 1, "views": 118, "accepted_answer": null, "answers": {"11037": {"line": 7406, "body": "I have encountered similar problem while trying to predict forex prices. I understand it this way:\n\nThe data based on which you try to model price differences are so poor that the lowest error is achieved by zeroing the predicted values\n\nIn other words, due to poor \"correlation\" in data, zero values are the best solution.\nMy advice would be to look for more correlated data to be used.\n", "type": 2, "id": "11037", "date": "2019-03-05T18:44:27.790", "score": 0, "comment_count": 0, "parent_id": "8778"}}}
{"line": 6661, "body": "Recurrent Neural Networks (RNN) With Attention Mechanism is generally used for Machine Translation and Natural Language Processing. In Python, implementation of RNN With Attention Mechanism is abundant in Machine Translation (For Eg. https://talbaumel.github.io/blog/attention/, however what I would like to do is to use RNN With Attention Mechanism on a temporal data file (not any textual/sentence based data). I have a CSV file with of dimensions 21000 x 1936, which I have converted to a Dataframe using Pandas. The first column is of Datetime Format and last column consists of target classes like \"Class1\", \"Class2\", \"Class3\" etc. which I would like to identify. So in total, there are 21000 rows (instances of data in 10 minutes time-steps) and 1935 features. The last (1936th column) is the label column.\nIt is predominant from existing literature that an Attention Mechanism works quite well when coupled into the RNN. I am unable to locate any such implementation of RNN with Attention Mechanism, which can also provide a visualisation as well. Any help in this regard would be highly appreciated. Cheers! \n", "type": 1, "id": "10010", "date": "2019-01-15T19:58:41.363", "score": 2, "comment_count": 0, "tags": ["neural-networks", "python", "recurrent-neural-networks", "attention"], "title": "How to use RNN With Attention Mechanism on Non Textual Data?", "answer_count": 2, "views": 396, "accepted_answer": "10089", "answers": {"10089": {"line": 6718, "body": "Project Definition\n\nLabelled data set contains 21 K rows; 1,936 features; and 1 textual label\nLabel can be 1 of 14 possible categories\nThe first feature is a time stamp reflecting exact or approximate 10 minute sampling period\nData content not primarily natural language\nThe intention is to learn the function mapping the features to the label\nVisualization to observe training intermediate and final results\nHoping to simplify implementation using already implemented algorithms and development support\n\nUse of Recurrent Artificial Network Learning\nIt is correct that recurrent networks are designed for temporally related data. The later variants of the original RNN design are most apt to produce favorable results. One of the most effective of these variants is the GRU network cell, which is well represented in all the main machine learning libraries, and visualization hooks in those libraries are well documented.\nVarious Meanings of Attention Mechanisms\nThe belief that an attention mechanism beyond those built into the RNN design are needed to emphasize important features may be over-complicating the problem.\nThe parameters of the GRU and the other RNN variants already focus attention on particular features during learning. Even a basic feed forward network does that, but the MLP (multilayer perceptron) does not recognize feature trends temporally, so the use of RNN variants is smart.\nThere are other kinds of attention mechanisms that are not inside each cell of a network layer. Research into advanced attention based designs that involve oversight, various forms of feedback from the environment, recursion, or generative designs is ongoing. As the question indicates, those are targeted for natural language work. There is also attention based design for motion and facial recognition and automated walking, driving, and piloting systems. They are designed, tested, optimized, and evolving for the purpose of natural language processing or robotics, not 1,936 feature rows. It is unlikely that those systems can be morphed into something any more effective than a GRU network for this project without considerable further R&D.\nOutput Layer and Class Encoding\nThe 14 labels should be coded as 14 of the 16 permutations of a 4 bit output prior to training. And the loss function should dissuade the two illegal permutations.\n\nResponse to Comments\n\n[Of the] 1936 features, one of them [is] date-time timestamps and [the] rest [are] numeric. ... Can you please suggest the format of the input? Should I convert each column of feature to a list and create a list of lists or some other way around?\n\nRegardless of what types the library you use expect as inputs, the theory is clear. Features with a finite set of fixed discrete values are ordinals. The magnitude of their information content is given in bits $b$ as follows, where $p$ is the total number of possible discrete values for the feature.\n$$ b = \\log_2 p $$\nThis is also true of the timestamp, which has a specific possible range and time resolution, where $t_{\\emptyset}$ is the initial timestamp where the project or its data began and $t_{res}$ is the time of one resolution step.\n$$ b_{timestamp} = \\log_2 \\frac {t_{max} - t_\\emptyset} {t_{res}} $$\nThe label also has a range. If the range is a fixed set of permutations, then assign an integer to each, starting with zero, to encode them. If the range of the text is unknown, use a library or utility that converts words or phrases to numbers. One popular one is word2vec.\nIntegrating the features to reduce the number of input bits actually wastes a layer, so don't do that. The total information is given as this.\n$$ b_{total} = \\sum_{i = 1}^{1,936} b_i $$\nThe features, if they are real numbers, can remain so. The input layer of an artificial network expects a number entering the data flow for each cell. One can change the data type of the numbers to reduce computational complexity if no overflow or other mapping issue will occur. This is where the above information content can be useful in understanding how far one can go in collapsing the information into a smaller computational footprint.\n", "type": 2, "id": "10089", "date": "2019-01-20T15:32:16.860", "score": 0, "comment_count": 1, "parent_id": "10010"}, "10012": {"line": 6663, "body": "The article referenced doesn't highlight the most common terms, which is why you may not be finding what you want. We've found that not all the designs have public implementations in every language. Some we've had to translate from Python or Java to C++ to get the speed we needed for embedded applications. Attention mechanisms are not one thing either. There are attention mechanisms in the cells, before the layers, after the layers, as oversight mechanisms, and in push-pull balancing arrangements. The common cell using attention ideas use gates in the cell and the article only briefly mentions it. That's the long-short term memory cell, which is a type of RNN. Look for LSTM and B-LSTM for the first examples, which are gated. The GRU is also a type of RNN cell with gating. This paper is recent and proposes two mechanisms: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099. If you search for DiSAN there are a few implementations. The newest kind, which has promise is not really an RNN strictly. That's the recursive network, which can have attention mechanisms but can build learned graph structures instead of tensors. That may also be what you want.\n", "type": 2, "id": "10012", "date": "2019-01-16T01:18:08.383", "score": 0, "comment_count": 5, "parent_id": "10010"}}}
{"line": 6609, "body": "I'm using an object detection neural network and I employ data augmentation to increase a little my small dataset. More specifically I do rotation, translation, mirroring and rescaling.\nI notice that rotating an image (and thus it's bounding box) changes its shape. This implies an erroneous box for elongated boxes, for instance on the augmented image (right image below) the box is not tightly packed around the left player as it was on the original image. \nThe problem is that this kind of data augmentation seems (in theory) to hamper the network to gain precision on bounding boxes location as it loosens the frame.\nAre there some studies dealing with the effect of data augmentation on the precision of detection networks? Are there systems that prevent this kind of thing?\nThank you in advance!\n(Obviously, it seems advisable to use small rotation angles)\n\n", "type": 1, "id": "9935", "date": "2019-01-11T16:48:59.080", "score": 5, "comment_count": 0, "tags": ["convolutional-neural-networks", "object-recognition"], "title": "How data augmentation like rotation affects the quality of detection?", "answer_count": 1, "views": 359, "accepted_answer": "9961", "answers": {"9961": {"line": 6628, "body": "\nThe problem is that this kind of data augmentation seems (in theory) to hamper the network to gain precision on bounding boxes location as it loosens the frame.\n\nYes, it is clear from your examples that the bounding boxes become wider. Generally, including large amounts of data like this in your training data will mean that your network will also have a tendency to learn slightly larger bounding boxes. Of course, if the majority of your training data still has tight boxes, it should stell tend towards learning those... but likely slightly wider ones than if the training data did not include these kinds of rotations.\n\nAre there some studies dealing with the effect of data augmentation on the precision of detection networks? Are there systems that prevent this kind of thing?\n(Obviously, it seems advisable to use small rotation angles)\n\nI do not personally work directly in the area of computer vision really, so I'm not sufficiently familiar with the literature to point you to any references on this particular issue. Based on my own intuition, I can recommend:\n\nUsing relatively small rotation angles, as you also already suggested yourself. The bounding boxes will become a little bit wider than in the original dataset, but not by too much.\nUsing rotation angles that are a multiple of $90^\\circ$. Note that if you rotate a bounding box by a multiple of $90^\\circ$, the rotated bounding boxes become axis-aligned and your problem disappears again, they'll become just as tight as the bounding boxes in the unrotated image. Of course, you can also combine this suggestion with the previous one, and use rotation angles in, for example, $[85^\\circ, 95^\\circ]$.\nApply larger rotations primarily in images that only have bounding boxes that are approximately \"square\". From looking at your image, I get the impression that the problem of bounding boxes becoming wider after rotations is much more severe when you have extremely wide or thin bounding boxes (with one dimension much greater than the other). When the original bounding box is square, there still will be some widening after rotation, but not nearly as much, so the problem may be more acceptable in such cases.\n\n", "type": 2, "id": "9961", "date": "2019-01-13T11:56:12.210", "score": 0, "comment_count": 0, "parent_id": "9935"}}}
{"line": 5663, "body": "In keras, when we use an LSTM/RNN model, we need to specify the node [i.e., LSTM(128)]. I have a doubt regarding how it actually works. From the LSTM/RNN unfolding image or description, I found that each RNN cell take one time step at a time. What if my sequence is larger than 128? How to interpret this? Can anyone please explain me? Thank in advance.\n", "type": 1, "id": "8412", "date": "2018-10-15T04:27:41.923", "score": 3, "comment_count": 0, "tags": ["machine-learning", "recurrent-neural-networks", "long-short-term-memory"], "title": "What should I do when I have a variable-length sequence when instantiating an LSTM in Keras?", "answer_count": 2, "views": 629, "accepted_answer": "8421", "answers": {"8421": {"line": 5667, "body": "In Keras, what you specify is the hidden layer size. So :\nLSTM(128)\n\ngives you a Keras layer representing a LSTM with a hidden layer size of 128.\nAs you said :\n\nFrom the LSTM/RNN unfolding image or description, I found that each RNN cell take one time step at a time\n\nSo if you picture your RNN for one time step, it will look like this : \n\nAnd if you unfold it in time, it look like this : \n\nYou are not limited in your sequence size, this is one of the feature of RNN : since you input your sequence element by element, the size of the sequence can be variable.  \nThat number, 128, represent just the size of the hidden layer of your LSTM. You can see the hidden layer of the LSTM as the memory of the RNN. \nOf course the goal is not for the LSTM to remember everything of the sequence, just link between elements. That's why the size of the hidden layer can be smaller than the size of your sequence.\nSources :\n\nKeras documentation\nThis blog\n\nEdit\nFrom this blog :\n\nThe larger the network, the more powerful, but it's also easier to overfit. Don't want to try to learn a million parameters from 10,000 examples - parameters > examples = trouble.\n\nSo the consequence of reducing the size of hidden state of LSTM is that it will be simpler. Might not be able to get the links between the element of the sequence. But if you put a too big size, your network will overfit ! And you absolutely don't want that.\nAnother really good blog on LSTM : this link\n", "type": 2, "id": "8421", "date": "2018-10-15T08:31:44.950", "score": 1, "comment_count": 2, "parent_id": "8412"}, "8459": {"line": 5698, "body": "Although this question has been answered I'd add a couple of remarks towards general neural networks design\nAs you know very NN has three types of layers: input, hidden, and output. Once this network is initialized, you can iteratively tune the configuration during training.\nTo optimize the network configuration we can use pruning.\nPruning describes a set of techniques to trim network size (by nodes not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.)\nYou can find more here: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n", "type": 2, "id": "8459", "date": "2018-10-16T07:22:32.797", "score": 0, "comment_count": 0, "parent_id": "8412"}}}
{"line": 6834, "body": "Is there a connection between the approximator network sizes in a RL task and the speed of convergence to an (near) optimal policy or value function?\nWhen thinking about this, I came across the following thoughts:\n\nIf the network would be too small, the problem won't get enough representation and would never be solved, and the network would converge to its final state quickly.\nIf the network would be infinitely big (assuming no vanishing gradients and the likes), the network would converge to some (desirable) over-fitting, and the network would converge to its final state very slowly, if at all.\nThis probably means there is some golden middle ground.\n\nWhich leads me to the interesting question:\n4. Assuming training time is insignificant relative to running the environment (like in real life environments), then if a network of size M converges to an optimal policy in average after N episodes, would changing M make a predictable change on N?\nIs there any research, or known answer to this?\nHow to know that there is no more need to increase the network size?\nHow to know if the current network is too large?\nNote: please regard question 4 as the main question here.\n", "type": 1, "id": "10265", "date": "2019-01-28T10:16:21.447", "score": 2, "comment_count": 3, "tags": ["neural-networks", "machine-learning", "reinforcement-learning", "deep-rl"], "title": "Is there a relation between the size of the neural networks and speed of convergence in deep reinforcement learning?", "answer_count": 1, "views": 68, "accepted_answer": "10297", "answers": {"10297": {"line": 6847, "body": "Speed and size, no. That's because speed is dependent on processor clock periods and the effective parallel nature of the particular deep RL design in the environment, which is also dependent on cores and host clustering. Size is not really quantifiable in a way that is meaningful in this relationship because there is a broader and more complex structure of a network that might be trained to produce a value to use in the RL algorithm chosen.\n\nWidth for each layer\nCell type and possibly activation function for each layer\nHyper-parameters\n\nOne can say that number of clock cycles times number of effective parallel processors is correlated to all the above, the overhead of the RL algorithm used, and the size of each data tensor.\nThere are a few inequalities developed in the PAC (probably approximately correct) framework, so it would not be surprising if there were some bounding rules for the relationship between clock cycles, effective parallelism, data widths, activation functions, and RL algorithms for deep RL.\nStudy of the algorithm, perhaps in conjunction with experimentation, may reveal the primary factors, essentially the processing bottlenecks. Further study of the factors involved in controlling loop iteration counts, which cause the bottleneck, could permit the quantification of computing effort required to maintain a particular maximum allowable response time, but that would be specific to a particular design.\nSuch might produce a metric that is effectively a count of clock cycles across the system's potentially parallel architecture for a worst case or mean RL action selection. That could then be used to determine the response time for a given system with all of the factors mentioned above fixed, including the priority and scheduling of the processes in each operating system involved.\nHere's a guess. Feel free to critique, since such a formulation is a project far beyond the effort that should be put into answering a question online.\n$$ t_r \\propto e^{k_v} \\, n_v \\, \\eta_v \\, t_v \\, \\mu_v \\, \\sum c_v + e^{k_d} \\, n_d \\, \\eta_d \\, t_d \\, \\mu_d \\, \\sum c_d \\; \\text{,} $$\nwhere $t$ is time, $k$ is tensor complexity, $n$ is number of cores, $\\eta$ is the effective efficiency of the parallel processing, $\\mu$ is the effective overhead cost of the glue code, $c$ is the cycles require of the significant (and probably repetitive) elements in the algorithms, $t_r$ is the total time to response, and the subscripts $v$ and $d$ designate the variable subscripted as either RL value determination metrics or deep network metrics respectively.\n", "type": 2, "id": "10297", "date": "2019-01-29T16:49:36.540", "score": 0, "comment_count": 3, "parent_id": "10265"}}}
{"line": 6567, "body": "I'm trying to understand how the dimensions of the feature maps produced by the convolution are determined in a ConvNet.\nLet's take, for instance, the VGG-16 architecture. How do I get from 224x224x3 to 112x112x64? (The 112 is understandable, it's the last part I don't get)\nI thought the CNN was to apply filters/convolutions to layers (for instance, 10 different filters to channel red, 10 to green: are they the same filters between channels ?), but, obviously, 64 is not divisible by 3.\nAnd then, how do we get from 64 to 128? Do we apply new filters to the outputs of the previous filters? (in this case, we only have 2 filters applied to previous outputs) Or is it something different?\n\n", "type": 1, "id": "9870", "date": "2019-01-07T11:54:32.547", "score": 0, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "convolutional-layers", "vgg"], "title": "How are the dimensions of the feature maps produced by the convolutional layer determined in VGG-16?", "answer_count": 3, "views": 301, "accepted_answer": "9891", "answers": {"9874": {"line": 6570, "body": "The 64 here is the number of filters that are used.\nThe picture is kind of misleading in that it leaves out the transition of the maxpool.\n\nBelow is a text description of the size of the features as they go through the network with the number of filters in bold.\n\nThe first 2 layers in the diagram you posted contain 64 3x3 convs\nresulting in a 224x224x64 matrix of features. \nThis is then fed into a maxpool which reduces the size to a 112x112x64 matrix. \nThis is then fed to 3 layers of 128 3x3 convs resulting in a 112x112x128\nmatrix. \nThen another maxpool giving a 56x56x128 matrix.\nFeeding that to 3 layers of 256 3x3 convs results in a 56x56x256 matrix.\nThis is then fed into another maxpool giving a 28x28x256 matrix,\nWhich is then fed into 3 layers of 512 3x3 convs resulting in a\n28x28x512 matrix.\nAnother maxpool gives a 14x14x512 matrix which is fed to 3 layers of 512 3x3 convs giving a matrix of 14x14x512 features.\nAnother maxpool reduces this to a 7x7x512 which is then\ngiven to 3 fully connected layers of 4096 units each before being\nsent to a softmax.\n\n", "type": 2, "id": "9874", "date": "2019-01-07T16:34:52.463", "score": 2, "comment_count": 0, "parent_id": "9870"}, "9891": {"line": 6578, "body": "Both responses I got are correct but do not answer exactly what I was looking for.\nThe answer to my question is : each filter is a 2D convolution. It is applied to every channel from previous node (so we get N 2D matrices). Then all of these matrices are added up to make a final matrix (1 matrix for 1 filter). Finally, the output is all filters' matrices in parallel (like channels).\nThe hard part was to find the \"sum up\", since many websites speak of it as a 3D convolution (which is not !).\n", "type": 2, "id": "9891", "date": "2019-01-08T12:15:37.960", "score": 0, "comment_count": 0, "parent_id": "9870"}, "9875": {"line": 6571, "body": "For learning image features with CNNs, we use 2D Convolutions. Here 2D does not refer to the input of the operation, but the output.\nConsider you have an input tensor of size 224 x 224 x 3. Say for example you have 64 different convolution kernels. Theses kernels are also 3 dimensional. Each kernel will produce a 2D matrix as output. Since you have 64 different kernels/filters, you will have 64 different 2D matrices. In other words, you got a tensor with depth 64 as output.\n\nI would suggest you to go through this question:\nUnderstanding 1D, 2D and 3D convolutions\n", "type": 2, "id": "9875", "date": "2019-01-07T16:45:31.357", "score": 2, "comment_count": 0, "parent_id": "9870"}}}
{"line": 7385, "body": "I am doing some experimentation on neural networks, and for that I am trying to program a plain OCR task. I have learned CNNs are the best choice ,but for the time being and due to my inexperience, I wanna go step by step and start with feedforward nets.\nSo my training data is a set of roughly 400 16*16 images extracted from a script that draws every alphabet char in a tiny image for a small set of fonts registered in my computer.\nThen the test data set is extracted from the same procedure, but for all fonts in my computer.\nWell, results are quite bad. Get an accuracy of aprox. 45-50%, which is very poor... but that's not my question.\nThe point is that I can't get the MSE below 0.0049, no matter what hidden layer distribution I apply to the net. I have tried with several architectures and all comes down to this figure. Does that mean the net cannot learn any further given the data?\nThis MSE value however throws this poor results too.\nI am using Tensorflow API directly, no keras or estimators and for a list of 62 recognizable characters these are examples of the architectures I have used: [256,1860,62] [256,130,62] [256,256, 128 ,62] [256,3600,62] ....\nBut never get the MSE below 0.0049, and still results are not over 50%.\nAny hints are greatly appreciated.\n", "type": 1, "id": "11004", "date": "2019-03-04T21:13:16.217", "score": 2, "comment_count": 5, "tags": ["feedforward-neural-networks", "optical-character-recognition"], "title": "Attempting to solve a optical character recognition task using a feed-forward network", "answer_count": 2, "views": 80, "accepted_answer": null, "answers": {"11024": {"line": 7398, "body": "If you are serious about OCR systems and not just trying to learn CNN, start with a known good OSS product like tesseract. OCR some 1200 DPI grayscale documents, reverse engineer it to some degree so that you understand the design, join the development team, and pick up a ticket or two. Once you get into pull requests for a real system that is in constant use for a while, you will be an expert. Starting from scratch would make sense after you are an expert.\n", "type": 2, "id": "11024", "date": "2019-03-05T12:27:09.167", "score": 0, "comment_count": 0, "parent_id": "11004"}, "27485": {"line": 18860, "body": "So the training data is a small number of examples (400) drawn from a small set of fonts, and the test data is a much larger dataset drawn from a much larger set of fonts and is therefore much more variable than the training data. Two issues here are the small training data size and the difference in distributions between the training and test data. I would try the following:\n\nInstead of defining your own architecture, try some of the pretrained architectures available in Keras such as ResNet50 or VGG16. You can start them either with random weights or imagenet weights. Remove the top layer and put your own layers on. You can also selectively unfreeze layers and see if that makes any difference.\nTo deal with the issues I mention above, use data augmentation to introduce variability into the training set.\n\nYou also have not mentioned in the post how you decide to stop training the network. In case the network is overfitting, you could try finishing training earlier.\n", "type": 2, "id": "27485", "date": "2021-04-23T19:15:04.710", "score": 0, "comment_count": 0, "parent_id": "11004"}}}
{"line": 7742, "body": "This is my first post so please forgive me for any mistakes.\nI am working on an object detection algorithm that can detect abnormalities in an x-ray. As a prototype, I will be using yolov3 (more about yolo here: 'https://pjreddie.com/darknet/yolo/')\nHowever, one radiologist mentioned that in order to produce a good result you need to take into account the demographics of the patient.\nIn order to do that, my neural network must take into account both text an an image. Some suggestions have been made by other people for this question. For example, someone recommended taking the result of a convolution neural network and a seperate text neural network.\nHere is an image for clarification:\n\nImage Credits: This image (https://cdn-images-1.medium.com/max/1600/1*oiLg3C3-7Ocklg9_xubRRw.jpeg) from Christopher Bonnett's article (https://blog.insightdatascience.com/classifying-e-commerce-products-based-on-images-and-text-14b3f98f899e)\nFor more details, please refer to above-mentioned article. It has explained how e-commerce products can be classified into various category hierarchies using both image and text data.\nHowever, a when convolution  neural network is mention it usssualy means it is used for classification instead of detection \nhttps://www.quora.com/What-is-the-difference-between-detection-and-classification-in-computer-vision (Link for comparison between detection and classification)\nIn my case, when I am using yolov3, how would it work. Would I be using yolov3 output vector which would be like this format \nclass, center_x, center_y, width and height\nMy main question is how would the overall structure of my neural network be like if I have both image and text as input while using yolov3.\nThank you for taking the time to read this.\n", "type": 1, "id": "11510", "date": "2019-03-28T23:55:54.163", "score": 1, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "computer-vision", "object-recognition", "automation"], "title": "Detecting abnormalities in x-rays while taking into account demographics of a patient -automated", "answer_count": 2, "views": 66, "accepted_answer": "11531", "answers": {"11562": {"line": 7784, "body": "\nyou need to take into account the demographics of the patient\n\nHow, exactly? \n\nIs it a difference of, say, threshold? In this case you can do this serially (as @mirror2image mentions): process the image and then conclude by comparing the size of what you saw to, say, an age-dependent threshold.\nOr has the whole processing to be different? In the extreme, you would not wait until the very end before asking whether the patient is a man if you are looking for prostate cancer.\n\nTo design the model, you need enough medical understanding to make such choices. The model can handle the parameters, but you have to choose the architecture.\n", "type": 2, "id": "11562", "date": "2019-03-31T17:05:14.057", "score": 0, "comment_count": 6, "parent_id": "11510"}, "11531": {"line": 7760, "body": "First of all you don't need \"text\" input as in Christopher Bonnett's blog. Your case is more easy - demographic is table data, which can be expressed as vector of numeric values. This data should be processed - pushed through one or two fully connected layers. The trick is where, to what part of yolo to concatenate results of processing of this vector. Because it's vector data it should concatenated to fully connected layer. Where exactly should be found by experiments, but as starting point it could be concatenated to before-last (before output) fully connected layer (I think for yolo it's 4096-size layer). \nOverall I'd say that is not a trivial task. It require some experience with deep learning, good understanding of yolo design, yolo algorithm and a lot of experimentation, both with architecture and hyperparameters. Probably worth solid paper. Good luck.\n", "type": 2, "id": "11531", "date": "2019-03-29T17:16:04.607", "score": 0, "comment_count": 4, "parent_id": "11510"}}}
{"line": 7450, "body": "I am working on a MLP neural networks, using supervised learning (2 classes and multi-class classification problems). For the hidden layers, I am using $\\tanh$ (which produces an output in the range $[-1, 1]$) and for the output layer a softmax (which gives the probability distribution between $0$ and $1$). As I am working with supervised learning, should be my targets output between 0 and 1, or $-1$ and $1$ (because of the $\\tanh$ function), or it does not matter?\nThe loss function is quadratic (MSE).\n", "type": 1, "id": "11100", "date": "2019-03-08T12:52:43.333", "score": 5, "comment_count": 1, "tags": ["neural-networks", "classification", "activation-function", "supervised-learning"], "title": "What should the range of the output layer be when performing classification?", "answer_count": 2, "views": 104, "accepted_answer": null, "answers": {"11106": {"line": 7455, "body": "For this particular classification problem, I would recommend you using a softmax function whose output range is [0,1].\nThe sum of all outputs should be 1, so an advantage of using a softmax function is that you get a percentage of how confident the network is in this classification.\nSide note: As DuttaA has commented, cross entropy loss is a better loss function than the quadratic mean squared error.\n", "type": 2, "id": "11106", "date": "2019-03-08T20:05:36.407", "score": 1, "comment_count": 0, "parent_id": "11100"}, "13801": {"line": 9552, "body": "your targets should be in the same range as your output functions other wise your loss function wont be accurate, with supervised learning your trying to reduce the loss of your output against your targets so in this case your targets should be the true/optimal probability distribution for that set of input data. Im from the midwest so obligatory \"cant compare apples to oranges\" here ;)\n", "type": 2, "id": "13801", "date": "2019-08-05T21:43:34.387", "score": 0, "comment_count": 0, "parent_id": "11100"}}}
{"line": 6817, "body": "In an MLP with ReLU activation functions after each hidden layer (except the final),\nLet's say the final layer should output positive and negative values.\nWith ReLU intermediary activations, this is still possible because the final layer, despite taking in positive inputs only, can combine them to be negative.\nHowever, would using leaky ReLU allow faster convergence? Because you can pass in negative values as input to the final layer instead of waiting till the final layer to make things negative\n", "type": 1, "id": "10228", "date": "2019-01-27T05:54:39.257", "score": 4, "comment_count": 0, "tags": ["neural-networks"], "title": "Does leaky relu help learning if the final output needs negative values?", "answer_count": 3, "views": 1474, "accepted_answer": null, "answers": {"10236": {"line": 6820, "body": "Both the output and the the gradient at the negative part of leaky ReLU is 100 times lower than at the positive part. I doubt that they have any significant impact on training direction and/or on the final output of a trained model unless the model is severely underfitting.\n", "type": 2, "id": "10236", "date": "2019-01-27T11:38:12.923", "score": 0, "comment_count": 0, "parent_id": "10228"}, "10343": {"line": 6878, "body": "The answer depends on a case to case basis. It may so happen that a dataset performs very well on ReLu but takes more number of iterations to converge on leaky ReLu's or PReLu's or vice-versa. There are 2 arguments to consider here:\n\nReLu is the most non-linear among all other type of ReLu's, and by this not so mathematical term I mean to say that it has the steepest drop in slope at 0, as compared to any other type of modified ReLu's.\nReLu's omit negative values which can be a significant problem with context to data normalisation. As this video (~10:00) from Stanford explains how data normalisation is necessary in context of signs of weight updates, so we can very roughly say any form of Leaky ReLU's somewhat normalise the data.\n\nSo theoretically speaking (might not be mathematically rigorous) iff all the inputs have a positive correlation to the output(input increases, output also increases), ReLu should work very well and converge faster. Whereas if there is negative correlation as well then Leaky ReLu's might work better.\nThe point is that unless someone gives definitive mathematical relations of what's going on inside a NN when it is being trained, its hard to tell which will work well and which will not except from intuition.\n", "type": 2, "id": "10343", "date": "2019-02-01T10:18:47.137", "score": 2, "comment_count": 0, "parent_id": "10228"}, "10232": {"line": 6819, "body": "In short, yes Leaky Relu helps in faster convergence if your output requires both positive and negative values. But the catch is that you need to tune the negative slope of Leaky Relu which is a hyperparameter to get better accuracy.\n", "type": 2, "id": "10232", "date": "2019-01-27T09:50:19.457", "score": 2, "comment_count": 3, "parent_id": "10228"}}}
{"line": 6237, "body": "Can the same input for a plain neural network be used for CNNs? Or does the input matrix need to be structured in a different way for CNNs compared to regular NNs? \n", "type": 1, "id": "9311", "date": "2018-12-02T20:50:27.887", "score": 2, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks"], "title": "Can the same input for a plain neural network be used for a convolutional neural network?", "answer_count": 2, "views": 96, "accepted_answer": "9318", "answers": {"9316": {"line": 6239, "body": "\nNeural networks generally deal with 1 D data. For example, the data for a NN would be ( 10 , 12 ) , where 10 is the number of samples.\nConvolutional neural networks generally deal with 1D, 2D and 3D data. For example, a 2D CNN would have input as shape ( 10 , 28 , 28 , 3 ) where 10 is the number of samples and ( 28 , 28 , 3 ) is the image size. ( if the input is a image )\n\n", "type": 2, "id": "9316", "date": "2018-12-03T08:57:12.603", "score": 0, "comment_count": 0, "parent_id": "9311"}, "9318": {"line": 6240, "body": "There is no restriction on how you input a data to a NN. You can input it in 1D arrays and do element-wise multiplication using 4-5 loops and imposing certain conditions(which will be slow and hence $nD$ matrix notations are used for a CNN).\nUltimately, the library you are using (TensorFlow, NumPy might convert it into its own convenient dimensions).\nThe main thing different of a CNN from a normal NN is:\n\nThe number of parameters of a CNN in a convolutional layer is less than the number of input features.\n$parameters \\le features$ (in general it is less than). \n\nDifferent people have different ways of viewing how the convolutional layer work but the general consensus is that the weights of the convolutional layers of a CNN are like digital filters. It will be a $nD$ filter if input dimension is $nD$. The  output is obtained by superimposing the filter on a certain part of the input and doing element-wise multiplication of the values of filter and the values of the input upon which the filter is superimposed upon. How you implement this particular operation depends on you.\nSo the answer to your question will be same network cannot be used, but it might be used with modifications (a normal NN is the limiting case of a CNN where $features=parameters$.\n", "type": 2, "id": "9318", "date": "2018-12-03T13:29:41.020", "score": 1, "comment_count": 4, "parent_id": "9311"}}}
{"line": 7772, "body": "Can the inputs and outputs of a neural network (NN) be a neural network (that is, neurons and connections), so that \"if some NN exist, then edit any NN\".\nI think that by creating NNs with various inputs and outputs, interacting with each other, and optimizing them with evolution, we can create strong intelligence.\n", "type": 1, "id": "11546", "date": "2019-03-30T11:47:15.987", "score": 0, "comment_count": 1, "tags": ["neural-networks"], "title": "Can the inputs and outputs of a neural network be a neural network?", "answer_count": 1, "views": 204, "accepted_answer": "11561", "answers": {"11561": {"line": 7783, "body": "A neural network essentially is a function:\n$$\\mathbf{y} = f(\\mathbf{x}, \\mathbf{\\theta})$$\nWhere $\\mathbf{x}$ is a vector input, $\\mathbf{\\theta}$ are changeable or learnable parameters, and $\\mathbf{y}$ is a vector output.\nThere are some variations of this in practice, as you can make special arrangements of $\\mathbf{x}$ and $\\mathbf{y}$, or use an internal state and feedback loops to allow either or both $\\mathbf{x}$, $\\mathbf{y}$ to be sequences. However, the above function is basically what a neural network is; how it works beyond that summary are details that you can study.\nIn supervised learning, you are interested in fixed sizes/shapes of $\\mathbf{x}, \\mathbf{y}, \\mathbf{\\theta}$ and trying to find a value of $\\mathbf{\\theta}$ such that\n$$f(\\mathbf{x}, \\mathbf{\\theta}) \\approx g(\\mathbf{x})$$\nwhere $g(\\mathbf{x})$ is some \"true\" function that you care about, and can find or generate examples of, but typically don't fully know. The value $\\mathbf{\\theta}$ is called the parameters of the neural network. In addition to these parameters, there are also hyper-parameters of the neural network, which include how many neurons there are in each layer, the valid connections between them, which non-linear function is applied after summing connections between them, etc.\nThere are learning algorithms used to find $\\mathbf{\\theta}$ - the many variations of gradient descent being the most popular. Some algorithms - mainly evolutionary approaches - can also vary hyper-parameters, although a more common approach is to repeatedly find $\\mathbf{\\theta}$ using gradient descent, and vary hyper-parameters in different learning trials, using some metric of performance using test data to pick the best one.\n\nCan the inputs and outputs of a neural network (NN) be a neural network (that is, neurons and connections), so that \"if some NN exist, then edit any NN\".\n\nYes - partially. This is a data representation issue. To use it as an input, you would need to express the state of a neural network as a vector - or sequence of vectors - for the input, and the output/edit would also need to be a vector. Probably the simplest way to do this would be to use one network directly output a fixed length vector for $\\mathbf{\\theta}$ of the target network given an existing value of $\\mathbf{\\theta}$. That would not allow you to change connections or layer sizes etc, but it would be a very straightforward way to express \"one neural network altering another\" (ignoring whether this was in any way useful for a task).\nIf the output was a full representation of the new network, then you would have a function that took as input the definition of one neural network, and output the definition of another network. It would be up to you to convert to/from implemented neural networks and the representations for input $\\mathbf{x}$ and $\\mathbf{y}$.\nIf the output $\\mathbf{y}$ was an \"edit\" for a change $\\mathbf{x} \\rightarrow \\mathbf{x}'$, then you would have to decide what edits are allowed, design the representation and write code that applied the edit (the NN would not actually make any changes to another NN by itself). There is no standard way to do this, although there are things you could base this on (such as NEAT).\nThe big unanswered question with both of the approaches though is what your \"true function\" $g(\\mathbf{x})$ is supposed to be. Having a neural network that represents a neural network generator or edit function is only half of the problem. You also need a way to either generate \"correct\" outputs to learn from, or a way to assess outputs against a goal. \nThe goal cannot simply be \"make a valid edit\", as the number of valid edits that will do nothing useful vastly outnumbers the number of edits that have some specific purpose. This is a similar issue to the fact that there are roughly $2^{8000000}$ valid 1MB files, but only a small proportion of those will be valid image files, and a smaller proportion still will be valid images that represent a natural image that could be taken by a camera. Neural networks that generate natural images therefore must be trained using natural images as a reference, otherwise they will tend to produce meaningless static-like noise.\n\nI think that by creating NNs with various inputs and outputs, interacting with each other, and optimizing them with evolution, we can create strong intelligence\n\nThis is very broadly compatible with an Articial Life approach to AI. Although there are two differences between what you are proposing and typical A-life approaches:\n\nEvolutionary algorithms need some measure of fitness, in order to select the best performing individuals to take forward. A-life solves this by implementing a very open environment that makes no direct judgement on outputs of functions, but allows virtual creatures that collect enough resources (defined in the environment) to procreate. \n\nYour suggestion contains no hint that you are thinking of any kind of measure of success or fitness for either the editing NN or target NN. You will need some measure of fitness at least for the target network (and maybe the editing network too) if you intend to use evolutionary algorithms\n\nA-life typically does not treat the evolutionary algorithm itself (the editing or the NNs) as a learning goal. You will not see the results of a good or bad editor until many simulations have passed, so this \"meta-search\" is likely to be incredibly slow.\n\nA-life simulations are typically already quite slow to reach behaviour which is interesting (because it has emerged without direction from the developer), but usually quite simple for the given environment such as predators chasing prey.\n\n\nFrom what we know of the evolution of life, a simple feedback mechanism of RNA molecules editing other RNA molecules - the \"RNA world\" - is considered a likely pre-life step. This has some parallels with what you are suggesting - and this or something similar perhaps, has resulted in intelligent beings such as ourselves. So your idea perhaps has some merit in a theoretical sense. However, it took biology billions of years to go from such a basic stage to self-aware creatures, and this took the full processing power of large numbers of atoms interacting in ways that even all the computers in the world could not simulate in a fraction of real time.\nTo speed things up, and turn the idea into something feasible, you would need to look into viable environments and evaluations that would focus the learning towards direct measures of intelligence. Also, you would do well to compare your idea about NNs editing NNs with learning approaches that don't use such a feedback loop. \nThere is no theory that suggests a NN that can edit or output other NNs would offer any advantage for research into strong AI, compared to other search methods. The idea is basically \"AI alchemy\" - an idea of an experiment that could perhaps be done, but without any theory backing it as being better or worse than other ideas.\nPersonally, I would expect the search for a NN which is a good NN editor for a NN which has some other task, to be too slow to be useful when faced with very broad tasks such as exhibiting high level reasoning.\n", "type": 2, "id": "11561", "date": "2019-03-31T16:31:01.130", "score": 0, "comment_count": 12, "parent_id": "11546"}}}
{"line": 8090, "body": "Suppose I have a standard image classification problem (i.e. CNN is shown a single image and predicts a single classification for it). If I were to use bounding boxes to surround the target image (i.e. convert this into an object detection problem), would this increase classification accuracy purely through the use of the bounding box? \nI'm curious if the neural network can be \"assisted\" by us when we show it bounding boxes as opposed to just showing it the entire image and letting it figure it all out by itself.\n", "type": 1, "id": "11949", "date": "2019-04-22T20:45:37.747", "score": 3, "comment_count": 0, "tags": ["convolutional-neural-networks", "object-detection", "object-recognition", "bounding-box"], "title": "Can bounding boxes further improve the performance of a CNN classifier?", "answer_count": 4, "views": 370, "accepted_answer": null, "answers": {"11955": {"line": 8094, "body": "Another way to ask the question is: Does sound get clearer when you remove the background noise? \nThe obvious answer is yes and in the case of image classification, the answer is also generally yes.\nIn most cases reducing the noise (irrelevant pixels) will strengthen the signal (activations) the neural network is trying to find. \n", "type": 2, "id": "11955", "date": "2019-04-23T03:18:03.823", "score": 0, "comment_count": 1, "parent_id": "11949"}, "15560": {"line": 10149, "body": "For sure, this may be helpful. You remove excessive information from image and make the classification task a bit more simple. But you need to remember that bounding boxes may work not perfectly and accuracy of the classification algorithm may suffer from corrupted inputs (when bounding boxes are corrupted).\n", "type": 2, "id": "15560", "date": "2019-09-20T15:00:53.350", "score": 0, "comment_count": 0, "parent_id": "11949"}, "22541": {"line": 15020, "body": "One could imagine using a segmentation network as a first step of processing. Then feeding an area corresponding to a bounding box of each segmented object to the classifier.\nPotentially that could yield an increase in performance in classifying objects in an image, but not without a cost of training time, sine suddenly there are two networks to train instead of just one.\n", "type": 2, "id": "22541", "date": "2020-07-17T09:24:23.853", "score": 0, "comment_count": 0, "parent_id": "11949"}, "21958": {"line": 14566, "body": "Information outside of the bounding box could still be useful as context. So not to  lose it you can but bounding box as pixel mask into additional \"pseudo-color\" layer. That way you can also have many bounding boxes without changing input architecture. You will give network additional info without losing anything, so result shouldn't be worse at least.\n", "type": 2, "id": "21958", "date": "2020-06-17T05:03:40.307", "score": 0, "comment_count": 0, "parent_id": "11949"}}}
{"line": 7202, "body": "I am confused about how neural networks weigh different features or inputs.\nConsider this example. I have 3 features/inputs: an image, a dollar amount, and a rating.  However, since one feature is an image, I need to represent it with very high dimensionality, for example with $128 \\times 128 = 16384$ pixel values.  (I am just using 'image' as an example, my question holds for any feature that needs high dimensional representation: word counts, one-hot encodings, etc.)\nWill the $16384$ 'features' representing the image completely overwhelm the other 2 features that are the dollar amount and rating?  Ideally, I would think the network would consider each of the three true features relatively equally.  Would this issue naturally resolve itself in the training process? Would training become much more difficult of a task?\n", "type": 1, "id": "10773", "date": "2019-02-20T21:20:58.600", "score": 2, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "features"], "title": "How do neural networks weigh multiple inputs/features of different dimensionality?", "answer_count": 2, "views": 88, "accepted_answer": "10780", "answers": {"10784": {"line": 7211, "body": "The answer by ssh is correct. Your results could be further improved i) by extracting image features by a convolutional (instead of fully connected) architecture, and ii) by exploiting transfer learning. \nTo exploit transfer learning you i) pick some widely used model, eg. ResNet-18, ii) initialize it with ImageNet pretrained parameters, iii) replace its fully connected layer (the one that produces 1000-D softmax input) with your own randomly initialized fully connected layer. If you are interested, have a look at detailed instructions.\n", "type": 2, "id": "10784", "date": "2019-02-21T10:53:30.810", "score": 0, "comment_count": 0, "parent_id": "10773"}, "10780": {"line": 7209, "body": "As stated in your example, the three features are: an image, a price, a rating. Now, you want to build a model that uses all of these features and the simplest way to do is to feed them directly into the neural network, but it's inefficient and fundamentally flawed, due to the following reasons:\n\nIn the first dense layer, the neural network will try to combine raw pixel values linearly with price and rating, which will produce features that are meaningless for inference.\n\nIt could perform well just by optimizing the cost function, but the model performance will be nowhere as good as it could perform with a good architecture.\n\n\nSo, the neural network doesn't care if the data is a raw pixel value, price, or rating: it would just optimize it to produce the desired output. That is why it is necessary to design a suitable architecture for the given problem.\nPossible architecture for your given example :\n\nSeparate your raw features, i.e. pixel value, and high-level data, i.e. price and rating\n\nStack 2-3 dense layers for raw features (to find complex patterns in images)\n\nStack 1-2 dense layers for high-level features\n\nCombine them together in a final dense layer\n\n\nIf you want to de-emphasize the importance of the image, just connect the first dense layer 16,384 to another layer having fewer connections, say 1024, and have more connections from the high-level data, say 2048.\nSo, again, here's the possible architecture\n\nRaw features -> dense layer (16384) -> dense layer (1024)\nHigh-level features -> dense layer (2048)\nCombine 1 and 2 with another dense layer\n\n", "type": 2, "id": "10780", "date": "2019-02-21T08:48:47.957", "score": 2, "comment_count": 0, "parent_id": "10773"}}}
{"line": 6671, "body": "I have a Deep Feedforward Neural Network $F: W \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ (where $W$ is the space of the weights) with $L$ hidden layers, $m$ neurones per layer and ReLu activation. The output layer has a softmax activation function.\nI can consider two different loss functions: \n$L_1 = \\frac{1}{2} \\sum_i || F(W,x_i) - y||^2$ $\n \\ \\ \\ $ and   $\\ \\ \\ L_2  = -\\sum_i log(F(w,x_i)_{y_i})$\nwhere the first one is the classic quadratic loss and the second one is cross entropy loss. \nI'd like to study the norm of the derivative of the loss function and see how the two are related, which means: \n1) Let's assume I know that $|| \\frac{\\partial L_2(W, x_i)}{\\partial W}|| > r$, where $r$ is a small constant. What can I assume about $|| \\frac{\\partial L_1(W, x_i)}{\\partial W}||$ ?\n2) Are there any result which tell you that, under some hypothesis (even strict ones) such as a specific random initialisation,  $|| \\frac{\\partial L_1(W, x_i)}{\\partial W}||$ doesn't go to zero   during training?\nThank you\n", "type": 1, "id": "10025", "date": "2019-01-16T13:36:55.310", "score": 2, "comment_count": 0, "tags": ["neural-networks", "feedforward-neural-networks", "objective-functions"], "title": "Comparing and studying Loss Functions", "answer_count": 1, "views": 95, "accepted_answer": "10029", "answers": {"10029": {"line": 6674, "body": "Let's first express a network of arbitrary topology and heterogeneous or homogeneous cell type arrangements as\n$$ N(T, H, s) := \\, \\big[\\, \\mathcal{Y} = F(P_s, \\, \\mathcal{X}) \\,\\big] \\\\\n   s \\in \\mathbb{C} \\; \\land \\; s \\le S \\; \\text{,} $$\nwhere $S$ is the number of learning states or rounds, $N$ is the network of $T$ topology and $H$ hyper-parameter structure and values that at stage $s$ produces a $P$ parameterized function $f$ of $\\mathcal{X}$ resulting in $\\mathcal{Y}$. In supervised learning, the goal is that $F(P_s)$ approaches a conceptually ideal function $F_i$ as $s \\rightarrow S$.\nThe popular loss aggregation norms are not quite as the question defines them. The below more canonically expresses the level 1 and 2 norms, which systematically aggregate multidimensional disparity between an intermediate result at some stage (epoch and example index) of training and the conceptual ideal toward which the network in training is intended to converge.\n$$ {||F-\\mathcal{Y}||}_1 = \\sum{|F_i - y_i|} \\\\\n   {||F-\\mathcal{Y}||}_2 = \\sqrt{\\sum{(F_i - y_i)}^2} $$\nThese equations have been mutated by various authors to make various points, but those mutations have obscured the obviousness of their original relationship. The first is where distance can be aggregated through only orthogonal vector displacements. The second is where aggregation uses the minimum Cartesian distance by extending the Pythagorean theorem.\nNote that quadratic loss is a term with some ambiguity. These are all broadly describable as quadratic expressions of loss.\n\nDistance as described in the second expression above\nRMS where the sum is divided by the number of dimensions\n$\\text{count}(i)$\nJust the sum of squared difference by itself\n\nCross entropy is an extension of Claude Shannon's information theory concepts based on the work of Bohr, Boltzmann, Gibbs, Maxwell, von Neumann, Frisch, Fermi, and others who were interested in quanta and the thermodynamic concept of entropy as a universal principle running through mater, energy, and knowledge.\n$$ S = k_B \\log{\\Omega} \\\\\n   H(X) = - \\sum_i p(x_i) \\, \\log_2{\\, p(x_i)} \\\\\n   H(p, \\, q) = -\\sum_{x \\in \\mathcal{X}} \\, p(x) \\, \\log_2{\\, q(x)} $$\nIn this progression of theory, we begin with a fundamental postulate in quantum physics, where $k_B$ is Boltzmann's constant and $\\Omega$ are the number of microstates for the quanta. The next relation is Shannon's adaptation for information, where $H$ is the entropy in bits, thus the $\\log_2$ instead of a natural logarithm. The third relation above expresses cross-entropy in bits for features $\\mathcal{X}$ is based on the Kullback-Leibler divergence.  the p-attenuated sum of bits of q-information in .\nNotice that $p$ and $q$ are probabilities, not $F$ or $\\mathcal{Y}$ values, so one cannot substitute labels and outputs of a network into them and retain the meaning of cross entropy. Therefore level 1 and 2 norms are closely related, but cross-entropy is not a norm; it is the dispersion of one thing Cartesian distance aggregation scheme like them. Cross-entropy is remotely related but is statistically more sophisticated. To produce a cross-entropy loss function of form\n$$ {||F-\\mathcal{Y}||}_H = \\mathcal{P}(F, y) \\; \\text{,} $$\none must derive the probabilistic function $\\mathcal{P}$ that represents the cross entropy for two distributions in some way that is theoretically sound on the basis of both information theory and convergence resource conservation. It is not clear that the interpretation of cross entropy in the context of gradient descent and back propagation has caught up with the concepts of entropy in quantum theory. That's an area needing further research and deeper theoretical consideration.\nIn the question, the cross-entropy expression is not properly characterized, most evident in the fact that the expression is independent of the labels $\\mathcal{Y}$, which would be fine if for unsupervised learning except that no other basis for evaluation is represented in the expression. For the term cross-entropy to be valid, the basis for evaluation must include two distributions, a target one and one that represents the current state of learning.\nThe derivatives of the three norms (assuming the cross entropy is properly characterized) can be studied for the case of $\\ell$ ReLU layers by generalizing the chain rule (from differential calculus) as applied to ReLU and the loss function developed by applying each of the three norms to aggregate measures of disparity from optimal.\nRegarding the inference in sub-question (1) nothing of particular value can be assumed about the Jacobians of level 2 norms from level 1 norms, both with respect to parameters $P$ or vice versa, except the retention of sign. This is because we cannot determine much about the correlation between the output channels of the network.\nThere is no doubt, regarding sub-question (2), that some constraint, set of constraints, stochastic distribution applied to initialization, hyper-parameter settings, or data set features, labels, or number of examples have implications for the reliability and accuracy of convergence. The PAC (probably approximately correct) learning framework is one system of theory that approaches this question with mathematical rigor. One of its practical uses, among others, is to derive inequalities that predict feasibility in some cases and produce more lucid approaches to learning system projects.\n", "type": 2, "id": "10029", "date": "2019-01-16T15:49:58.077", "score": 0, "comment_count": 0, "parent_id": "10025"}}}
{"line": 8467, "body": "I'm studying convolutional neural networks from the following article https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/.\nIf we take a grayscale image, the value of the pixel will be between 0 and 255. Now, if we apply a filter to our \"new image\", can we have pixels whose values are not included in this range? In this case, how can we create the convolved image?\n", "type": 1, "id": "12429", "date": "2019-05-20T11:10:51.767", "score": 2, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks"], "title": "What are the value of the pixels of the convolved image?", "answer_count": 2, "views": 109, "accepted_answer": null, "answers": {"12943": {"line": 8860, "body": "I think normalisation into range 0-1 is needed or at least to 5-10 cliping   because the values will become astronomical after many layers. Convolved image has a vector of features for each pixel. Take a RGB for example -> each color is a feature in one pixel, the next map will be like 'horisontal line, vertical line, circle' for one pixel surroundings.\n", "type": 2, "id": "12943", "date": "2019-06-19T15:17:34.720", "score": 0, "comment_count": 0, "parent_id": "12429"}, "12430": {"line": 8468, "body": "The convolved image can be considered a feature map, where each new neuron represents some indication (or lack-there-of) of a feature in some receptive field of the original image, so no it does not need to be a valid image in the output.  \nIf you specifically care for it to be an image as an output, you can do a couple of things:  \n1) normalize the produced feature map to some set range that youre working in (0-255 or 0-1)  \n2) make the filter a valid probability distribution, and you know the output will stay in the same range as the input (ex: Gaussian filters) \n", "type": 2, "id": "12430", "date": "2019-05-20T12:23:23.923", "score": 1, "comment_count": 2, "parent_id": "12429"}}}
{"line": 8026, "body": "In section 3.2.1 of Attention Is All You Need the claim is made that:\n\nDot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n\nIt does not make sense why dot product attention would be faster. Additive attention is nearly identical computation wise; the main difference is $Q + K$ instead of $Q K^T$ in dot product attention. $Q K^T$ requires at least as many addition operations as $Q + K$, so how can it possibly be faster?\n", "type": 1, "id": "11866", "date": "2019-04-17T04:38:33.790", "score": 8, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "attention"], "title": "Why is dot product attention faster than additive attention?", "answer_count": 2, "views": 2484, "accepted_answer": null, "answers": {"11874": {"line": 8034, "body": "The additive attention method that the researchers are comparing to corresponds to a neural network with 3 layers (it is not actually straight addition). Computing this will involve one multiplication of the input vector by a matrix, then by another matrix, and then the computation of something like a softmax. Smart implementation of a dot-product will not break out the whole matrix multiplication algorithm for it, and it will basically be a tight, easily parallelized loop.\n", "type": 2, "id": "11874", "date": "2019-04-18T04:00:34.207", "score": 2, "comment_count": 5, "parent_id": "11866"}, "31584": {"line": 20608, "body": "In additive attention (as described in the paper by Bahdanau et al. (2014)), the alignment model $a$ is represented by a feedforward neural network. If you look in their appendix, they actually implement this as\n$$ e_{ij} = V_a^T \\tanh \\left(W_a s_{i-1} + U_a h_j\\right) = V_a^T \\tanh \\left(Q + K\\right).$$\nIn contrast, in dot-product attention (as described in the paper by Vaswani et al. (2017)), the alignment model is implemented as\n$$ e_{ij} = W_Q s_{i-1} \\left(W_K h_j\\right)^T = Q K^T.$$\nThe computational advantage is that the dot-product alignment model has only two weight matrices and only needs matrix multiplication, for which highly-optimized code exists.\n", "type": 2, "id": "31584", "date": "2021-09-07T07:33:22.737", "score": 0, "comment_count": 0, "parent_id": "11866"}}}
{"line": 9164, "body": "The Wikipedia article for the universal approximation theorem cites a version of the universal approximation theorem for Lebesgue-measurable functions from this conference paper. However, the paper does not include the proofs of the theorem. Does anybody know where the proof can be found?\n", "type": 1, "id": "13317", "date": "2019-07-11T08:40:14.007", "score": 18, "comment_count": 0, "tags": ["neural-networks", "reference-request", "proofs", "function-approximation", "universal-approximation-theorems"], "title": "Where can I find the proof of the universal approximation theorem?", "answer_count": 3, "views": 6314, "accepted_answer": "13319", "answers": {"13319": {"line": 9166, "body": "There are multiple papers on the topic because there have been multiple attempts to prove that neural networks are universal (i.e. they can approximate any continuous function) from slightly different perspectives and using slightly different assumptions (e.g. assuming that certain activation functions are used). Note that these proofs tell you that neural networks can approximate any continuous function, but they do not tell you exactly how you need to train your neural network so that it approximates your desired function. Moreover, most papers on the topic are quite technical and mathematical, so, if you do not have a solid knowledge of approximation theory and related fields, they may be difficult to read and understand. Nonetheless, below there are some links to some possibly useful articles and papers.\nThe article A visual proof that neural nets can compute any function (by Michael Nielsen) should give you some intuition behind the universality of neural networks, so this is probably the first article you should read.\nThen you should probably read the paper Approximation by Superpositions of a Sigmoidal Function (1989), by G. Cybenko, who proves that multi-layer perceptrons (i.e. feed-forward neural networks with at least one hidden layer) can approximate any continuous function. However, he assumes that the neural network uses sigmoid activations functions, which, nowadays, have been replaced in many scenarios by ReLU activation functions. Other works (e.g. [1, 2]) showed that you don't necessarily need sigmoid activation functions, but only certain classes of activation functions do not make neural networks universal.\nThe universality property (i.e. the ability to approximate any continuous function) has also been proved in the case of convolutional neural networks. For example, see Universality of Deep Convolutional Neural Networks (2020), by Ding-Xuan Zhou, which shows that convolutional neural networks can approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough.\nSee also page 632 of Recurrent Neural Networks Are Universal Approximators (2006), by Schafer et al., which shows that recurrent neural networks are universal function approximators. See also On the computational power of neural nets (1992, COLT) by Siegelmann and Sontag. This answer could also be useful.\n", "type": 2, "id": "13319", "date": "2019-07-11T09:05:24.107", "score": 19, "comment_count": 0, "parent_id": "13317"}, "22490": {"line": 14979, "body": "Just wanted to add that the new text Deep Learning Architectures\nA Mathematical Approach mentions this result, but I'm not sure if it gives a proof. It does mention an improved result by Hanin (http://arxiv.org/abs/1708.02691) for which I think it does give at least a partial proof. The original paper by Hanin seems to omit some proofs as well, but the published version (https://www.mdpi.com/2227-7390/7/10/992/htm) may be more complete.\n", "type": 2, "id": "22490", "date": "2020-07-14T01:22:11.070", "score": 0, "comment_count": 2, "parent_id": "13317"}, "25917": {"line": 17637, "body": "\"Modern\" Guarantees for Feed-Forward Neural Networks\nMy answer will complement nbro's above, which gave a very nice overview of universal approximation theorems for different types of commonly used architectures, by focusing on recent developments specifically for feed-forward networks.  I'll try an emphasis depth over breadth (sometimes called width) as much as possible.  Enjoy!\n\nPart 1: Universal Approximation\nHere I've listed a few recent universal approximation results that come to mind.  Remember, universal approximation asks if feed-forward networks (or some other architecture type) can approximate any (in this case continuous) function to arbitrary accuracy (I'll focus on the : uniformly on compacts sense).\nLet me mention, that there are two types of guarantees: quantitative ones and qualitative ones.  The latter are akin to Hornik's results (Neural Networks - 1989) which simply state that some neural networks can approximate a given (continuous) function to arbitrary precision.  The former of these types of guarantees quantifies the number of parameters required for a neural network to actually perform the approximation and are akin to Barron's (now) classical paper (IEEE - 1993)'s breakthrough results.\n\nShallow Case:  If you want quantitative results only for shallow networks:\nThen J. Siegel and J. Xu (Neural Networks - 2020) will do the trick but (note: The authors deal with the Sobolev case but you get the continuous case immediately via the Soblev-Morrey embedding theorem.)\n\nDeep (not narrow) ReLU Case:  If you want a quantitative proof for deep networks (but not too narrow) with ReLU activation function then Dimity Yarotsky's result (COLT - 2018) will do the trick!\n\nDeep and Narrow: To the best of my knowledge, the first quantitative proof for deep and narrow neural networks with general input and output spaces has recently appeared here:\nhttps://arxiv.org/abs/2101.05390 (preprint - 2021).\nThe article is a constructive version of P. Kidger and T. Lyon's recent deep and narrow universal approximation theorem (COLT - 2020) (qualitative) for functions from $\\mathbb{R}^p$ to $\\mathbb{R}^m$ and A. Kratsios and E. Bilokpytov's recent Non-Euclidean Universal Approximation Theorem (NeurIPS - 2020).\n\n\n\nPart 2: Memory Capacity\nA related concept is that of \"memory capacity of a deep neural network\".\nThese results seek to quantify the number of parameters needed for a deep network to learn (exactly) the assignment of some input data $\\{x_n\\}_{n=1}^N$ to some output data $\\{y_n\\}_{n=1}^N$.  For example; you may want to take a look here:\n\nMemory Capacity of Deep ReLU networks: R. Vershynin's very recent publication Memory Capacity of Neural Networks with Threshold and Rectified Linear Unit Activations - (SIAM's SIMODS 2020)\n\n", "type": 2, "id": "25917", "date": "2021-01-22T12:25:26.100", "score": 5, "comment_count": 4, "parent_id": "13317"}}}
{"line": 8110, "body": "We often train neural networks by optimizing the mean squared error (MSE), which is an equation of a parabola $y=x^2$, with gradient descent.\nWe also say that weight adjustment in a neural network by the gradient descent algorithm can hit a local minimum and get stuck in there.\nHow are multiple local minima on the equation of a parabola possible, if a parabola has only one minimum?\n", "type": 1, "id": "11979", "date": "2019-04-23T19:12:53.727", "score": 8, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "optimization", "gradient-descent", "mean-squared-error"], "title": "How is it possible that the MSE used to train neural networks with gradient descent has multiple local minima?", "answer_count": 3, "views": 1159, "accepted_answer": "11982", "answers": {"11981": {"line": 8112, "body": "\nHow are multiple local minima on the equation of a parabola possible, if a parabola has only one minimum?\n\nA parabola has one minimum, and no separate local minima. So it isn't possible.\nHowever...\n\nGradient descent works on the equation of mean squared error, which is an equation of a parabola $y=x^2$\n\nJust because the loss function is a parabola with respect to the direct input, does not mean that the loss function is a parabola with respect to the parameters that indirectly cause that error.\nIn fact it only remains true for linear functions. When considering linear regression $\\hat{y} = \\sum_i w_i x_i + b$, there is only one global minimum (with specific values of $w_i$ or specific vector $\\mathbf{w}$), and your assertion is true.\nOnce you add nonlinear activations, as in neural networks, then the relationship between error function and parameters of the model becomes far more complex. For the last/output layer you can carefully choose a loss function so that this cancels out - you can keep your single global minimum for logistic regression and softmax regression. However, one or more hidden layers, and all bets are off.\nIn fact you can prove quite easily that a neural network with a hidden layer must have multiple stationary points (not necessarily local minima). The outline of the proof is to note that there must be multiple equivalent solutions, since in a fully-connected network you can re-arrange the nodes into any order, move the weights to match, and it will be a new solution with exactly the same behaviour, including the same loss on the dataset. So a neural network with one hidden layer with $n$ nodes must have $n!$ absolute minimums. There is no way for these to exist without other stationary points in-between them.\nThere is theory to suggest that most of the stationary points found in practice will not be local minima, but saddle points.\nAs an example, this is an analysis of saddle points in a simple XOR approximator.\n", "type": 2, "id": "11981", "date": "2019-04-23T19:44:56.560", "score": 2, "comment_count": 0, "parent_id": "11979"}, "11982": {"line": 8113, "body": "$g(x) = x^2$ is indeed a parabola and thus has just one optimum. \nHowever, the $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y}) = \\sum_i (y_i - f(x_i))^2$, where $\\boldsymbol{x}$ are the inputs, $\\boldsymbol{y}$ the corresponding labels and the function $f$ is the model (e.g. a neural network), is not necessarily a parabola. In general, it is only a parabola if $f$ is a constant function and the sum is over one element. \nFor example, suppose that $f(x_i) = c, \\forall i$, where $c \\in \\mathbb{R}$. Then $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y}) = \\sum_i (y_i - c)^2$ will only change as a function of one variable, $\\boldsymbol{y}$, as in the case of $g(x) = x^2$, where $g$ is a function of one variable, $x$. In that case, $(y_i - c)^2$ will just be a shifted version (either to the right or left depending on the sign of $c$) of $y_i^2$, so, for simplicity, let's ignore $c$. So, in the case $f$ is a constant function, then $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y}) = \\sum_i y_i^2$, which is a sum of parabolas $y_i^2$, which is called a paraboloid. In this case, the paraboloid corresponding to $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y}) = \\sum_i y_i^2$ will only have one optimum, just like a parabola. Furthermore, if the sum is just over one $y_i$, that is, $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y}) = \\sum_i y_i^2 = y^2$ (where $\\boldsymbol{y} = y$), then the MSE becomes a parabola. \nIn other cases, the MSE might not be a parabola or have just one optimum. For example, suppose that $f(x) = x^2$, $y_i = 1$ ($\\forall i$), then $h(x) = (1 - x^2)^2$  looks as follows\n\nwhich has two minima (and one maximum): at $x=0$ and $x=1$. We can find the two minima of this function $h$ using calculus: $h'(x) = -4x(1 - x^2)$, which becomes zero when $x=0$ and $x=1$. \nIn this case, we only considered one term of the sum. If we considered the sum of terms of the form of $h$, then we could even have more \"complicated\" functions.\nTo conclude, given that $f$ can be arbitrarily complex, then also $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y})$, which is a function of $f$, can also become arbitrarily complex and have multiple minima. Given that neural networks can implement arbitrarily complex functions, then $\\text{MSE}(\\boldsymbol{x}, \\boldsymbol{y})$ can easily have multiple minima. Moreover, the function $f$ (e.g. the neural network) changes during the training phase, which might introduce more complexity, in terms of which functions the MSE can be and thus which (and how many) optima it can have.\n", "type": 2, "id": "11982", "date": "2019-04-23T19:49:21.737", "score": 6, "comment_count": 1, "parent_id": "11979"}, "32453": {"line": 21332, "body": "Another view on this topic: think about the derivative of the MSE with respect to the inputs. You will need to apply the chain rule:\n$$\\frac{dMSE}{dx_i}=\\sum_i2(y_i-f(x_i))\\cdot(-1)\\cdot\\frac{df(x_i)}{dx_i}$$\nwhich only resembles the derivative of the parabola when, as mentioned in nbro's answer, $f(x_i)=c, \\forall\\ i$ or, more generally, when they $f(x)$ is linear (Neil's answer).\n", "type": 2, "id": "32453", "date": "2021-11-18T14:22:22.453", "score": 0, "comment_count": 0, "parent_id": "11979"}}}
{"line": 8397, "body": "In LeNet 5's first layer, the number of feature maps is equal to the number of kernels. However, the second convolutional layer has a depth different from the 3rd layer. Does the filter size dictate the number of feature maps?\n", "type": 1, "id": "12345", "date": "2019-05-16T09:32:32.857", "score": 2, "comment_count": 2, "tags": ["deep-learning", "convolutional-neural-networks", "features"], "title": "Is the number of feature maps equal to the number of kernels in the LeNet 5 architecture?", "answer_count": 2, "views": 125, "accepted_answer": "12369", "answers": {"12369": {"line": 8415, "body": "\nIn this case, each kernel has the same depth as the depth of the input cuboid. In the architecture above, we have an (input) cuboid of dimension $14 \\times 14 \\times 6$, where $6$ is the depth, which is followed by an (output) cuboid of dimension $10 \\times 10 \\times 16$, where $16$ is the depth, which is the number of (output) feature maps (or channels) and it is also equal to the number of kernels applied to the (input) cuboid of dimensions $14 \\times 14 \\times 6$. Each of these $16$ kernels has a depth of $6$. So, each of these $16$ kernels has a shape of $5\\times 5 \\times 6$.\nIn general, the depth of the input cuboid can be different than the depth of the output cuboid, which is often equal to the number of kernels applied to the input cuboid (but this is not always the case). Furthermore, the depth of each kernel (applied to the input cuboid) has (often) the same depth as the input cuboid.\n", "type": 2, "id": "12369", "date": "2019-05-17T13:18:25.587", "score": 0, "comment_count": 1, "parent_id": "12345"}, "12368": {"line": 8414, "body": "The # of kernels will be the channel length, looking at the image you posted in your comment from post, I do not understand where you see the inconsistency.\n", "type": 2, "id": "12368", "date": "2019-05-17T12:56:34.727", "score": 0, "comment_count": 0, "parent_id": "12345"}}}
{"line": 7864, "body": "I read that functions are used as activation functions only when they are differentiable. What about the unit step activation function? So, is there any other reason a function can be used as an activation function (apart from being differentiable)?\n", "type": 1, "id": "11672", "date": "2019-04-05T10:47:53.840", "score": 3, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "activation-function"], "title": "What kind of functions can be used as activation functions?", "answer_count": 2, "views": 197, "accepted_answer": null, "answers": {"11680": {"line": 7871, "body": "Not completely sure your question. Do you mean\nQ. why should we use activation function? \nAns: we need to introduce non-linearity to the network. Otherwise, multiple layers are no difference from single layer network. (It is obvious as we write things in matrix form, and say when we have two layers with weights $W_1$ and $W_2$, the two layer is no difference from a single layer with weight $W_2 W_1$.\nQ. why they need to be differentiable?\nAns: Just for sake that we can back-propagate gradients back to earlier layers. Note that back-propagation is nothing but the chain rule in calculus. Say $f(\\cdot)$ is an activation function in one layer and the output of that activation function is $\\bf y$ and the input is ${\\bf u}=W \\bf x$, where $\\bf x$ is output from the previous layer and mix with weights $W$ in current layer. Of course, the final loss $L$ will depend on ${\\bf y} = f({\\bf u})= f(W {\\bf x})$. Say, loss $L=g(\\bf y)$ somehow. To train the weights $W$, we have to find the gradient $\\frac{\\partial L}{\\partial W}$ so that we can adjust weight $W$ to minimize $L$. But $\\frac{\\partial L}{\\partial W}=\\frac{\\partial g({\\bf y})}{\\partial W}=\\frac{\\partial g({\\bf y})}{\\partial \\bf y}\\frac{\\partial {\\bf y}}{\\partial {\\bf u}}\\frac{\\partial {\\bf u}}{\\partial W}$. Each of these product terms can be computed locally and will be accumulatively multiplied as we apply backprop.  And note that the middle term $\\frac{\\partial {\\bf y}}{\\partial {\\bf u}}=\\frac{\\partial f({\\bf u})}{\\partial {\\bf u}}$ is just \"derivative\" of $f(\\cdot)$, thus we require the activation function to be differentiable and \"informative\"/non-zero (at least most of the time). Note that ReLU is not differentiable everywhere and that is why researchers (at least Yoshua Bengio) worried about that when they first tried to adopt ReLU. You may check out the interview of Bengio by Andrew Ng for that. \nQ. why step function is a bad activation function?\nAns: Note that step function is differentiable almost everywhere but is not \"informative\" though. For places (flat region) where it is differentiable, the derivative is simply zero. Consequently, any later layer gradient (information) will get cut off as it passes through a step function activation function.\n", "type": 2, "id": "11680", "date": "2019-04-05T19:19:34.603", "score": 2, "comment_count": 2, "parent_id": "11672"}, "22180": {"line": 14735, "body": "Differentiability for activation functions is desirable but not necessary since you can reformulate the derivatives at the non-differentiable points, like in the case of ReLU.\nProperties Needed:\n\nTo take advantage of Universal Approximation Theorem, and to take advantage of the modeling capacity it promises, the activation functions need to be continuous or Borel measurable (if this term is confusing just think of this as common functions), discriminatory (similarly discriminatory means the function doesn't just produce 0 all the time when integrating w.r.t. another function) and nonpolynomial (nonlinear).\nAlso with the recent research works it is beneficial to have it monotonically increasing.\n\nRecent theoretical analysis of the activation functions gives the edge to ReLU with some preliminary theoretical guarantees like this \"Convergence Analysis of Two-layer Neural Networks with ReLU Activation\" https://arxiv.org/abs/1705.09886.\n", "type": 2, "id": "22180", "date": "2020-06-25T20:22:04.743", "score": 0, "comment_count": 0, "parent_id": "11672"}}}
{"line": 8082, "body": "I wrote a convolutional neural network for the MNIST dataset with Numpy from scratch. I am currently trying to understand every part and calculation.\nBut one thing I noticed was the \"just positive\" derivative of the ReLU function.\nMy network structure is the following:\n\n(Input 28x28)\nConv Layer (filter count = 6, filter size = 3x3,  stride = 1)\nMax Pool Layer (Size 2x2) with RELU\nConv Layer (filter count = 6, filter size = 3x3,  stride = 1)\nMax Pool Layer (Size 2x2) with RELU\nDense (128)\nDense (10)\n\nI noticed, when looking at the gradients, that the ReLU derivative is always (as it should be) positive. But is it right that the filter weights are always decreasing their weights? Or is there any way they can increase their weight?\nWhenever I look at any of the filter's values, they decreased after training. Is that correct?\nBy the way, I am using stochastic gradient descent with a fixed learning rate for training.\n", "type": 1, "id": "11937", "date": "2019-04-22T09:09:17.073", "score": 0, "comment_count": 2, "tags": ["convolutional-neural-networks", "backpropagation", "relu", "convolution"], "title": "How should the values of the filters of a CNN change?", "answer_count": 2, "views": 698, "accepted_answer": "12040", "answers": {"11983": {"line": 8114, "body": "The weights of the filters do not always and necessarily decrease. Consider the extreme case when you initialise them to $-\\infty$ and you want to approximate a function different than the one the CNN represents initially with all weights set to $-\\infty$. You will have to increase one or more weights. \n", "type": 2, "id": "11983", "date": "2019-04-23T20:14:46.117", "score": 0, "comment_count": 0, "parent_id": "11937"}, "12040": {"line": 8163, "body": "All weight matrices in a Neural Network, adapt to map input and output. ReLU, as you pointed out, doesn't give negative derivatives. You're right. But notice the weight update equation in Backpropagation, it uses a multitude of parameters like:\n\nError value\nActivation from the previous layer\nWeight matrix of the previous layer\nThe pre-activation cache of the previous layer\n\nThese can be positive or negative. Hence weight updations dW and db can be +ve or -ve, allowing for decreasing (or increasing) value of weights.\n", "type": 2, "id": "12040", "date": "2019-04-28T13:13:01.497", "score": 0, "comment_count": 0, "parent_id": "11937"}}}
{"line": 6796, "body": "If one examines SSD: Single Shot MultiBox Detector code from GitHub repository, it can be seen that, for a testing phase (evaluating network on test data set), there is a parameter test batch size. It is not mentioned in the paper.\nI am not familiar with using batches during  network evaluation. Can someone explain what is the reason behind using it and what are advantages and disadvantages?\n", "type": 1, "id": "10201", "date": "2019-01-25T17:45:23.587", "score": 1, "comment_count": 1, "tags": ["neural-networks", "convolutional-neural-networks", "object-recognition"], "title": "Using batches in testing", "answer_count": 2, "views": 37, "accepted_answer": "10887", "answers": {"10227": {"line": 6816, "body": "Test batch time is referenced in the paper on page 33, Section 3.6, Inference Time and in Table 6. When an example set is used, a portion of it is used for training and another portion is used for testing. In mini-batch configurations the mini batch sizes are also independently configurable between train and test phases in many implementations.\n\nReferences\nSSD: Single Shot MultiBox Detector, 2016, Wei Liu\n", "type": 2, "id": "10227", "date": "2019-01-27T04:30:31.920", "score": 0, "comment_count": 1, "parent_id": "10201"}, "10887": {"line": 7290, "body": "\nI am not familiar with using batches during network evaluation. Can someone explain what is the reason behind using it and what are advantages and disadvantages?\n\nIt is usually just for memory use limitation vs speed of assessment. Larger batches evaluate faster on parallelised systems such as GPUs, but use more memory to process. Test results should be identical, with same size of dataset and same model, regardless of batch size.\nTypically you would set batch size at least high enough to take advantage of available hardware, and after that as high as you dare without taking the risk of getting memory errors. Generally there is less to gain than with training optimisation though, so it is not worth spending a huge amount of time optimising the batch size to each model you want to test. In most code I have seen, users pick a moderate \"safe\" value that will speed up testing but doesn't risk failing if you wanted to add a few layers to the model and check what that does.\n", "type": 2, "id": "10887", "date": "2019-02-26T07:37:24.623", "score": 0, "comment_count": 0, "parent_id": "10201"}}}
{"line": 8864, "body": "I am trying to implement a Neural Network for binary classification using python and numpy only. \nMy network structure is as follows:\ninput features: 2 [1X2] matrix\nHidden layer1: 5 neurons [2X5] matrix\nHidden layer2: 5 neurons [5X5] matrix\nOutput layer: 1 neuron [5X1]matrix\nI have used the sigmoid activation function in all the layers.\nNow lets say I use binary cross entropy as my loss function.\nHow do I do the back propagation on these matrices to update weights?\nclass Layer():\n    def __init__(self,number_of_neurons,number_of_inputs):\n        self.weights=np.random.rand(number_of_neurons, number_of_inputs)\n        self.bias=np.random.rand(number_of_neurons,1)     \n\n\nclass NeralNetwork():\n        def __init__(self, layer1, layer2,layer3):\n            self.layer1 = layer1\n            self.layer2 = layer2\n            self.layer3 = layer3\n\n        def sigmoid(self,x):\n            return 1 / (1 + np.exp(-x))\n\n        def derivative_sigmoid(self,x):\n            return x*(1-x)\n\n        def get_cost_value(self,Y_hat, Y):\n            m = Y_hat.shape[1]\n            cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n            return np.squeeze(cost)\n\n        def get_cost_derivative(self,Y_hat,Y):\n            return  - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n\n        def train(self,inputs,labels,epocs):\n            for epoc in range(1,epocs+1):\n                z1=np.dot(self.layer1.weights,inputs)+self.layer1.bias\n                a1=self.sigmoid(z1)\n                z2=np.dot(self.layer2.weights,a1)+self.layer2.bias\n                a2=self.sigmoid(z2)\n                #print(a2.shape)\n                z3=np.dot(self.layer3.weights,a2)+self.layer3.bias\n                a3=self.sigmoid(z3)\n                #print(a3.shape)\n                if epoc%100 is 0:\n                    print(a3)\n\n                cost=self.get_cost_value(a3,labels)\n                #print(cost)\n\n\n\n                layer3_delta=self.derivative_sigmoid(a3)*self.get_cost_derivative(a3,labels)\n                print(layer3_delta.shape)\n                Dw_layer3=np.dot(layer3_delta,a2.T)\n                Db_layer3=layer3_delta\n                #print(Dw_layer3.shape)\n                layer2_delta=np.dot(self.layer3.weights.T,layer3_delta)*self.derivative_sigmoid(a2)\n                #print(layer2_delta.shape)\n                Dw_layer2=np.dot(layer2_delta,a1.T)\n                Db_layer2=layer2_delta\n\n                layer1_delta=np.dot(self.layer2.weights.T,layer2_delta)*self.derivative_sigmoid(a1)\n                Dw_layer1=np.dot(layer1_delta,inputs.T)\n                Db_layer1=layer1_delta\n                #print(Dw_layer1)\n\n                self.layer1.weights-=((1/epoc)*Dw_layer1)\n                self.layer2.weights-=((1/epoc)*Dw_layer2)\n                self.layer3.weights-=((1/epoc)*Dw_layer3)\n\n                self.layer1.bias-=((1/epoc)*Db_layer1)\n                self.layer2.bias-=((1/epoc)*Db_layer2)\n                self.layer3.bias-=((1/epoc)*Db_layer3)\n\nSo far I have tried to implement this as shown above. But there seems to be a mistake because, after training, the network doesn't seem to have learned. Please let me know if you have any inputs.\n", "type": 1, "id": "12948", "date": "2019-06-19T19:49:11.070", "score": 0, "comment_count": 2, "tags": ["neural-networks", "backpropagation"], "title": "Back propagation on matrix of weights", "answer_count": 1, "views": 101, "accepted_answer": null, "answers": {"12951": {"line": 8867, "body": "The back propagation must be done in two steps.\n\nTransfer derivative\nGiven the output value, you must calculate its slope. This is simple as you are using the sigmoid activation function. So the derivative can be calculated as follows\n\n# Calculating slope\ndef transfer_derivative(output):\n    return output * (1.0 - output)\n\n\nBack propagate the error\nCalculate the errors for the various layers of the network and update the weights and bias.\n\n# Error calculation\n\n# For output layer\nerror = (expected - output) * transfer_derivative(output)\n\n# Hidden layers\nerror = (weight_k * error_j) * transfer_derivative(output)\n\nWhere error_j is the error signal from the jth neuron in the output layer, weight_k is the weight that connects the kth neuron to the current neuron and output is the output for the current neuron.\nAnd finally updating weights\nweight = weight + learning_rate * error * input\n\nHere is a detailed explanation of implementing neural networks from scratch using Python along with sample code. Give it a read.\n", "type": 2, "id": "12951", "date": "2019-06-19T21:19:06.847", "score": -1, "comment_count": 2, "parent_id": "12948"}}}
{"line": 9061, "body": "Let's say I have an adjustable loaded die, and I want to train a neural network to give me the probability of each face, depending on the settings of the loaded die.\nI can't mesure its performance on individual die roll, since it does not give me a probability.\nI could batch a lot of roll to calculate a probability and use this batch as an individual test case, but my problem does not allow this (let's say the settings are complex and randomized between each roll).\nI have 2 ideas:\n\ntrain it as a classification problem which output confidence, and hope that the confidence will reflect the actual probability. Sometimes the network would output the correct probability and fail the test, but on average it would tend to the correct probability. However it may require a lot of training and data.\nbatch random rolls together and compare the mean/median/standard deviation of the measured result vs the predictions. It could work but I don't know the good batch size.\n\nThank you.\n", "type": 1, "id": "13189", "date": "2019-07-03T15:41:03.197", "score": 2, "comment_count": 3, "tags": ["neural-networks", "machine-learning"], "title": "How can I train a neural network to give probability of a random event?", "answer_count": 3, "views": 409, "accepted_answer": null, "answers": {"13190": {"line": 9062, "body": "Neural network isnt what you want here. You have a limited number of events and draws from some unknown distribution that you want to recover.  \nIn that case, just use the empirical probabilities $p(event_i) = \\frac{\\# event_i}{total\\ events}$ which given enough draws will converge to the true probability. \n", "type": 2, "id": "13190", "date": "2019-07-03T17:48:38.393", "score": 0, "comment_count": 2, "parent_id": "13189"}, "13200": {"line": 9070, "body": "This approach:\n\ntrain it as a classification problem which output confidence, and hope that the confidence will reflect the actual probability. Sometimes the network would output the correct probability and fail the test, but on average it would tend to the correct probability.\n\nwill work with some limitations. If you use a classifier with softmax activation and multiclass log loss\n$$\\mathcal{L}(\\mathbf{\\hat{y}},\\mathbf{y}) = -\\mathbf{y} \\cdot  \\text{log}(\\mathbf{\\hat{y}})$$\nwhere $\\mathbf{\\hat{y}}$ is the network output as a vector, and $\\mathbf{y}$ is the actual output from an individual sample. Your input should be the settings of the die. \nOptimising this loss will converge on approximated probabilities for each discrete output. You can demonstrate this with some simple examples - for instance if you train a network with a single input - a one-hot-encoded die type from the classic D&D dice sets, plus deliberately chosen examples of different results in the right frequencies, you will end up with a classifier that predicts roughly $p=0.25$ for results of 1,2,3,4 for a d4 and $p=0.125$ for results of 1,2,3,4,5,6,7,8 for a d8\nSo it works mathematically. Whether it works for your situation depends on details. You need enough data samples to cover both the distribution of results under each setting, and any complexities of how that distribution varies with the settings. In the limit of wanting very accurate predictions of probability within a complex space you will need a huge number of samples. You should be able to find a compromise between accuracy and generalisation by trying different levels of regularisation - this will be necessary as over-fitting to input/output sample pairs as seen is going to be a serious problem for a neural network trained on this data. \nOne thing you can do to help a classifier learn probabilities is always take some number of samples with the same settings - e.g. 10 or 100 or 1000 each with the same settings - that should guarantee that the network cannot simply converge to predict high $p$ values for single outputs as seen, as it will have counter-examples to work with.\nYou mention that you have 40 dimensions of settings. Whether this is an issue will depend on how the probability distribution varies based on those settings. However, at minimum you should be thinking in terms of millions of samples for training, or possibly a fast on-demand generator that can generate 1000s of new samples per second to train with.\nYou can test accuracy by building histograms using some fixed (and as yet unseen) setting and comparing to NN predictions of probabilities of that setting. Even getting accurate test results is likely to require 1000s of samples.\n\nHowever it may require a lot of training and data.\n\nIf you cannot obtain a very large training set here, then a purely statistical \"black box\" approach is probably not feasible, regardless of whether you use neural networks, or more raw analysis. What neural networks add is smooth interpolation between different settings values, as a form of approximation. This seems desirable for your problem, as you will never fully explore 40 dimensions of 100 values in the lifetime of the universe - but you need some confidence that minor changes in settings equate to minor changes in probability distributions in most cases. \nIt's OK to have one or two major shifts across the input space, but if the distribution depends in some cryptographic primitive or similar complex high frequency (over space) and high amplitude on the input variables, there is no way to obtain approximations using statistics.\nThe alternative to statistical approaches is to find some way to break open this black box through analysis. No AI system can do that in general at the moment, so you would rely on human ingenuity.\n", "type": 2, "id": "13200", "date": "2019-07-04T08:12:46.533", "score": 0, "comment_count": 3, "parent_id": "13189"}, "13763": {"line": 9520, "body": "The starting point is that for a fair dice thrown fairly the p(n) is 1/n where n is the number of sides.\nYou said both\nand\n\nthere are too many variables (up 40 dimensions with value range 1-100) in input, I don't know how these properties relate and an empirical approach would require too much data.\n\nIt seems that this problem has 2 solutions:\n\nDon't use a neural net and create a 'std' statistical model.\nIt may be possible since you said:\n\n\nI know there is some underlying rule that simplify a lot the problem (ie. reduce the actual number of dimension of the input)\n\n\nUse a neural network (with softmax at the end) - for a fair dice; with enough training data the classifier should arrive as 1/n as the approximating function for a fair dice. The other 40 dimensions/settings your mentioned are the inputs. I think a 'basic' neural network with dense layers only could work for your task.\n\n", "type": 2, "id": "13763", "date": "2019-08-03T21:52:45.590", "score": 1, "comment_count": 0, "parent_id": "13189"}}}
{"line": 8670, "body": "I was reading the following book: http://neuralnetworksanddeeplearning.com/chap2.html\nand towards the end of equation 29, there is a paragraph that explains this: \n\nHowever I am unsure how the equation below is derived:\n\n", "type": 1, "id": "12682", "date": "2019-06-04T13:42:58.250", "score": 3, "comment_count": 10, "tags": ["neural-networks", "backpropagation"], "title": "How does adding a small change to an neuron's weighted input affect the overall cost?", "answer_count": 3, "views": 89, "accepted_answer": null, "answers": {"12687": {"line": 8674, "body": "The derivative of a function ($f(x_1,x_2..x_n)$) w.r.t to one of the variables ($x_1,x_2..x_n$) gives us the rate of change of the function w.r.t the rate of change of the variable. This roughly means that by how much will the function value change if we change the variable by a \"unit amount\" or $+1$. (we cannot use the change as $+1$ as the change needs to be infinitesimally small, this is just a rough explanation) \nThe image shows a tangent line to a curve or function $f(x)$. The slope of this tangent is given by $\\frac{df(x)}{dx}$ at that particular $x$. If you move by a very very small amount in the direction of positive $x$ i.e. $x+\\delta x$ the change in the value of the $f(x)= y $ will almost be the same as the change in the value of the $y$ of the tangent line.\n\nNow as per the excerpt the cost function $C$ is a function of $z_j^l$. Thus, it can be written as $$C = f(z_j^l, .....).$$ So, $$\\frac{\\partial C}{\\partial z_j^l}$$ indicates how much $C$ will vary w.r.t $z_j^l$, i.e. when $z_j^l$ is changed by an infinitesimally small amount and thus the formulae:\n$$\\frac{\\partial C}{\\partial z_j^l} \\Delta z_j^l$$ where the author assumed $\\Delta z_j^l$ to be very small. This gives the infinitesimal change in $C$ or gives us $\\Delta C$, for infinitesimally small change in $z_j^l$ or any varible affecting the cost function. This can be derived by series expansions too (given below), but this is an intuitive explanation.\nAn explanation can be given from Taylor Series Theorem which states: :\n\nLet $f(x)$ be a function which is analytic at $x = a$. Then we can write\n  $f(x)$ as the following power series, called the Taylor series of $f(x)$\n  at $x = a$, then we can write $f(x)$ as:\n\n$$f(x) = f(a) + f'(a)(x-a) + f''(a)\\frac{(x-a)^2}{2!} + f'''(a)\\frac{(x-a)^3}{3!}... $$\nNow if we keep other variables constant and make cost function $f$ vary only with $z^l_j$\nand if we put $a=z^l_j$ and $x=z^l_j + \\Delta z^l_j$ the equation becomes:\n$$f(z^l_j + \\Delta z^l_j) = f(z^l_j) + f'(z^l_j)(\\Delta z^l_j) + f''(z^l_j)\\frac{(\\Delta z^l_j)^2}{2!} + f'''(a)\\frac{(\\Delta z^l_j)^3}{3!}... $$\nwhich if we ignore the higher order terms of  $\\Delta z^l_j$, since terms containing $\\Delta z^l_j$ for powers greater than 1 will be negligible compared to $\\Delta z^l_j$ with power 1. Thus the equation now effectively is:\n$$f(z^l_j + \\Delta z^l_j) = f(z^l_j) + f'(z^l_j)(\\Delta z^l_j)$$\n$$f(z^l_j + \\Delta z^l_j) - f(z^l_j)=   f'(z^l_j)(\\Delta z^l_j)$$\n$$f(z^l_j + \\Delta z^l_j) - f(z^l_j)=   \\frac{\\partial f(z^l_j)}{\\partial z^l_j}(\\Delta z^l_j)$$ where $$f(z^l_j + \\Delta z^l_j) - f(z^l_j)$$ can be thought of as $\\Delta C$ or the change in cost function for small change in $z^l_j$\nNOTE: I have glossed over some requirements for a Taylor Series to be convergent.\n", "type": 2, "id": "12687", "date": "2019-06-04T16:05:29.357", "score": 0, "comment_count": 12, "parent_id": "12682"}, "12685": {"line": 8672, "body": "I think that Nielsen just wanted to convey the idea of the back-propagation algorithm using that formula, as you can read from the next paragraph \"Now, this demon is a good demon...\", so I don't think that that partial derivative is mathematically correct, provided the partial derivative is still with respect to $z_j^l$.\n$C$ is the cost (or loss) function. $z_j^l$ is the linear output of neuron $j$ in layer $l$, which is followed by a non-linear function (e.g. sigmoid), denoted by $\\sigma$. So, the actual output of neuron $j$ in layer $l$ is $\\sigma(z_j^l)$. \nThe partial derivative of the cost function $C$ with respect to this neuron's linear output, $z_j^l$, is $$\\frac{\\partial C}{\\partial z_j^l} = \\frac{\\partial C}{\\partial z_j^l} 1 = \\frac{\\partial C}{\\partial z_j^l} \\frac{\\partial z_j^l}{\\partial z_j^l}.$$\nIf the the linear output of node $j$ in layer $l$ is now $z_j^l + \\Delta z_j^l$, then the partial derivative with respect to $z_j^l$ becomes\n\\begin{align}\n\\frac{\\partial C}{\\partial z_j^l} \n&= \\frac{\\partial C}{\\partial (z_j^l + \\Delta z_j^l)}\\frac{\\partial (z_j^l + \\Delta z_j^l)}{\\partial z_j^l}  \\\\\n&= \\frac{\\partial C}{\\partial (z_j^l + \\Delta z_j^l)} \\left( \\frac{\\partial z_j^l}{\\partial z_j^l} +  \\frac{\\partial \\Delta z_j^l}{\\partial z_j^l} \\right) \\\\\n&= \\frac{\\partial C}{\\partial (z_j^l + \\Delta z_j^l)} \\left( 1 +  \\frac{\\partial \\Delta z_j^l}{\\partial z_j^l} \\right) \\\\\n&= \\frac{\\partial C}{\\partial (z_j^l + \\Delta z_j^l)} +  \\frac{\\partial C}{\\partial (z_j^l + \\Delta z_j^l)}\\frac{\\partial \\Delta z_j^l}{\\partial z_j^l} \\\\\n\\end{align}\n$\\Delta z_j^l$ depends on $z_j^l$, but it is not specified how.\n", "type": 2, "id": "12685", "date": "2019-06-04T15:39:18.310", "score": 0, "comment_count": 0, "parent_id": "12682"}, "12686": {"line": 8673, "body": "I believe he's just saying that:\n$$\n\\frac{\\partial C}{\\partial z_j^l} \\Delta z_j^l \\approx \\frac{\\partial C}{\\partial z_j^l} \\partial z_j^l \\approx \\partial C\n$$\nso that the change in cost function can be arrived at simply for a small enough perturbation $\\Delta z_j^l$.\nOr, taking that line of approximations backwards, the change in the cost function for a given perturbation is just:\n$$\n\\partial C \\approx \\frac{\\partial C}{\\partial z_j^l} \\partial z_j^l \\approx  \\frac{\\partial C}{\\partial z_j^l} \\Delta z_j^l\n$$\n", "type": 2, "id": "12686", "date": "2019-06-04T16:01:08.137", "score": 1, "comment_count": 8, "parent_id": "12682"}}}
{"line": 8931, "body": "Actually, I am \"fresh-water\", and I've never known what is neural network. Now I am trying understand how to design simple neuronetwork for this problem:\nI'd like to make up such neural network that after learning it could predicate a mark of passed exam (for example, math). There is such factors that influence on a mark:\n\nChosen topic (integral, derivative, series)\nPerfomance (low, medium, high)\nDoes a student work? (Yes, No, flexible schedule)\nHave a student ever gotten through a add course? (Yes, No)\n\nThe output is a mark (A,B,C,D,E,F)\nI don't know should I add few layers between inputs and output\n\nMoreover, I have few results from past years:\n\n(integral, low, Yes, No, E) \n(integral, medium, Yes, Yes, B)\n(series, high, No, Yes, A)\nand so on. What do I need to know else for designing this NN?\n\n", "type": 1, "id": "13031", "date": "2019-06-24T13:51:43.990", "score": 1, "comment_count": 3, "tags": ["neural-networks"], "title": "How to create neural network that predicates result of exam?", "answer_count": 2, "views": 93, "accepted_answer": "13177", "answers": {"13177": {"line": 9051, "body": "You will need one of two things or both.\n\nA theoretical model of grade probability distribution already verified against data that contains the features listed\nA data set mapping the features listed to grades (labels) for training an artificial network\n\nThe features can be specified more formally\n\nChosen topic --- from a finite list of topics, encoded by integer (numbered strings)\nPerformance --- low, medium, or high --- it is not clear from where this judgment comes, but that would need to be clarified and also included in either the verified theoretical model or among the features of the training data\nDoes a student work? --- hours per week --- note that the flexibility of the schedule is not as critical as the hours of the 168 hours of each week not available for sleep, self-care, class time, or study\nHave a student ever gotten through a add course? --- binary --- this question is unclear in its current phrasing\nGrade --- converted to numeric value from 0.0 to 4.0, which can be represented as an integer from 00 to 40\n\nThe difficult part will be finding either the verified theoretical model or the data set for training or both.\nIf a verified theoretical model is found, apply a gradient descent implementation to tune that model, which is a statistics and conversion problem only partly related to AI and machine learning. Otherwise using a model free artificial network will likely allow you to reach your objective. A simple feed forward network with gradient descent and back-propagation and three layers should suffice. \nThere are many examples in Python, Java, and other languages available online, usually in conjunction with a library that the examples leverage for computation and instructions on how to install the software. There are other questions and answers in this forum that explain what books to buy to get started. In fact, there is a tag specifically for that purpose: https://ai.stackexchange.com/questions/tagged/getting-started.\n", "type": 2, "id": "13177", "date": "2019-07-03T03:23:12.037", "score": 0, "comment_count": 0, "parent_id": "13031"}, "13042": {"line": 8940, "body": "You can add as many layers (with any arbitrary number of nodes) as you want.\nPlease note that as you add more learning parameters (layers and nodes), your model complexity increases. This means the model can potentially learn a more complex input-output relationship. However, it also increases the risk of overfitting. Overfitting generally happens when the model you build is more complex than the data you have. In such a scenario, the model memorizes the data instead of learning from it. In other words, it can produce a very good result for the same data as it was trained on but cannot generalize well. So, it performs poorly when the inputs are slightly different from what is used to be fed at the training stage.\nIn practice, you may try different architectures and parameter configurations, and measure the generalization capacity of the models (via cross-validation for example) to choose the best model. In English, a generalizable model is the one that performs (almost) equally good on both training/validation and testing sets.\n", "type": 2, "id": "13042", "date": "2019-06-25T01:57:31.333", "score": 0, "comment_count": 0, "parent_id": "13031"}}}
{"line": 8572, "body": "When training a neural network, we often run into the issue of overfitting.\nHowever, is it possible to put overfitting to use? Basically, my idea is, instead of storing a large dataset in a database, you can just train a neural network on the entire dataset until it overfits as much as possible, then retrieve data \"stored\" in the neural network like it's a hashing function.\n", "type": 1, "id": "12565", "date": "2019-05-28T09:45:44.620", "score": 5, "comment_count": 0, "tags": ["neural-networks", "overfitting"], "title": "Is it possible for a neural network to be used to compress data?", "answer_count": 4, "views": 590, "accepted_answer": null, "answers": {"12567": {"line": 8574, "body": "Train a network that has large input and small output. Turn it upside down (yes, you may do that). By giving the small outputs, corresponging to input, the ideally- trained network will generate those large data. \nBut you see in all compression there will be data lost, so generated data will be slightly :DD different then original dataset. So its suitable for statistical data, like images, whatever, but not for structured like text or the most unsuitable example - program source code.\n", "type": 2, "id": "12567", "date": "2019-05-28T13:42:37.613", "score": 1, "comment_count": 0, "parent_id": "12565"}, "12566": {"line": 8573, "body": "The auto-encoder (AE) can be used to learn a compressed representation (a vectorised hash value) of each observation in the training dataset, $z$, which can then be used to later retrieve the original (or similar) observation.\nThe variational auto-encoder (VAE), a statistical variation of AE, can also be used to generate objects similar to the observations (or inputs) in the training set.\nThere are other data compressor models, for example, Helmholtz machine, which precedes both the AE and VAE.\n", "type": 2, "id": "12566", "date": "2019-05-28T10:26:27.470", "score": 6, "comment_count": 0, "parent_id": "12565"}, "12716": {"line": 8699, "body": "An Auto-Encoder is probably what you are looking for. AE is a very powerful Neural Network when you want to compress data and get a lower dimensional representation of the data with maximum information retained. \nIf you are interested to know how it works -- Think of it as training a NN to predict the data itself. Which means your input and output layers would exactly be the same. So how does this help in compression ? The Hidden Layer -- Here is where the magic happens. The compression comes from the fact that we use lesser number of neurons in the hidden layer than in the i/p layer. Assuming that the NN we build nicely predicts the i/p at the end of the training,  the output of the hidden layers can be thought of as newly engineered features which are less in dimension yet powerful enough to represent the information in original data. \n", "type": 2, "id": "12716", "date": "2019-06-06T08:16:14.677", "score": 1, "comment_count": 0, "parent_id": "12565"}, "12719": {"line": 8701, "body": "See What is the difference between encoders and auto-encoders? For a working example of a neural network being used to compress (and decompress) images.\n", "type": 2, "id": "12719", "date": "2019-06-06T11:01:52.903", "score": 0, "comment_count": 0, "parent_id": "12565"}}}
{"line": 8099, "body": "Could changing the order of convolution layers in a CNN improve accuracy or training time?\n", "type": 1, "id": "11966", "date": "2019-04-23T10:11:10.767", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "convolutional-neural-networks"], "title": "Does changing the order of the convolution layers in a CNN have any impact?", "answer_count": 1, "views": 103, "accepted_answer": "11972", "answers": {"11972": {"line": 8104, "body": "Conventionally, CNN layers downsample over and over, which enables them to capture details at different levels of abstractions. Usually, it is observed that the initial layers do nothing more than detecting edges, or filtering color channels; the combinations of these edges are what we perceive as 'features'.\nIf you reverse the order, you essentially are changing sampling modes down the line.\nCNNs detect by 'downsampling' the inputs and therefore 'extracting' features.\nIt may not work as expected!\n", "type": 2, "id": "11972", "date": "2019-04-23T15:18:40.130", "score": 0, "comment_count": 1, "parent_id": "11966"}}}
{"line": 8728, "body": "From the answers to this question In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?, I got the fact that each filter has different weights for each input channel. But why should that be the case? What if we apply the same weights to each input channel? Does it work or not?\n", "type": 1, "id": "12755", "date": "2019-06-09T10:15:23.850", "score": 1, "comment_count": 0, "tags": ["machine-learning", "convolutional-neural-networks", "weights"], "title": "Why should each filter have different weights for each input channel?", "answer_count": 1, "views": 207, "accepted_answer": null, "answers": {"12756": {"line": 8729, "body": "For simplicitly, let's consider only the first convolutional layer, that is, the one applied to the image. If you consider an RGB image, then there are $3$ channels: the red channel, the green channel and the blue channel. Thus, a kernel that is applied to this image will also have $3$ channels: the red channel, the green channel and the blue channel. In general, the distributions of the intensity of the red, green and blue colors in the image are different, so, in general, the red, green and blue channels of the kernel will also be different because they need to keep track of different information.\n", "type": 2, "id": "12756", "date": "2019-06-09T14:00:05.123", "score": 0, "comment_count": 3, "parent_id": "12755"}}}
{"line": 8736, "body": "Here's the famous VGG-16 model.\n\nDo the inputs and outputs of a convolutional layer, before pooling, usually have the same depth?  What's the reason for that?\nIs there a theory or paper trying to explain this kind of setting?\n", "type": 1, "id": "12768", "date": "2019-06-10T09:08:59.117", "score": 2, "comment_count": 1, "tags": ["neural-networks", "convolutional-neural-networks", "convolution", "convolutional-layers"], "title": "Why do the inputs and outputs of a convolutional layer usually have the same depth?", "answer_count": 1, "views": 166, "accepted_answer": null, "answers": {"12781": {"line": 8747, "body": "Keeping the same channel size allows the model to maintain rank but i would say the main reason is convenience. Its easier book keeping.\nAlso in many model cases output features need some form of alignment with the input (example being all models using residual units -- $\\hat{x} = F(x) + x$ \n", "type": 2, "id": "12781", "date": "2019-06-10T16:14:20.947", "score": -1, "comment_count": 2, "parent_id": "12768"}}}
{"line": 9662, "body": "Consider an MLP that outputs an integer 'rating' of 0 to 4. Would it be correct to say this could be modeled in either of the following ways:\n\nmap each rating in the dataset to a 'normalized set' between 0 and 1 (i.e. 0, 0.25, 0.5, 0.75, 1), have a single neuron with sigmoid activation at output provide a single decimal value and then take as the rating whatever is closest to that value in the 'normalized set'\nhave 5 output neurons with a softmax activation function output 5 values, each representing a probability of one of the 5 ratings as the outcome, and then take as the rating whichever neuron gives the highest probability?\n\nIf this is indeed the case, how does one typically decide 'which way to go'? Approach 1 certainly appears to yield a simpler model. What are the considerations, pros/cons of each approach? Perhaps a couple of concrete examples to illustrate?\n", "type": 1, "id": "13944", "date": "2019-08-13T11:51:01.170", "score": 2, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "ai-design", "multilayer-perceptrons"], "title": "One vs multiple output neurons", "answer_count": 3, "views": 312, "accepted_answer": null, "answers": {"13949": {"line": 9666, "body": "This depends on whether the output is a continuous or discrete variable. If the output variable is discrete (there are a finite number of possibilities that it can be), as in a classification task (such as this one, where you are trying to place the input into one of 5 categories), you want to use one output neuron for each class. If the variable is continuous, however, you should only use one output neuron. \nThis is because of how the training process works. When training your network successively makes adjustments to try and reduce the errors. These adjustments are made in the direction of the error - so if the network predicts a value which is too high then the network's weights are adjusted to make the output value lower. On the other hand if the network's predicts a value which is too low the network's weights are adjusted to make the output bigger.\nIf you have output neurons labeled 0 to 4 and a training sample with some input value and a target prediction of 2 then the neural network will make its prediction. Once the prediction has made each neuron is adjusted individually - in this case neuron 2 will be adjusted in the direction of the correct probability and all the other neurons will be adjusted in the direction of the incorrect probability. In this way you have one prediction for each class. \nBackpropagation is a about error attribution, and using multiple neurons allows the error of the neural network to be better attributed as the neural network can adjust each neuron individually, and thus adjust the required probabilities for each class. \nUsing a single neuron with a sigmoid activation function would be less good as the sigmoid function saturates values close to 0 and 1 so there would be an unnatural bias towards category 0 and category 4 over the other categories. The neural network could learn to overcome this, but it would take more time.\n", "type": 2, "id": "13949", "date": "2019-08-13T15:52:40.207", "score": 1, "comment_count": 2, "parent_id": "13944"}, "14064": {"line": 9765, "body": "A somewhat large set of designs and set-ups can be made to learn a rating function for a given set of labeled examples.  If the objectives are simplicity and effectiveness (accuracy, reliability, and speed), then a third option should be considered.\nThe requirement in the question includes, \"Outputs an integer rating 0 [through] 4 [inclusive].\"  For such a discrete result, the number of required output bits $b$ (where $s$ is the number of possible states and $I$ is the set of integers) is given as follows.\n$$\\min_b \\, (b \\in I \\; \\land \\; b \\ge \\log_2 s)$$\nIn this case, we require three bits of output.\n$$s = 5 \\quad \\implies \\quad b = 3$$\nNote that with similar configuration ratings of 0 through 7 would also require only three bits of output.  Either way, the output layer would likely be simplest and most efficient if its activation function was binary step function.  This removes the need for rounding after it is applied.  The output layer would then provide a binary value indicating rating.  The goal of learning would be to reduce the error between the feed forward output and the associated the binary value of the label for each example.\nPrevious layer(s) could be sigmoid or a more contemporary and less problematic continuous activation function like ISRLU.\nSince the engineer can select the error function used by the learning framework to accept any input range and distribution, normalizing the labels for supervised learning is primarily employed to remove redundancy from time and resource consuming operations required to compute error.  With ratings as the labels, unless the distribution of ratings is skewed and the data set is such that learning time is excessive, normalization may not be necessary.  If it is, it would likely be because improving the label distribution in advance (requiring floating point input to the error function and removing skew) would reduce learning time.\nThe other two approaches introduce unnecessary complexities mentioned in context above.  A consequence of removing complexities without adding impediments to convergence is more efficiency during learning and during execution after learning.\n", "type": 2, "id": "14064", "date": "2019-08-20T12:28:34.353", "score": 0, "comment_count": 0, "parent_id": "13944"}, "13967": {"line": 9682, "body": "In the case of one output neuron, you don't have to use sigmoid. As Teymour Aldridge suggested, it would cause a tendency to output 0 or 1. What I normally do is that I set the layer before the output layer to sigmoid of tanh so it won't output ridiculously off numbers, and set the output layer to linear. There would be cases that it outputs something like 1.5, but over time that disappears.\nHope it helps :)\n", "type": 2, "id": "13967", "date": "2019-08-14T08:14:56.780", "score": 0, "comment_count": 1, "parent_id": "13944"}}}
{"line": 7502, "body": "What is geometric deep learning (GDL)?\nHere are a few sub-questions\n\nHow is it different from deep learning?\nWhy do we need GDL?\nWhat are some applications of GDL?\n\n", "type": 1, "id": "11166", "date": "2019-03-12T10:29:49.193", "score": 17, "comment_count": 1, "tags": ["deep-learning", "definitions", "geometric-deep-learning", "graphs", "graph-neural-networks"], "title": "What is geometric deep learning?", "answer_count": 3, "views": 3858, "accepted_answer": null, "answers": {"11201": {"line": 7527, "body": "The article Geometric deep learning: going beyond Euclidean data (by Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst) provides an overview of this relatively new sub-field of deep learning. It answers all the questions asked above (and more). If you are familiar with deep learning, graphs, linear algebra and calculus, you should be able to follow this article.\nWhat is geometric deep learning (GDL)?\nThis article describes GDL as follows\n\nGeometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds.\n\nSo, the inputs to these GDL models are graphs (or representations of graphs), or, in general, any non-Euclidean data. To be more concrete, the input to these models (e.g. graph neural networks) are e.g. feature vectors associated with the nodes of the graphs and matrices which describe the graph structure (e.g. the adjacency matrix of the graphs).\nWhy are e.g. graphs non-Euclidean data?\nA graph is a non-Euclidean structure because e.g. distances between nodes are not well defined. Yes, you can have graphs with weights associated with the edges, but not all graphs have this property.\nWhat classes of problems does GDL address?\nIn GDL, there are two classes of problems that are often tackled:\n\ncharacterise the structure of the data (e.g. of a graph)\nanalyse functions defined on a given non-Euclidean domain\n\nThese classes of problems are related, given that the structure of the graph imposes certain properties on the functions that can be defined on it. Furthermore, these properties of these functions can also convey information about the structure of the graph.\nWhat are applications of GDL?\nAn example of an application where this type of data (graphs) arises is in the context of social networks, where each user can be associated with a vertex of the social graph and the characteristics (or features) of each user (e.g. number of friends) can be represented as a feature vector (which can then be associated with the corresponding vertex of a graph). In this context, the goal might e.g. be to determine different groups of users in the social network (i.e. clustering).\nWhy can't we simply use deep learning methods (like CNNs) when the data is non-Euclidean?\nThere are several problems that arise when dealing with non-Euclidean data. For example, operations like convolution are not (usually) defined on non-Euclidean data. More concretely, the relative position of nodes is not defined on graphs (but this would be required to perform the usual convolution operation): in other words, it is meaningless to talk about a vertex that is e.g. on the left of another vertex. In practice, it means that we can't simply use the usual CNN when we are given non-Euclidean data. There have been attempts to generalise the convolution operation to graphs (or to approximate it). The field is still quite new, so there will certainly be new developments and breakthroughs.\n", "type": 2, "id": "11201", "date": "2019-03-13T17:39:23.130", "score": 8, "comment_count": 3, "parent_id": "11166"}, "11252": {"line": 7566, "body": "To complete the first answer that is rather graph oriented, I will write a little about deep learning on manifolds, which is quite general in terms of GDL thanks to the nature of manifolds. \nNote that the description of GDL through the explanation of what are DL on graphs and manifolds, in opposition to DL on euclidean domains, comes from the 2017 paper Geometric deep learning: going beyond Euclidean data (this paper is excellent at clarifying both the intuition and the mathematics of what I'm writing).\n1. In case you don't know what a manifold is\nAs the previously cited paper puts it:\n\nRoughly, a manifold is a space that is locally Euclidean. One of the\n  simplest examples is a spherical surface modeling our planet: around a\n  point, it seems to be planar, which has led generations of people to\n  believe in the flatness of the Earth. Formally speaking, a\n  (differentiable) d-dimensional manifold X is a topological space where\n  each point x has a neighborhood that is topologically equivalent\n  (homeomorphic) to a d-dimensional Euclidean space, called the tangent\n  space.\n\nGood other not-so-technical explanation on stats.stackexchange\nOther Wikipedia examples to develop not too abstract understanding\nVery shortly put, it's an interesting mathematical set on which to work (different kinds exist, see papers at the end of this answer for DL related manifolds uses). By work, you can typically understand that you constrain the neural net parameters to the manifold you chose (e.g. training with parameters constrained on a hypersphere, among the geomstats paper examples).\nYour data can also be represented thanks to a practical manifold. For example, you can choose to work on images and videos by representing the samples using Symmetric Positive Definite (SPD) matrices (see this paper), the space of SPD matrices being a manifold itself.\n2. Why bother learning on manifolds ?\nDefining a clearer/better adapted set (understand that it's a sort of constraint!) on which to learn parameters and features can make it simpler to formally understand what your model is doing, and can lead to better results. I see it as a part of the effort of deep learning formalization. One could say you're looking for the best information geometry for your task, the one that best captures the desirable data distribution properties.To develop this intuition consider the solar system analogy for manifold learning of this Kaggle kernel:\n\nPerhaps a good analogy here is that of a solar system: the surface of our planets are the manifolds we're interested in, one for each digit. Now say you're on the surface of the earth which is a 2-manifold and you start moving in a random direction (let's assume gravity doesn't exist and you can go through solid objects). If you don't understand the structure of earth you'll quickly find yourself in space or inside the earth. But if you instead move within the local earth (say spherical) coordinates you will stay on the surface and get to see all the cool stuff.\n\nThis analogy reminds us of the spherical surface planet model from Bronstein's paper already quoted above. This paper also describes a typical case for which manifolds are interesting: where graphs (the other example of GDL/DL on non euclidean data) are better at handling data from social or sensor networks, manifolds are good at modeling 3D objects endowed with properties like color texture in computer vision.\n3. Regarding deep neural networks on manifolds\nI would advise reading the geomstats associated paper, which does a great job at showing what it is and how it can be used, along with example codes (e.g. MNIST on hyperspheres manifold example code here). This library implements manifolds and associated metrics on Keras. The choice of metrics is essential to understand the point of working on manifolds: it's because you need to work on an adapted mathematical set (ie with the right properties) with an adapted distance definition (so that the measure actually means something when considering the problem you're trying to solve) that you switch to working on manifolds.\nIf you want to dive in the details and examples of deep learning on manifolds here are some papers:\n\nA Riemannian Network for SPD Matrix Learning: new backpropagation to learn SPD matrices on Riemannian manifolds\nLearning a Robust Representation via a Deep Network on Symmetric Positive Definite Manifolds: using SPD matrices to aggregate convolutional features\n\n4. Why Riemannian manifolds ?\nTL;DR: you need a metric to do machine learning (otherwise, how could you evaluate how much you actually learned !)\nStill based on Bronstein's paper:\n\nOn each tangent space, we define an inner product [...]. This inner\n  product is called a Riemannian metric in differential geometry and\n  allows performing local measurements of angles, distances, and\n  volumes. A manifold equipped with a metric is called a Riemannian\n  manifold.\n\n5. What's the relation between a Riemannian manifold and a Euclidean space ?\nStill based on Bronstein's paper:\n\na Riemannian manifold can be realized as a subset of a Euclidean space\n  (in which case it is said to be embedded in that space) by using the\n  structure of the Euclidean space to induce a Riemannian metric.\n\nI leave the details to the paper, otherwise this answer will never end.\n6. Answers to questions in comments\nWill only answer once I think I've found a relatively well-argued answer, so won't answer everything at once.\n\nIsn't manifold learning just a way of dimensionality reduction? \n\nI don't think so, it isn't just that. I haven't seen any dimensional reduction constraint (yet ?) in the papers I've read (cf. geomstats again). \nIn the hypersphere/MNIST geomstats code example, you can see the chosen manifold dimension hypersphere_dimension = 17. Since we're working with MNIST data I guess this would mean a dimension reduction in this particular case. I admit I would need to check exactly what that dimension implies on the neural net architecture, I haven't discussed my understanding of this yet.\nDisclaimer\nI'm still developing a more rigorous mathematical understanding of manifolds, and shall update this post to make additional necessary clarifications: exactly what can be considered as a manifold in a traditional deep learning context, why do we use the word manifold when speaking about the hidden state of auto-encoders (see the previously cited Kaggle kernel that quotes Goodfellow's book on this). All of this if the perfectly clear answer doesn't show up here before !\n", "type": 2, "id": "11252", "date": "2019-03-15T09:15:07.867", "score": 18, "comment_count": 8, "parent_id": "11166"}, "30369": {"line": 20429, "body": "I didn't read the paper in depth, but one example of where assumptions of Euclidean space are made in the design of the networks are with ConvNets in image processing.\nSpecifically, Euclidean spaces are transformationally invariant, meaning that $d(a,b) = d(a+c,b+c)$. Each convolution layer iterates over the image with a certain amount of stride, which guarantees a certain amount of translational invariance in exchange for a smaller size of network parameters.\nConvNets would not naively work for a dataset which comes from, say, a graph, because its not clear how to stride over it, as a possible example.\n", "type": 2, "id": "30369", "date": "2021-08-25T15:45:22.720", "score": 0, "comment_count": 1, "parent_id": "11166"}}}
{"line": 8107, "body": "What is an artificial neural network in artificial intelligence?\nIt is apparently used to find patterns in data and it is loosely inspired by human neural networks.\n", "type": 1, "id": "11976", "date": "2019-04-23T16:50:10.383", "score": 4, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "ai-basics", "terminology", "definitions"], "title": "What is an artificial neural network?", "answer_count": 5, "views": 294, "accepted_answer": null, "answers": {"11978": {"line": 8109, "body": "An (artificial) neural network (NN) can be described at different levels of detail, but, essentially, an NN is a data structure (or model), which behaves like a function: that is, it can receive one or more inputs and it produces one or more outputs. \nMost of the NNs can be thought of as graph-like data structures that contain nodes which are connected by weighted edges, where some of the nodes are associated with the inputs and some of the nodes are associated with the outputs. The remaining nodes are often called \"hidden nodes\". However, there are NNs which might not be easily understood as a graph-like structure: for example, convolutional neural networks. \nThe number of nodes and the connections between them defines the architecture of the NN and, intuitively, its \"expressivity\", that is, the number or type of functions it can approximate. Each node in an NN often computes a \"function\" of its inputs: for example, it could sum the inputs.\nThe weights of the edges connecting the nodes can change, during a \"learning or training phase\", so that the NN produces the desired outputs given certain inputs. For example, we can train an NN to \"approximate\" the function $f(x) = x$. Thus, an NN is often described as a \"function  approximator\". \nThere are several algorithms that can be used to \"train\" an NN to approximate a desired (and often unknown) function. The most common one is based on an optimisation algorithm (like gradient descent) and the back-propagation algorithm. In order to approximate a function, an NN requires data, often in the form of pairs of inputs and outputs, $(x, y)$, where $x$ is the input and $y$ is the output of the function $f$ (that we want to approximate). Intuitively, after the learning phase, an NN that receives an input similar to $x$, denoted by $x'$, will then produce an output similar to $y$, $y'$.\nThere are a lot of tasks that can be thought of as functions. For example, the task of machine translation can be thought of as a function $f(x) = y$, where the input, $x$, is a sentence in one language (e.g. English) and the output, $y$, is a sentence in another language (e.g. Chinese). In this case, $f$ is unknown (to a lot of people), but an NN can still learn (from the data) an approximation of it. Therefore, NNs have wide applicability and are thus quite useful.\n", "type": 2, "id": "11978", "date": "2019-04-23T17:59:30.183", "score": 2, "comment_count": 1, "parent_id": "11976"}, "12039": {"line": 8162, "body": "You can find a lot of content regarding the mathematical definitions. Let's take it simpler:\nIt is a universal function approximator.\nThink about this:\n\nYou can approximate this locus with a line, two parameters: slope and offset.\nThat's the simplest you can do, with a straight line.\nNow, Think about this:\n\nYou approximated it with planes and curvatures, didn't you?\nThat's a little gist of what Neural Networks do. \n\nThey approximate functions with given I/O.\nThey find patterns.\nThey find behaviours in data (image, text, speech, numbers).\n\nWe think it is artificial intelligence, and it has worked out pretty well so far.\n", "type": 2, "id": "12039", "date": "2019-04-28T12:52:55.327", "score": 2, "comment_count": 2, "parent_id": "11976"}, "12260": {"line": 8329, "body": "Defining Contemporary Artificial Networks\nThe definition of an artificial neural network is a challenge. The most traveled sites on the Internet do not provide any adequate formal definitions and include much misconception. Wikipedia's articles contain some truth but read much like a blog in places, with too much ambiguity and misconception to be considered authoritative.\nThe question in the body of this question is phrased in the wider field of AI rather than in the narrower sub-field of machine learning, which is excellent.\nWhat is an artificial neural network in artificial intelligence?\n\nThat's the question to be answered in this particular post.\nThe answer given in the question is a synopsis of common understanding in a single sentence.\nIt is apparently used to find patterns in data\nand it is loosely inspired by human neural networks.\n\nCommon Misconception\nThis synopsis sentence is an accurate representation of common understanding, but it that common conception is inaccurate in a few ways.\n\nThere is very little in current artificial networks inspired specifically from human brains. Insect brains could just as easily be construed as the inspiration. Current artificial networks do not yet simulate the cognitive abilities of the cerebral cortex of people.\nPattern recognition can be accomplished with an artificial network, but that is not their defining characteristic. When machine learning is directed at the task of recognizing a pattern, it is not the same as targeting the AlphaZero at winning a Chess or Go tournament. It is not the same as a child recognizing her or his mother's face when walking among many adults at the mall. The concept class of a specific pattern is trained into the network by providing a loss function that compares the network output with an expectation from labels, some external signal, or a correlative model.\nCurrent artificial networks don't actually recognize anything. They can be trained to do many things that are tiny subsets of recognition, but they don't see and understand that what they are seeing is a car, pedestrian, missile, submarine, tapped hole into which a screw can be screwed, or crumb to be vacuumed up. They don't think, \"Oh no, and ICBM. I'd better send out some countermeasures and notify the President.\" They don't know what a missile is, but they can detect the features of one, having been trained to do so, and perform some action they were trained to perform.\n\nWhat Then is an Artificial Network in the Context of AI?\nWhatever definition we pick, it must apply to unadorned feed forward networks, CNNs, attention based networks, GRU cell networks, and other designs inspired by the original Perceptrons and CNNs, developed prior to the discovery of usable gradient descent and back-propagation mathematics and corresponding software technique.\nOur fitting of artificial networks into the context of AI is limited by the lack of cross-cultural or academic consensus on what intelligence is. Because of that, we cannot yet produce a formal definition on what qualifies as artificial intelligence.\nAllan Turing's Imitation Game was intended by him to be a thought experiment to consider what we think of as intelligence when in conversation. He never intended it to be a definition or a test of intelligence, but rather a way to test natural language and perhaps cognitive ability in machines against a human. If the machine could not be differentiated from the human in a double blind experiment, then the machine was, at at least a superficial level, acting human.\nDefinition By Universal Features of Artificial Networks\nLet's consider what is common to all contemporary artificial networks.\n\nThe idea of interconnected cells that together carry a signal, often in sequences of layers but not necessarily so, producing a structure that is conducive to learning an output behavior as a function of input stimuli: $\\mathcal{Y} = f(\\mathcal{X})$, where $\\mathcal{X}$ is a representation of input stimuli and $\\mathcal{Y} is a representation of output behavior. \u2014 If the cells are, as is the typical convention, arranged in layers, signals propagate from the input layer to the output layer to produce a behavior. Each layer contain cells identical to one another except for the values of signal attenuation parameters (usually called weights or simply parameters), which can be initialized in some way and then change in subsequent learning iterations based improvement strategies that have been shown to work both theoretically and empirically.\nThe mathematical concept of convergence on an optimum set of output behaviors in response to input stimuli, which we call training, and which relies on a numerical computation that indicates the degree of optimization \u2014 This can be called the error function, loss function, deviation function, pain function, or disparity function. The inverse function can be called a gain function, value function, pleasure function, accuracy function, or some other similar name. In systems theory terms, these are names of a feedback device, a corrective signal that causes a well designed learning system to adapt its behavior to hopefully achieve the optimum or sufficiently close to it to satisfy the system requirements. It is a hope because artificial networks are not guaranteed to converge on an optimum in every situation. There may be a deficiency in the training data, its presentation to the learning system, computing resources, overall approach, or hyper-parameter.\nCell non-linearity \u2014 not representable as a first degree polynomial of the form $y = A \\, \\vec{x} + B$, where $\\vec{x}$ is the cell input vector from immediately upstream cells in the network and $y$ is the cell output \u2014 This is a critical requirement to permit the learning of nontrivial behavioral complexity. Although this first degree polynomial representation is the representation of a typical attenuation at the front end of the cell, at which time $A$ is a parameter vector or matrix and $B$ is an offset, the right hand side of the expression is enclosed in what is normally called an activation function. This name arose from the obsolete conception of neurons, when it was thought that some learned synaptic attenuation of signals (the weights in $A$) and some learned threshold (-$B$) determined when a neuron fired. This is now known to be a gross oversimplification, for reasons about two thirds of the way down in this answer.\n\nAlthough there are many other characteristics that are common across many artificial network types, such as gradient descent, back-propagation, learning rate strategies, the set of commonly used activation functions, hyper-parameters, batching, data set analytics, approaches to training and use, and libraries used, none of these are absolute requirements on a design that makes it an artificial network. These three sum up the universal requirements.\nFitting Artificial Networks in AI\nWe can see that components with the above three characteristics have been used effectively in many systems that bear marks of intelligence. Some of the most notable cases can be enumerated.\n\nThe ability to prevail in the games of chess and go\nThe ability to identify and locate objects of certain types in an image\nThe ability to map trajectories or expression changes in a movie\nThe ability to generate images of certain types\n\nThe question of whether these abilities are examples of intelligence is debated. Artificial networks cannot get honest and explain to some humans why it made a particular game move, how it determined what pixels were what object, or what it had in mind when producing a new living room interior design. Artificial networks are not yet cognitive in the way a cognitive scientist uses that term, although no one has proved that they cannot one day develop cognitive abilities.\n\nReferences\n\nConvergence of ion channel genome content in early animal evolution, \nBenjamin J. Liebeskind, David M. Hillis, and Harold H. Zakon, 2014\nA measure for brain complexity: relating functional segregation and integration in the nervous system, G Tononi, O Sporns, and G M Edelman, PNAS, 1994\nHow artificial networks and biological neural systems are dissimilar\n\n", "type": 2, "id": "12260", "date": "2019-05-11T14:04:12.027", "score": 1, "comment_count": 0, "parent_id": "11976"}, "20500": {"line": 13442, "body": "Artificial Neural Network (ANN) is a technique or approach to information processing that is inspired by the workings of the biological nervous system, especially on human brain cells in processing information.\nThe key element of this technique is the structure of information processing systems that are unique and varied for each application. Neural Networks consist of a large number of information processing elements (neurons) that are interconnected and work together to solve a particular problem, which is generally a classification or prediction problem.\nsource : https://dosen.perbanas.id/artificial-neural-network/\n", "type": 2, "id": "20500", "date": "2020-04-21T11:01:08.717", "score": 0, "comment_count": 0, "parent_id": "11976"}, "20496": {"line": 13440, "body": "Artificial neural networks are one of the main tools used in machine learning. As the \"neural\" part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize.\nWhile neural networks (also called \"perceptrons\") have been around since the 1940s, it is only in the last several decades where they have become a major part of artificial intelligence. This is due to the arrival of a technique called \"backpropagation,\" which allows networks to adjust their hidden layers of neurons in situations where the outcome doesn't match what the creator is hoping for -- like a network designed to recognize dogs, which misidentifies a cat\nFor more info, visit: https://www.digitaltrends.com/cool-tech/what-is-an-artificial-neural-network/\n", "type": 2, "id": "20496", "date": "2020-04-21T10:55:21.573", "score": 0, "comment_count": 1, "parent_id": "11976"}}}
{"line": 10275, "body": "As I understand, ResNet has some identity mapping layers, whose task is to create the output as the same as the input of the layer. The ResNet solved the problem of accuracy degrading. But what is the benefit of adding identity mapping layers in intermediate layers?\nWhat's the effect of these identity layers on the feature vectors that will be produced in the last layers of the network? Is it helpful for the network to produce better representation for the input? If this expression is correct, what is the reason?\n", "type": 1, "id": "15743", "date": "2019-10-05T18:25:11.080", "score": 8, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "residual-networks"], "title": "What is the benefit of using identity mapping layers in deep neural networks like ResNet?", "answer_count": 4, "views": 3457, "accepted_answer": "15748", "answers": {"15748": {"line": 10280, "body": "TL;DR: Deep networks have some issues that skip connections fix.\nTo address this statement:\n\nAs I understand Resnet has some identity mapping layers that their task is to create the output as the same as the input of the layer\n\nThe residual blocks don't strictly learn the identity mapping. They are simply capable of learning such a mapping. That is, the residual block makes learning the identity function easy. So, at the very least, skip connections will not hurt performance (this is explained formally in the paper).\nFrom the paper:\n\nObserve: it's taking some of the layer outputs from earlier layers and passing their outputs further down and element wise summing these with the the outputs from the skipped layers. These blocks may learn mappings that are not the identity map.\nFrom paper (some benefits):\n\n$$\\boldsymbol{y} = \\mathcal{F}(\\boldsymbol{x},\\{W_i\\})+\\boldsymbol{x}\\quad\\text{(1)}$$The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).\n\nAn example of a residual mapping from the paper is $$\\mathcal{F} = W_2\\sigma_2(W_1\\boldsymbol{x})$$\nThat is $\\{W_i\\}$ represents a set of i weight matrices ($W_1,W_2$ in the example) occurring in the layers of the residual (skipped) layers. The \"identity shortcuts\" are referring to performing the element wise addition of $\\boldsymbol{x}$ with the output of the residual layers.\nSo using the residual mapping from the example (1) becomes:\n$$\\boldsymbol{y} = W_2\\sigma_2(W_1\\boldsymbol{x})+\\boldsymbol{x}$$\nIn short, you take the output $\\boldsymbol{x}$ of a layer skip it forward and element wise sum it with the output of the residual mapping and thus produce a residual block. \nLimitations of deep networks expressed in paper:\n\nWhen deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example.\n\nThe skip connections and hence the residual blocks allow for stacking deeper networks while avoiding this degradation issue. \nLink to paper\nI hope this helps. \n", "type": 2, "id": "15748", "date": "2019-10-06T04:10:29.097", "score": 8, "comment_count": 0, "parent_id": "15743"}, "15750": {"line": 10282, "body": "As explained in this paper , the major benefit of identity mapping is that it enables backpropagation signal to reach  from output (last) layers to input (first) layers. \nYou can see on the paper at section 2 that it resolves vanishing gradient problem which arises in deeper networks. \n", "type": 2, "id": "15750", "date": "2019-10-06T10:05:47.720", "score": 3, "comment_count": 0, "parent_id": "15743"}, "20229": {"line": 13230, "body": "\nAs I understand Resnet has some identity mapping layers that their task is to create the output as the same as the input of the layer. the resnet solved the problem of accuracy degrading. But what is the benefit of adding identity mapping layers in intermediate layers?\n\nSee this is applicable to deep/very deep networks. We decide to add layers when the model output is not converging to the expected output (it is due to very slow convergence). By this mapping, author has suggested that some portion of complexity of the model can directly be adjusted with input value leaving just residual value for adjustment. The output is mapped to input by identity function - so it is identity mapping. So the shortcut identity mapping is doing the task of some layers in plain neural network.\nThe identity mapping is applicable only if output and input are of same shape otherwise linear projection is required.\n", "type": 2, "id": "20229", "date": "2020-04-13T17:09:09.327", "score": 0, "comment_count": 0, "parent_id": "15743"}, "27582": {"line": 18931, "body": "As explained here\n\nonly if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network. For deep neural networks, if we can train the newly-added layer into an identity function $f(x)=x$, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors.\n\n", "type": 2, "id": "27582", "date": "2021-04-30T18:17:47.720", "score": 0, "comment_count": 0, "parent_id": "15743"}}}
{"line": 8146, "body": "I have an idea for a new type of AI for two-player games with alternating turns, like chess, checkers, connect four, and so on.\nA little background: Traditionally engines for such games have used the minimax algorithm combined with a heuristic function when a certain depth has been reached to find the best moves. In recent days engines using reinforcement learning, etc (like AlphaZero in chess) have increased popularity, and become as strong as or stronger than the traditional minimax engines.\nMy approach is to combine these ideas, to some level. A minimax tree with alpha-beta pruning will be used, but instead of considering every move in a position, these moves will be evaluated with a neural net or some other machine learning method, and the moves which seem least promising will not be considered further. The more interesting moves are expanded like in traditional minimax algorithms, and the same evaluation are again done for these nodes' children.\nThe pros and cons are pretty obvious: By decreasing the breadth (number of moves in a position), the computation time will be reduced, which again can increase the search depth. The downside is that good moves may not be considered, if the machine learning method used to evaluate moves are not good enough.\nOne could of course hope that the position evaluation itself (from the neural net, etc) is good enough to pick the best move, so that no minimax is needed. However, combining the two approaches will hopefully make better results.\nA big motivation for this approach is that it resembles how humans act when playing games like chess. One tends to use intuition (which will be what the neural net represents in this approach) to find moves which looks interesting. Then one will look more thoroughly at these interesting moves by calculating moves ahead. However, one does not do this for all moves, only those which seem interesting. The idea is that a computer engine can play well by using the same approach, but can of course calculate much faster than a human.\nTo illustrate the performance gain: The size of a minimax tree is about b^d, where b is the average number of moves possible in each position, and d is the search depth. If the neural net can reduce the size of considered moves b to half, the new complexity will be (b/2)^d. If d is 20, that means reducing the computation time by approx. 1 million.\nMy questions are:\n\nDoes anyone see any obvious flaws about this idea, which I might have missed?\nHas it been attempted before? I have looked a bit around for information about this, but haven't found anything. Please give me some references if you know any articles about this.\nDo you think the performance of such a system could compete with those of pure minimax or those using deep reinforcement learning?\n\nExactly how the neural net will be trained, I have not determined yet, but there should be several options possible.\n", "type": 1, "id": "12020", "date": "2019-04-26T12:36:13.383", "score": 1, "comment_count": 2, "tags": ["neural-networks", "game-ai", "minimax", "chess", "alpha-beta-pruning"], "title": "Minimax combined with machine learning to determine if a path should be explored", "answer_count": 2, "views": 1318, "accepted_answer": null, "answers": {"20294": {"line": 13287, "body": "The use of of a neural network to push the search algorithm to continually only along a promising path is the same that was described in the AlphaZero paper. In AlphaZero, the NN loop contained the search function and would encourage the continued search of high probability moves that were then simulated by the same NN that now contained the Value Net. The use of alpha-beta specifically is not necessary. Just a search function aptly known as PUCT (Predictor + Upper Confidence Bounds applied to Trees)\n", "type": 2, "id": "20294", "date": "2020-04-15T20:00:33.363", "score": 1, "comment_count": 0, "parent_id": "12020"}, "23552": {"line": 15777, "body": "There is a lot of related research out there.\nYou can look at contextual bandit problems, which is the basis for \"Monte Carlo Tree Search\". Here some clever bookkeeping is used to make sure that branches that looked bad but haven't been explored recently will still get explored eventually. This results in the UCT algorithm, which then got used in combination with deep learning in AlphaGo, but you can use it without deep learning too, if that makes more sense for your particular problem.\nThe exploration / exploitation trade-off is crucial for this kind of problem. Your original estimates of the value of a position will be very uninformed, so it should not be used to prune the search tree too aggressively. This is exactly what UTC does, with provable theoretical guarantees.\n", "type": 2, "id": "23552", "date": "2020-09-12T21:28:52.663", "score": 0, "comment_count": 0, "parent_id": "12020"}}}
{"line": 8198, "body": "Can a normal neural network work as good as a convolutional network? If yes, how much more time and neurons would it need compared to a CNN?\n", "type": 1, "id": "12083", "date": "2019-05-01T08:09:32.053", "score": 2, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks"], "title": "Is it possible for a NN to reach the same results as CNNs?", "answer_count": 2, "views": 69, "accepted_answer": "12088", "answers": {"12089": {"line": 8203, "body": "Yes. In theory, a single layer neural network can compute any function. In practice, such a network would have to be much larger than a CNN with equivalent functionality and would therefore be much harder to train. \n", "type": 2, "id": "12089", "date": "2019-05-01T11:40:03.137", "score": 0, "comment_count": 0, "parent_id": "12083"}, "12088": {"line": 8202, "body": "NNs won't be able to reach the performance of CNNs, in general.\nBy a Neural Network, I am assuming you're pointing to 'vanilla' vector based neural networks. \nLet's take the MNIST dataset, it performs almost similar in both NNs and CNNs; the reason being digits of almost same size, and similar spatial drawings. To put it easy, all the digits roughly take up the same area as the other 60k. If you were to zoom in our zoom out these digits, the trained NN may not perform well.\nWhy?\nPlain NNs lack the ability to extract 'position independent' features. A cat is a cat to your eye, no matter whether you saw it in the center of an image, or left corner. CNNs use 'filters' to extract 'templates' of cat. Hence CNNs can 'localize' what it is searching for. \nYou could say, NNs look at the 'whole' data to make a sense, CNNs look at the 'special' features at certain parts to make a sense.\nThis is also a reason why CNNs are popular. NNs are suited for their own applications. Every input node in a vectorized Neural Networks represent a feature, and that feature is tied to a purpose throughout training, i.e. its position is fixed throughout the training.\n", "type": 2, "id": "12088", "date": "2019-05-01T11:19:12.393", "score": 2, "comment_count": 0, "parent_id": "12083"}}}
{"line": 9604, "body": "I am wondering if I can use neural networks to find features importances in similar manner as it can be done for random forests or decision trees and if so, how to do it?\nI would like to use it on tabular time series data (not images). The reason why I want to find importances on neural networks not on decision trees is that NNs are more complicated algorithms so using NNs might point out some correlations that are not seen by simple algorithms and I need to know what features are found to be more useful with that complicated correlations.\nI am not sure if I made it clear enough, please let me know if I have to explain something more.\n", "type": 1, "id": "13861", "date": "2019-08-08T17:21:03.900", "score": 1, "comment_count": 0, "tags": ["neural-networks", "feature-selection", "feature-engineering"], "title": "Can neural networks be used to find features importance?", "answer_count": 3, "views": 112, "accepted_answer": null, "answers": {"13912": {"line": 9636, "body": "It's possible. I've used the olden() function from NeuralNetTools.\nPlease take a look at this example found online:\nhttp://blogs2.datall-analyse.nl/2016/02/19/rcode_variable_importance_neural_network/\n", "type": 2, "id": "13912", "date": "2019-08-12T11:16:46.017", "score": -1, "comment_count": 2, "parent_id": "13861"}, "13918": {"line": 9642, "body": "This should be possible, considering universal approximation theorem you should be able to build a ann that approximates features that gives the most likely best feature set for a different net to train on. I would us a rnn for with a softmax output layer that ranks features by performance.\nYou can find a good explanation of softmax here: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\nbasically it will assign probability values for each output node with all of these values adding up to 1.0 \n", "type": 2, "id": "13918", "date": "2019-08-12T15:03:21.907", "score": 2, "comment_count": 4, "parent_id": "13861"}, "27475": {"line": 18852, "body": "There are multiple standard ways of feature selection, for example ranking features by information gain, that you could use, and then you can train the neural network on just those features.\nHowever, let's assume you have trained a neural network on all of the features and now want to estimate their importance. One approach you could take is to perform a sensitivity analysis on the inputs: add random noise in a controlled fashion to different features and see what effect it has. If the training dataset has been centered (so each feature has zero mean) then you could set the inputs to zero (the \"average\" training example) and then perturb each feature in turn to see what the effect is. You could also fix a feature permanently to zero and then run your validation data through the network and see how accuracy changes. There should be no major effect for insignificant features, but important features being zeroed should lead to a decrease in accuracy. You can also do something like this when predicting specific examples: perturb the example's features and see how much the prediction changes. LIME does something like this to explain why a black box like a neural network makes the predictions it does.\n", "type": 2, "id": "27475", "date": "2021-04-22T21:57:07.387", "score": 0, "comment_count": 0, "parent_id": "13861"}}}
{"line": 8740, "body": "I am interested in creating a neural network-based engine for chess. It uses a $8 \\times 8 \\times 73$ output space for each possible move as proposed in the Alpha Zero paper: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.\nHowever, when running the network, the first selected move is invalid. How should we deal with this? Basically, I see two options.\n\nPick the next highest outputted move, until it is a valid move. In this case, the network might automatically over time not put illegal moves on top.\nProcess the game as a loss for the player who picked the illegal move. This might have the disadvantage that the network might be 'stuck' on only a few legal moves.\n\nWhat is the preferred solution to this particular problem?\n", "type": 1, "id": "12773", "date": "2019-06-10T12:19:14.283", "score": 1, "comment_count": 0, "tags": ["neural-networks", "ai-design", "game-ai", "chess", "alphazero"], "title": "How to deal with invalid output in a policy network?", "answer_count": 1, "views": 120, "accepted_answer": null, "answers": {"12774": {"line": 8741, "body": "You should have a method to generate a possible moves output based on the board state. Use this as a mask before normalization in the policy head.\n", "type": 2, "id": "12774", "date": "2019-06-10T12:27:27.000", "score": 0, "comment_count": 0, "parent_id": "12773"}}}
{"line": 9047, "body": "The basic seq-2-seq model consists of 2 parts: a recurrent encoder that compresses a sequence to a vector and decoder that unrolls the vector into the output sequence:\n\nWhy is the output, w, x, y, z of the decoder used as its input? Shouldn't the hidden state of the RNN from the previous timestamps be enough?\n", "type": 1, "id": "13172", "date": "2019-07-02T23:02:12.503", "score": 6, "comment_count": 0, "tags": ["recurrent-neural-networks", "sequence-modelling"], "title": "In sequence-to-sequence, why is the output of the decoder used as its input?", "answer_count": 3, "views": 161, "accepted_answer": null, "answers": {"13182": {"line": 9056, "body": "In the original seq2seq paper, they used two RNN, one for encoding and one for decoding. In the encoder they need to unroll the inputs to capture the time dependency. Now if we want to pass the hidden state from the encoder to the decoder that means that the decoder hidden state shape needs to match the encoder (aka same architecture). Since the architecture is the same, we can not directly generate a sequence of n samples within the decoder without unrolling it and you can not unroll it without an input.  \n", "type": 2, "id": "13182", "date": "2019-07-03T08:05:57.273", "score": 0, "comment_count": 0, "parent_id": "13172"}, "13187": {"line": 9060, "body": "In seq2seq they model the joint distribution of whatever char/word sequence by decomposing it into time-forward conditionals:\n$\\begin{align*}\np(w_1,w_2,...,w_n) =& \\ p(w_1)*p(w_2|w_1) * \\ ... \\ * p(w_n|w_1,...,w_{n-1}) \\\\\n =& \\ p(w_1)*\\prod_{i=2}^{n}p(w_i|w_{<i})\n\\end{align*}\n$ \nThis can be sampled by samping each of the conditional is ascending order. So thats exactly what theyre trying to imitate. You want the second output dependant on the sampled first output, not its distribution.  \nThis is why the hidden state is NOT good for modeling this setup because it is a latent representation of the distribution, not a sample of the distribution.  \nNote: In training they use ground truth as input by default because its working under the assumption the model shouldve predicted the correct word and if it didnt the gradient of the word/char level loss will reflect that (this is called teacher forcing and has a multitude of pitfalls)\n", "type": 2, "id": "13187", "date": "2019-07-03T11:51:06.437", "score": 3, "comment_count": 2, "parent_id": "13172"}, "13185": {"line": 9058, "body": "\nShouldn't the hidden state of the RNN from the previous timestamps be enough?\n\nIt is theoretically enough to generate a sequence. However, allowing an input offers a couple of convenient extras:\n\nTraining data for output sequences is used twice - once as input (as previous sequence data), once as target (to establish loss metric). This may help training process as the decoder trains both as a decoder to the new sequence type and as a predictive model over the output sequence semi-independently - i.e. weights from input to RNN layer are affected by error gradients separately to weights between previous hidden state and next state, although the two sets of weights together influence output and next state, so are not fully independent over a sequence.\nBy allowing input of sequence so far generated, the decoder can work as a generator, where the next item in the sequence does not need to be the maximum probability item, but can be sampled or have rules applied. This allows for approaches such as BEAM search, commonly used in machine translation, which maintains several potential outputs, selecting best one at the end.\n\nI have not done the experiment, but I suspect the first item results in faster and better generalisation. The second one is very convenient for natural language generation and similar problems.\n", "type": 2, "id": "13185", "date": "2019-07-03T09:05:35.023", "score": 1, "comment_count": 2, "parent_id": "13172"}}}
{"line": 9608, "body": "This seems like such a simple idea, but I've never heard anyone that has addressed it, and a quick Google revealed nothing, so here it goes.\nThe way I learned about machine learning is that it recognizes patterns in data, and not necessarily ones that exist -- which can lead to bias. One such example is hiring AIs: If an AI is trained to hire employees based on previous examples, it might recreate previous, human, biases towards, let's say, women. \nWhy can't we just feed the training data without data that we would consider discriminatory or irrelevant, for example, without fields for gender, race, etc., can AI still draw those prejudiced connections? If so, how? If not, why has this not been considered before?\nAgain, this seems like such an easy topic, so I apologize if I'm just being ignorant. But I have learned a bit about AI and machine learning specifically for some time now, and I'm just surprised this hasn't ever been mentioned, not even as a \"here's-what-won't-work\" example.\n", "type": 1, "id": "13866", "date": "2019-08-09T03:26:20.213", "score": 5, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "social", "algorithmic-bias"], "title": "Preventing bias by not providing irrelevant data", "answer_count": 3, "views": 122, "accepted_answer": null, "answers": {"13874": {"line": 9613, "body": "\nWhy can't we just feed the training data without data that we would consider discriminatory or irrelevant, for example, without fields for gender, race, etc., can AI still draw those prejudiced connections? If so, how? If not, why has this not been considered before?\n\nYes. The AI/ model still can learn those prejudiced connections. Consider that you have a  third variable which is a confounding variable or has spurious relationship that is correlated with the bias variable (BV) and the dependent variable (DV). And, the analyst removed the BV but failed to remove the third variable from the data that is fed to the model. Then the model will learn the relationships the analyst didn't want it to learn.\nBut, at the same time the removal of the variables could lead to omitted variable bias, which occurs when a relevant variable is left out.\nEx:\nSuppose that the goal is prediction of salary ($S$) of an individual and the independent variables are age ($A$) and experience ($E$) of the individual. The analyst wants to remove the bias that could come in because of age. So, she removes age from one of the models and comes up with two competing linear models:\n$S = \\beta_0 + \\beta_1E + \\varepsilon$\n$S = \\beta_0 + \\beta_1^*E + \\beta_2A + \\varepsilon$\nSince, experience is highly correlated with age, in presence of age in the model, it is very likely that $\\beta_1^* < \\beta_1$. $\\beta_1$ will be a bogus estimate of a person's experience on salary as the first model suffers from the omitted variable bias. \nAt the same time the predictions from the first model would be reasonably good although the second model is very likely to beat the first model. So, if the analyst wants to remove any 'bias' that might come in because of age i.e. $A$ she must also remove $E$ from the model.\n", "type": 2, "id": "13874", "date": "2019-08-09T11:57:48.853", "score": 4, "comment_count": 8, "parent_id": "13866"}, "13878": {"line": 9614, "body": "Sometimes, the reason that this isn't an option is that you don't have that much control over what data is provided. Suppose, for example, you want a fancy AI that reads a Resume and filters on suitability for a job. There isn't a particularly rigid formula about what people put in their Resume, which makes it difficult to exclude things you'd rather not consider. \nWhere you do have more control about exactly what information you consider, it can still be thwarted by correlations. Think, for a moment, how this pans out with a human decision maker. You want to ensure that Joe Sexist gives women a fair chance at being hired, so you make sure that there isn't a gender field in the application form. You also blind out the applicant's name, since there is no good reason that a name should determine suitability for a role, and including it would reveal a lot of genders. But you don't block out the hobbies, clubs and societies entry, because it's thought to say something positive about an applicant if they were the captain of their college sports team. Joe Sexist, however, considers it a positive if an applicant captained a male dominated team such as American football, but considers negative being captain of a female dominated team! Some might say that wouldn't quite be bias against women; it's bias against players of sports that Joe Sexist considers effeminate. But really a skunk by any other name would stink as bad. \nThe same sort of thing can happen with AI. Now to be clear, the AI is not sexist. It is a blank sheet with no preconceptions until it gets fed data. But when it gets fed data, it will find patterns in the same way. The dataset it gets given is years of hiring decisions by Joe Sexist. As suggested, there is no entry for gender, but there are fields for all the things that might be considered slightly relevent. For example, we include whether they have a clean driving license. The AI notices that there is a positive correlation between the number of road traffic offences an applicant has and Joe's likelihood of hiring them (Because, of course, there happens to be a gender correlation between dangerous driving and gender). Again, the AI has no preconceptions. It doesn't know that traffic offences are dangerous and should be weighted against. As far as its dataset suggests, they're points! With this sort of information in a dataset, the AI can exhibit all the same sorts of biases as Joe Sexist, even though it doesn't know what a \"woman\" is! \n\nTo expand this with specific numbers, suppose that your dataset has 1000 male and 1000 female applicants for a total of 1000 places. Of those, 400 of the men and 100 of the women have a tarnished traffic record. \nJoe Sexist was not in favour of reckless drivers: in fact a clean traffic record guaranteed you would beat an equivalent candidate with a tarnished record. But he was very in favour of men: being male made you 9 times more likely to get hired than being female.\nSo he gives places to 900 of the men: all 600 of the clean drivers and 300 dirty drivers. \nHe gives places to 100 of the women: all to clean drivers. \nNow, you take away any mention of gender in the dataset. \nThere are 2000 people, 500 drive badly, 1500 drive well. \nOf these, 300 bad drivers get jobs, and 700 good drivers get jobs. \nTherefore the 25% of the population who drive badly get 30% of the jobs, which means (as far as an AI that just looks blindly at the numbers is concerned) that driving badly suggests you should get the job. That's a problem.\nFurther, suppose you have a new batch of 2000 applicants with the same ratios and it's the AI's turn to decide. Now often AIs actually make this even worse by exagerating the significance of subtle indicators, but let's suppose that this one does everything in strict proportionality. The AI has learned that 60% (300 / 500) of the bad drivers should get the job. It doesn't know about gender, so it at least allocates the bad driver bonus \"fairly\": 240 male and 60 female bad drivers get jobs. Then 280 male and 420 female good drivers get jobs. This comes to 520 male and 480 female applicants getting in. Even though the original applicant pool was balanced and if anything women were better (at least at driving) the original sexism in the training dataset still gives some advantage to the men. (as well as giving an advantage to bad drivers)\n\nNow, don't let me completely disuade you. In the human case, it is a known fact that blinding out some information does indeed give more balanced hiring decisions. And even in my toy example, while it doesn't get to fairness it has massively reduced the scale of the sexism. So yes, it probably would make the AI somewhat less sexist if the most blatant indicators aren't provided in the dataset. But perhaps this gives some intuition about why it's not a complete solution to the problem. There is some sexism that leaks through, and it also causes the system to make very weird associations with other bits of the dataset. \n", "type": 2, "id": "13878", "date": "2019-08-09T19:58:54.613", "score": 1, "comment_count": 0, "parent_id": "13866"}, "13879": {"line": 9615, "body": "There is a wider social issue to consider here also. When we build machines, we evaluate what they do and decide if the action that they undertake is to our benefit or not. All societies do this, although you are probably more aware of obvious examples such as the Amish than you are of your own society.\nWhen people complain about biased decision making by AI systems, they are not just evaluating if the result is accurate, but also if that decision supports the values that they wish to see instantiated in society. \nYou can make a human take cultural factors into account when making a decision, but not an AI that is completely unaware of them. People describe this as complaining about 'bias', but that is not always completely accurate. They are really complaining that the use of AI systems fail to take into account wider social issues that they consider to be important. \n", "type": 2, "id": "13879", "date": "2019-08-09T21:27:12.007", "score": 0, "comment_count": 0, "parent_id": "13866"}}}
{"line": 9735, "body": "I have developed face recognition algorithms by using pre-built libraries in Python and open CV. However, suppose if I want to make my own neural network algorithm for face recognition, what are the steps that I need to follow?\nI have just seen Andrew Ng's course videos (specifically, I watched 70 videos).\n", "type": 1, "id": "14028", "date": "2019-08-18T04:51:13.947", "score": 0, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "face-recognition"], "title": "What are the steps that I need to follow to build a neural network for face recognition?", "answer_count": 3, "views": 182, "accepted_answer": "14092", "answers": {"14092": {"line": 9787, "body": "My implementation to increase detection on video is using object tracking algorithms.\nMore specifically, first, I detect the object using a trained classifier. Then I track the object with the KCF algorithm. If the object tracker misses the object, again I call for the classifier.\n", "type": 2, "id": "14092", "date": "2019-08-22T09:55:53.633", "score": 0, "comment_count": 0, "parent_id": "14028"}, "23463": {"line": 15711, "body": "You could try building Siamese network and train it on a large set of faces.\nTwo identical networks are used; one taking the known signature for the person, and another taking a candidate signature. The outputs of both networks are combined and scored to indicate whether the candidate signature is real or a forgery. The deep CNN are first trained to discriminate between examples of each class. The models are then re-purposed for verification to predict whether new examples match a template for each class. Specifically, each network produces a feature vector for an input image, which are then compared using the L1 distance and a sigmoid activation. Similar goes with face.\n", "type": 2, "id": "23463", "date": "2020-09-07T17:35:19.307", "score": 0, "comment_count": 4, "parent_id": "14028"}, "23471": {"line": 15716, "body": "For the Construction of Deep Learning Models\nBackbone Deep Learning models which can be applied to a variety of deep learning tasks (including facial recognition) have been implemented in a range of libraries available in Python. I'm assuming by constructing your own algorithm you mean a novel implementation of the model structure. Taking the PyTorch framework as an example, some common pretrained models are available here:\nhttps://github.com/pytorch/vision/tree/master/torchvision/models\nTo train a novel face recognition model you could follow the tutorial for object detection available here:\nhttps://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\nand make changes to the model.\nIn the tutorial they use model features from the library in the following section of code:\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n\nFor the simplest example torchvision.models.AlexNet.features look like this:\nself.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n\nAdding or subtracting layers from this backbone feature extractor would result in a new \"algorithm\" for object detection.\nIf you want to know exactly what mathematical operation each of these layers is performing you can look at the PyTorch documentation. For example, in the case of nn.Relu layer:\nhttps://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\nApplies the rectified linear unit function element-wise:\n$$ ReLU(x)=(x)^{+}=max(0,x)$$\n", "type": 2, "id": "23471", "date": "2020-09-08T05:43:35.543", "score": 0, "comment_count": 0, "parent_id": "14028"}}}
{"line": 9581, "body": "As everyone experienced in deep learning might know, in an image classification problem we normally add borders to images then resize it to the input size of a CNN network. The reason of doing this is to keep aspect ratio of the original image and retain it's information. \nI have seen people fill black (0 pixel value for each channel), gray (127 pixel value for each channel), or random value generated from gaussian distribution to the border. \nMy question is, is there any proof that which of these is correct?\n", "type": 1, "id": "13832", "date": "2019-08-06T19:52:50.630", "score": 3, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "computer-vision", "image-processing"], "title": "How should we pad an image to be fed in a CNN?", "answer_count": 2, "views": 594, "accepted_answer": null, "answers": {"14068": {"line": 9769, "body": "If the computational components of the forward feed through the network have no curvature, which is normally the case in a sum of products, then it can be proven that any constant pixel value is equivalent in terms of effect on convergence results.  We wouldn't expect a proof for that, since it would be too trivial to spend time writing up for publication.  In general, functioning vision systems have feed forward computational components with curvature, so the padding is likely significant.\nEven the convolutional layers may have activation functions or something even more complex going forward, as noted in Gauge Equivariant Convolutional Networks and the Icosahedral CNN (Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling, 2019).\nIf purely stochastic values with value distributions like that of the un-padded coordinates are used, it may be possible to prove that some gain is made, but none appeared in a few academic article searches just made.  Not surprisingly, there are many proofs regarding the properties of various message padding strategies for cryptography.\nShort of the inclusion of thermal or quantum noise acquisition devices in VLSI circuitry and exposure of those devices in software, purely stochastic values cannot be generated.  This leaves the risk of a learning approach expected to extract features from frames learning features of the pseudo-random noise generator used to pad.\nThe answer is that none are universally correct and there appears to be much work to do in proving advantages between different techniques in as many cases as such advantages can be proven.\n", "type": 2, "id": "14068", "date": "2019-08-20T15:20:32.270", "score": 2, "comment_count": 0, "parent_id": "13832"}, "13843": {"line": 9590, "body": "I've more often seen image resizing than padding to be honest and tend to resize the images. Maybe it's because datasets I've used have images with near equal aspect ratios.\nOne major exception was when I worked with MR images. These were orthogonal and it would be wrong to mess up the aspect ratio. However, in this domain images have black borders everywhere, so a zero-padding was easy to apply.\nThe most common use of padding I've seen is for data augmentations (to fill values gone due to translations, rotations, shifting etc.). In this regard, I've used many types of paddings (constant value, random value, 'same' padding, mirrored padding etc.) The best I've found to empirically work is zero-padding but I don't think that you'll ever find a proof for this. I like to think of it as a hyperparameter; different padding strategies maybe work better for different tasks. Though I think that zero-padding is the safest (there is a small chance of messing things up). \n", "type": 2, "id": "13843", "date": "2019-08-07T10:04:46.933", "score": 0, "comment_count": 1, "parent_id": "13832"}}}
{"line": 10971, "body": "I have a large set of data points describing mappings of binary vectors to real-valued outputs. I am using TensorFlow, and would like to train a model to predict these relationships. I used four hidden layers with 500 neurons in each layer, and sigmoidal activation functions in each layer.\nThe network appears to be unable to learn, and has high loss even on the training data. What might cause this to happen? Is there something wrong with the design of my network?\n", "type": 1, "id": "16557", "date": "2019-11-16T12:06:10.730", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "tensorflow", "activation-function", "hidden-layers"], "title": "What could be the problem when a neural network with four hidden layers with the sigmoid activation function is not learning?", "answer_count": 2, "views": 103, "accepted_answer": "16558", "answers": {"16609": {"line": 11014, "body": "Your code suggests a likely problem here: It looks like you are training a very deep neural network with sigmoidal activation functions at every layer. \nThe sigmoid has the property that its derivative (S*(1-S)) will be extremely small when the activation function's value is close to 0 or close to 1. In fact, the largest it can be is about 0.25.\nThe backpropigation algorithm, which is used to train a neural network, will propagate an error signal backwards. At each layer, the error signal will be multiplied by, among other things, the derivative of the activation function.\nIt is therefore the case that by the 4th layer your signal is at most $0.25^4 = \\frac{1}{256}$ the size that it was at the start of the network. In fact, it is likely much smaller than this. With a smaller signal, your learning rates at the bottom of the nextwork will effectively be much smaller than the learning rates at the top, which will make it very difficult to pick a learning rate that is effective overall.\nThis problem is known as the vanishing gradient. \nTo fix this, if you want to use a deep architecture, consider using an activation function that does not suffer from a vanishing gradient. The Rectified Linear activation function, used in so-called \"ReLU\" units, is a non-linear activation that does not have a vanishing gradient. It is common to use ReLUs for the earlier layers in a network, and a sigmoid at the output layer, if you need outputs to be bounded between 0 and 1.\n", "type": 2, "id": "16609", "date": "2019-11-19T01:55:29.953", "score": 3, "comment_count": 0, "parent_id": "16557"}, "16558": {"line": 10972, "body": "When training our neural network, you need to scale your dataset in order to avoid slowing down the learning or prevent effective learning.\nTry normalizing your output.\nThis Tutorial might help\n", "type": 2, "id": "16558", "date": "2019-11-16T12:41:08.467", "score": 0, "comment_count": 1, "parent_id": "16557"}}}
{"line": 10677, "body": "This is my first question in this community (Artificial Intelligence) and I hope I can get some answers to a troublesome issue of mine.\nI am trying to understand Stochastic Gradient Decent (SGD) (with possibly momentum) and modify the keras version to an accumulative one. I have asked a similar question on Stack Overflow but here I am interested in the actual function (and secondarily to the code itself).\nFor simplicity let's consider I have a neural network with just 10 layers and I am training for 100 iterations (regardless of epochs).\nFor easier reference I post the keras code here (and also skipped nesterov and constraints lines of code):\ndef get_updates(self, loss, params):\n    grads = self.get_gradients(loss, params)\n    self.updates = [K.update_add(self.iterations, 1)]\n\n    lr = self.lr\n    if self.initial_decay > 0:\n        lr = lr * (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n    shapes = [K.int_shape(p) for p in params]\n    moments = [K.zeros(shape) for shape in shapes]\n    self.weights = [self.iterations] + moments\n    for p, g, m in zip(params, grads, moments):\n        v = self.momentum * m - lr * g  # velocity\n        self.updates.append(K.update(m, v))\n        new_p = p + v\n\n        self.updates.append(K.update(p, new_p))\n    return self.updates\n\nAs far as I understand the mechanism of SGD works as follows:\n\nThe gradients are updated at every iteration, which in the keras code is performed by calling the get_updates(self, loss, params) function.\nThere is a gradient that correspond to each layer (in the keras code I think a list of Tensors are produced to keep the gradients).\nSo, in that sense the for p, g, m in zip(params, grads, moments) loop just iterates through all layers (and their corresponding gradients and moments) and calculates new values.\n\nThe really confusing part for me is that if the above function is being called at every iteration then the moments = [K.zeros(shape) for shape in shapes] basically initializes moments to a zero Tensor at every iteration. Wouldn't make more sense if moment was initialized once and then keep updating with new values?\nI have noticed that despite my understanding the moments are actually used (as it would be more expected for such a big project to have missed a large bug such as this one). I cannot really understand how though.\n", "type": 1, "id": "16213", "date": "2019-11-01T13:03:45.053", "score": 2, "comment_count": 1, "tags": ["neural-networks"], "title": "How exactly does SGD with momentum works in keras", "answer_count": 1, "views": 176, "accepted_answer": null, "answers": {"20306": {"line": 13298, "body": "I noticed the same issue. Also, it seems that the equation for applying momentum does not exactly match up with the standard one:\n$$ v_{d\\theta} := \\beta v_{d\\theta} + (1 - \\beta)d\\theta \\\\\n \\theta := \\theta - \\alpha v_{d\\theta}$$\n", "type": 2, "id": "20306", "date": "2020-04-16T03:17:57.770", "score": 0, "comment_count": 0, "parent_id": "16213"}}}
{"line": 9213, "body": "I learn a DNN for image recognition. During each epoch, I calculate mean loss in the training set. After each epoch, I calculate loss and number of errors over both training and test set. The problem is, training and test error go to (almost) zero, then increase, go to zero again, increase, and so on.  The process seems stochastic.\nepoch: 1 mean_loss=0.109 train: errs=7 loss=0.00622 test: errs=3 loss=0.00608\nepoch: 2 mean_loss=0.00524 train: errs=5 loss=0.00309 test: errs=3 loss=0.00369\nepoch: 3 mean_loss=0.00408 train: errs=13 loss=0.00614 test: errs=7 loss=0.00951\nepoch: 4 mean_loss=0.00198 train: errs=113 loss=0.102 test: errs=51 loss=0.265\nepoch: 5 mean_loss=0.00424 train: errs=3 loss=0.00201 test: errs=2 loss=0.00148\nepoch: 6 mean_loss=0.0027 train: errs=1 loss=0.000466 test: errs=2 loss=0.00193\nepoch: 7 mean_loss=0.00797 train: errs=5 loss=0.00381 test: errs=0 loss=0.000493\nepoch: 8 mean_loss=0.00368 train: errs=1 loss=0.000345 test: errs=2 loss=0.00148\nepoch: 9 mean_loss=0.000358 train: errs=0 loss=6.76e-05 test: errs=0 loss=0.000446\nepoch: 10 mean_loss=0.00101 train: errs=164 loss=0.0863 test: errs=67 loss=0.19\nepoch: 11 mean_loss=0.000665 train: errs=0 loss=2.38e-05 test: errs=0 loss=9.86e-05\nepoch: 12 mean_loss=0.00714 train: errs=5 loss=0.00909 test: errs=0 loss=0.00816\nepoch: 13 mean_loss=0.00266 train: errs=73 loss=0.0333 test: errs=10 loss=0.0192\nepoch: 14 mean_loss=0.00213 train: errs=0 loss=7.74e-05 test: errs=0 loss=0.000197\nepoch: 15 mean_loss=6.12e-05 train: errs=0 loss=7.66e-05 test: errs=0 loss=3.44e-05\nepoch: 16 mean_loss=0.00162 train: errs=5 loss=0.00265 test: errs=0 loss=0.0012\nepoch: 17 mean_loss=0.000159 train: errs=0 loss=3.11e-05 test: errs=0 loss=4.26e-05\nepoch: 18 mean_loss=4.68e-05 train: errs=0 loss=3.28e-05 test: errs=0 loss=6.05e-05\nepoch: 19 mean_loss=2.47e-05 train: errs=0 loss=2.8e-05 test: errs=0 loss=5.01e-05\nepoch: 20 mean_loss=2.2e-05 train: errs=0 loss=2.31e-05 test: errs=0 loss=3.95e-05\nepoch: 21 mean_loss=2.37e-05 train: errs=0 loss=1.76e-05 test: errs=0 loss=2.52e-05\nepoch: 22 mean_loss=1.4e-05 train: errs=0 loss=1.16e-05 test: errs=0 loss=1.52e-05\nepoch: 23 mean_loss=2.13e-05 train: errs=0 loss=1.65e-05 test: errs=0 loss=2.13e-05\nepoch: 24 mean_loss=1.53e-05 train: errs=0 loss=1.91e-05 test: errs=0 loss=2.46e-05\nepoch: 25 mean_loss=0.00419 train: errs=0 loss=5.27e-05 test: errs=0 loss=4.65e-05\nepoch: 26 mean_loss=0.000372 train: errs=6 loss=0.00297 test: errs=3 loss=0.00731\nepoch: 27 mean_loss=0.0016 train: errs=0 loss=4.23e-05 test: errs=0 loss=3.69e-05\nepoch: 28 mean_loss=3.34e-05 train: errs=0 loss=2.44e-05 test: errs=0 loss=2.76e-05\nepoch: 29 mean_loss=7.03e-05 train: errs=0 loss=2.16e-05 test: errs=0 loss=1.69e-05\nepoch: 30 mean_loss=2.41e-05 train: errs=0 loss=1.84e-05 test: errs=0 loss=1.77e-05\nepoch: 31 mean_loss=1.26e-05 train: errs=0 loss=2.11e-05 test: errs=0 loss=1.78e-05\nepoch: 32 mean_loss=1.39e-05 train: errs=0 loss=2.75e-05 test: errs=0 loss=2.42e-05\nepoch: 33 mean_loss=7.68e-05 train: errs=0 loss=0.00014 test: errs=0 loss=4.66e-05\nepoch: 34 mean_loss=2.53e-05 train: errs=0 loss=1.48e-05 test: errs=0 loss=1.56e-05\nepoch: 35 mean_loss=0.000352 train: errs=1786 loss=2.17 test: errs=493 loss=2.56\nepoch: 36 mean_loss=0.0088 train: errs=0 loss=0.000347 test: errs=0 loss=0.000449\nepoch: 37 mean_loss=0.000395 train: errs=0 loss=6.18e-05 test: errs=0 loss=0.000125\nepoch: 38 mean_loss=5e-05 train: errs=0 loss=6.73e-05 test: errs=0 loss=9.89e-05\nepoch: 39 mean_loss=0.00401 train: errs=26 loss=0.00836 test: errs=27 loss=0.0269\nepoch: 40 mean_loss=0.00051 train: errs=0 loss=7.66e-05 test: errs=0 loss=7.07e-05\nepoch: 41 mean_loss=5.49e-05 train: errs=0 loss=2.47e-05 test: errs=0 loss=2.58e-05\nepoch: 42 mean_loss=3.38e-05 train: errs=0 loss=1.67e-05 test: errs=0 loss=2.1e-05\nepoch: 43 mean_loss=2.45e-05 train: errs=0 loss=1.28e-05 test: errs=0 loss=2.95e-05\nepoch: 44 mean_loss=0.00137 train: errs=44 loss=0.0141 test: errs=16 loss=0.0207\nepoch: 45 mean_loss=0.000785 train: errs=1 loss=0.000493 test: errs=0 loss=4.46e-05\nepoch: 46 mean_loss=5.46e-05 train: errs=1 loss=0.000487 test: errs=0 loss=1.34e-05\nepoch: 47 mean_loss=1.99e-05 train: errs=1 loss=0.00033 test: errs=0 loss=1.57e-05\nepoch: 48 mean_loss=1.78e-05 train: errs=1 loss=0.000307 test: errs=0 loss=1.58e-05\nepoch: 49 mean_loss=0.000903 train: errs=1 loss=0.00103 test: errs=0 loss=0.000393\nepoch: 50 mean_loss=4.74e-05 train: errs=0 loss=4.63e-05 test: errs=0 loss=3.53e-05\nFinished Training, time: 234.69774420000002 sec\n\nThe images are 96*96 gray.  There are about 7000 training and 1750 test images. The order of presentation is random, and different at each epoch. Each image is either contains the object or not.  The architecture is (Conv2d->ReLU->BatchNorm2d->MaxPool)*4->AvgPool(6,6)->Flatten->Conv->Conv->Conv.  All MaxPool's are 2*2. First two Conv2d layers are 5*5, padding=2, others 3*3, padding=1.  The optimiser is like this:\nOptimizer= Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 0.001\n    weight_decay: 1e-05\n)\n\nCurrently I just choose the epoch when the training set error was minimal. \nif epoch == 0 or train_loss < train_loss_best:\n    net_best = copy.deepcopy(net)\n    train_loss_best = train_loss\n\nIt works, but I don't like it. Is there a way to make the learning more stable and steady?\n", "type": 1, "id": "13385", "date": "2019-07-16T08:23:12.440", "score": 2, "comment_count": 2, "tags": ["deep-learning", "deep-neural-networks", "learning-curve"], "title": "Spikes in of Train and Test error", "answer_count": 1, "views": 93, "accepted_answer": null, "answers": {"13396": {"line": 9222, "body": "I was actually very recently working on CNNs and extensively training models and have noticed the same thing. (NOTE: The answer I will give is purely based on empirical observations and my understanding of mathematics of deep learning).\nSo, the thing I observed on training set (since we are directly optimising on the training set) is that between period of low losses there was suddenly a large loss, and then again low losses, but, this time the loss (of training set) reached even more lower values and the accuracy of the test set reached a higher stable accuracy (by higher stable accuracy I mean, that in general I was observing the accuracy was somewhat oscillating around a fixed value of accuracy, and now the accuracy is oscillating around a higher fixed value of accuracy). From this I concluded that, the high loss was some sort of an obstacle which is stopping the weights from reaching global minima and is trapping it in local minima, unless it gains enough momentum to escape the obstacle, which is indicated by the high loss (think of the loss function as a rotated around y-axis sawtooth waveform inclined at  certain degree). So, as we reach lower loss it is expected (if the model is good) that you see better generalisation and hence the stable higher accuracy.\nYou can actually somewhat see this happening in your training too, although the data-set is smaller to make concrete comments.\nSeeing your results it seems pretty clear that training loss and test loss are going hand in hand as well as accuracy, which is a good sign, and it means your model has not over-fitted yet and you can train it more, which will probably make you encounter these loss spikes less, nevertheless I am pretty sure they will be there every now and then, whichever method you choose, as the loss curve is always a pretty jagged terrain for a DNN.\nI chose to disregard training accuracy, because most of the times it is not really related to the loss in a monotonically increasing way (check this thread), and the test loss is expected to increase since at such high accuracies, it has been empirically seen that test loss might decrease without affecting test accuracy (check this answer).\n", "type": 2, "id": "13396", "date": "2019-07-16T16:55:45.527", "score": 0, "comment_count": 0, "parent_id": "13385"}}}
{"line": 9419, "body": "I am working with the Inception ResNet V2 model, pre-trained with ImageNet, for face recognition.\nHowever, I'm so confused about what the exact output of the feature extraction layer (i.e. the layer just before the fully connected layer) of Inception ResNet V2 is. Can someone clarify exactly this?\n(By the way, if you know some resource that explains Inception ResNet V2 clearly, let me know).\n", "type": 1, "id": "13647", "date": "2019-07-29T11:25:56.983", "score": 2, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "architecture", "inception", "image-net"], "title": "What is the exact output of the Inception ResNet V2's feature extraction layer?", "answer_count": 2, "views": 1728, "accepted_answer": null, "answers": {"13667": {"line": 9438, "body": "Due to this article: https://arxiv.org/pdf/1512.00567v3.pdf?source=post_page--------------------------- , \nI try to flatten the 3-d tensor in to 1d vector: 8*8*2048, because in the article, the pool layer of inception resnet v2 at page 6 is Pool: 8 * 8 * 2048. \nBut at the end, my code showed the error:\n ValueError: cannot reshape array of size 33423360 into shape (340,131072)\nThis is all my code:\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import preprocess_input\nfrom keras.models import Model\nfrom keras.preprocessing.image import load_img\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\nfrom keras.applications import imagenet_utils\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import image\nimport random\nimport os\nimport numpy as np \nimport cv2\n\n\n# Path to image \nimage_path = list(paths.list_images('/content/drive/My Drive/casia-299-small'))\n# Random image path\nrandom.shuffle(image_path)\n# Get image name\nlabels = [p.split(os.path.sep)[-2] for p in image_path]\n\n\n# Encode face name in to number\nle = LabelEncoder()\nlabels = le.fit_transform(labels)\n\n# Load model inception v2, include_top = Fale to ignore Fully Connected layer\nmodel = InceptionResNetV2(include_top = False, weights = 'imagenet')\n\n\n# Load images and resize into required input size of Inception Resnet v2 299x299\nlist_image = []\nfor (j, imagePath) in enumerate(image_path):\n    image = load_img(imagePath, target_size = (299, 299, 3))\n    image = img_to_array(image)\n\n    image = np.expand_dims(image, 0)\n    image = imagenet_utils.preprocess_input(image)\n\n    list_image.append(image)\n\n# Use pre-trained model to extract feature\nlist_image = np.vstack(list_image)\nprint(\"LIst image: \", list_image)\nfeatures = model.predict(list_image)\nprint(\"feature: \", features)\nprint(\"feature shape[0]: \", features.shape[0])\nprint(\"feature shape: \", features.shape)\nfeatures = features.reshape((features.shape[0], 8*8*2048))\n\n# Split training set and test set n ratio of 80-20\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state =42)\n\nparams = {'C': [0.1, 1.0, 10.0, 100.0]}\nmodel = GridSearchCV(LogisticRegression(), params)\nmodel.fit(x_train,y_train)\nmodel.save('/content/drive/My Drive/casia-299-small/myweight1.h5')\nprint('Best parameter for the model {}'.format(model.best_params_))\n\npreds = model.predict(x_test)\nprint(classification_report(y_test, preds))\n```\n\n", "type": 2, "id": "13667", "date": "2019-07-30T02:28:14.813", "score": 0, "comment_count": 0, "parent_id": "13647"}, "16454": {"line": 10883, "body": "You can use this to view the Keras Resnet Inception V2 network.\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.layers import Input\nmodel = InceptionResNetV2(weights='imagenet', include_top=True)\nprint(model.summary())\n\nThis will Output (im showing only the last few layers):\n__________________________________________________________________________________________________\nconv_7b_ac (Activation)         (None, 8, 8, 1536)   0           conv_7b_bn[0][0]                 \n__________________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 1536)         0           conv_7b_ac[0][0]                 \n__________________________________________________________________________________________________\npredictions (Dense)             (None, 1000)         1537000     avg_pool[0][0]                   \n==================================================================================================\nTotal params: 55,873,736\nTrainable params: 55,813,192\nNon-trainable params: 60,544\n__________________________________________________________________________________________________\nNone\n\nIf we look at the output of the 'avg_pool' layer from 'Top'. There will be 1536 features at the output.\nYou can make a model in this way:\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.layers import Input\nimport numpy as np\ndef extract(image_path):\n    base_model = InceptionResNetV2(weights='imagenet', include_top=True)\n    model = Model(inputs=base_model.input,outputs=base_model.get_layer('avg_pool').output)\n\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n\n    # Get the prediction.\n    features = model.predict(x)    \n    features = features[0]\n    return features\n\n\nfeatures=[]\nfeatures = extract(image)\n\nI couldn't try the code as, right now, I don't have an environment to test this code.\n", "type": 2, "id": "16454", "date": "2019-11-12T13:25:14.710", "score": 0, "comment_count": 0, "parent_id": "13647"}}}
{"line": 8505, "body": "As a layman in AI, I want to get an idea of how big data players, like Facebook, model individuals (of which they have so many data).\nThere are two scenarios I can imagine:\n\nNeural networks build clusters of individuals by pure and \"unconscious\" big data analysis (not knowing, trying to understand and naming the clusters and \"feature neurons\" on intermediate levels of the network) with the only aim to predict some decisions of the members of these clusters with highest possible accuracy.\n\nLetting humans analyze the clusters and neurons (trying to understand what they mean) they give names to them and possibly add human-defined \"fields\" (like \"is an honest person\") if these were not found automatically, and whose values are then calculated from big data.\n\n\nThe second case would result in a specific psychological model of individuals with lots of \"human-understandable\" dimensions.\nIn case there is such a model, I would be very interested to know as much about it as possible.\nWhat can be said about this:\n\n\nIs there most probably such a model (that is kept as a secret e.g. by Facebook)?\n\nHas somehow tried to guess how it may look like?\n\nAre there leaked parts of the model?\n\n\n\nMy aim is to know and understand by which categories Facebook (as an example) classifies its users.\n", "type": 1, "id": "12470", "date": "2019-05-22T11:54:54.033", "score": 5, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "social"], "title": "How do big companies, like Facebook, model individuals and their interaction?", "answer_count": 1, "views": 155, "accepted_answer": null, "answers": {"12610": {"line": 8608, "body": "This article may shed some light on this question:\nFacebook Doesn't Tell Users Everything It Really Knows About Them\n", "type": 2, "id": "12610", "date": "2019-05-31T11:03:51.513", "score": 0, "comment_count": 1, "parent_id": "12470"}}}
{"line": 11273, "body": "During model training, I noticed various behaviour in between training and validation accuracy. I understand that 'The training set is used to train the model, while the validation set is only used to evaluate the model's performance...', but I'd like to know if there is any relationship between training and validation accuracy and if yes, \n1) what exactly is happening when training and validation accuracy change during training and;\n2) what do different behaviours imply\nFor instance, some believe there is overfitting problem if training > validation accuracy. What happens if one is greater than the other alternately, which is the case below? \nHere is the code\ninputs_1 = keras.Input(shape=(10081,1))\n\nlayer1 = Conv1D(64,14)(inputs_1)\nlayer2 = layers.MaxPool1D(5)(layer1)\nlayer3 = Conv1D(64, 14)(layer2)\nlayer4 = layers.GlobalMaxPooling1D()(layer3)\n\ninputs_2 = keras.Input(shape=(104,))             \nlayer5 = layers.concatenate([layer4, inputs_2])\nlayer6 = Dense(128, activation='relu')(layer5)\nlayer7 = Dense(2, activation='softmax')(layer6)\n\n\nmodel_2 = keras.models.Model(inputs = [inputs_1, inputs_2], output = [layer7])\nmodel_2.summary()\n\n\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:10185], df[['Result_cat','Result_cat1']].values, test_size=0.2) \nX_train = X_train.to_numpy()\nX_train = X_train.reshape([X_train.shape[0], X_train.shape[1], 1]) \nX_train_1 = X_train[:,0:10081,:]\nX_train_2 = X_train[:,10081:10185,:].reshape(736,104)   \n\n\nX_test = X_test.to_numpy()\nX_test = X_test.reshape([X_test.shape[0], X_test.shape[1], 1]) \nX_test_1 = X_test[:,0:10081,:]\nX_test_2 = X_test[:,10081:10185,:].reshape(185,104)    \n\nadam = keras.optimizers.Adam(lr = 0.0005)\nmodel_2.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['acc'])\n\nhistory = model_2.fit([X_train_1,X_train_2], y_train, epochs = 120, batch_size = 256, validation_split = 0.2, callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)])\n\nmodel summary\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n  from ipykernel import kernelapp as app\nModel: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            (None, 10081, 1)     0                                            \n__________________________________________________________________________________________________\nconv1d_5 (Conv1D)               (None, 10068, 64)    960         input_5[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling1d_3 (MaxPooling1D)  (None, 2013, 64)     0           conv1d_5[0][0]                   \n__________________________________________________________________________________________________\nconv1d_6 (Conv1D)               (None, 2000, 64)     57408       max_pooling1d_3[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_6[0][0]                   \n__________________________________________________________________________________________________\ninput_6 (InputLayer)            (None, 104)          0                                            \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 168)          0           global_max_pooling1d_3[0][0]     \n                                                                 input_6[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 128)          21632       concatenate_3[0][0]              \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 2)            258         dense_5[0][0]                    \n==================================================================================================\nTotal params: 80,258\nTrainable params: 80,258\nNon-trainable params: 0\n\nand the training process\n__________________________________________________________________________________________________\nTrain on 588 samples, validate on 148 samples\nEpoch 1/120\n588/588 [==============================] - 16s 26ms/step - loss: 5.6355 - acc: 0.4932 - val_loss: 4.1086 - val_acc: 0.6216\nEpoch 2/120\n588/588 [==============================] - 15s 25ms/step - loss: 4.5977 - acc: 0.5748 - val_loss: 3.8252 - val_acc: 0.4459\nEpoch 3/120\n588/588 [==============================] - 15s 25ms/step - loss: 4.3815 - acc: 0.4575 - val_loss: 2.4087 - val_acc: 0.6622\nEpoch 4/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.7480 - acc: 0.6003 - val_loss: 2.0060 - val_acc: 0.6892\nEpoch 5/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.3019 - acc: 0.5408 - val_loss: 2.3176 - val_acc: 0.5676\nEpoch 6/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.1739 - acc: 0.5663 - val_loss: 2.2607 - val_acc: 0.6892\nEpoch 7/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.2322 - acc: 0.6207 - val_loss: 1.8898 - val_acc: 0.7230\nEpoch 8/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.9777 - acc: 0.6020 - val_loss: 1.8401 - val_acc: 0.7500\nEpoch 9/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.8982 - acc: 0.6429 - val_loss: 1.8517 - val_acc: 0.7365\nEpoch 10/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.8342 - acc: 0.6344 - val_loss: 1.7941 - val_acc: 0.7095\nEpoch 11/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7426 - acc: 0.6327 - val_loss: 1.8495 - val_acc: 0.7162\nEpoch 12/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7340 - acc: 0.6531 - val_loss: 1.7652 - val_acc: 0.7162\nEpoch 13/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6680 - acc: 0.6616 - val_loss: 1.8097 - val_acc: 0.7365\nEpoch 14/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6922 - acc: 0.6786 - val_loss: 1.7143 - val_acc: 0.7500\nEpoch 15/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6161 - acc: 0.6786 - val_loss: 1.6960 - val_acc: 0.7568\nEpoch 16/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6054 - acc: 0.6905 - val_loss: 1.6779 - val_acc: 0.7297\nEpoch 17/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6072 - acc: 0.6684 - val_loss: 1.6750 - val_acc: 0.7703\nEpoch 18/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5907 - acc: 0.6871 - val_loss: 1.6774 - val_acc: 0.7432\nEpoch 19/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5854 - acc: 0.6718 - val_loss: 1.6609 - val_acc: 0.7770\nEpoch 20/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5621 - acc: 0.6905 - val_loss: 1.6709 - val_acc: 0.7365\nEpoch 21/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5515 - acc: 0.6854 - val_loss: 1.6904 - val_acc: 0.7703\nEpoch 22/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5749 - acc: 0.6837 - val_loss: 1.6862 - val_acc: 0.7297\nEpoch 23/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6212 - acc: 0.6514 - val_loss: 1.7215 - val_acc: 0.7568\nEpoch 24/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6532 - acc: 0.6633 - val_loss: 1.7105 - val_acc: 0.7230\nEpoch 25/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7300 - acc: 0.6344 - val_loss: 1.6870 - val_acc: 0.7432\nEpoch 26/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7355 - acc: 0.6650 - val_loss: 1.6733 - val_acc: 0.7703\nEpoch 27/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6336 - acc: 0.6650 - val_loss: 1.6572 - val_acc: 0.7297\nEpoch 28/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6018 - acc: 0.6803 - val_loss: 1.7292 - val_acc: 0.7635\nEpoch 29/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5448 - acc: 0.7143 - val_loss: 1.8065 - val_acc: 0.7095\nEpoch 30/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5724 - acc: 0.6820 - val_loss: 1.8029 - val_acc: 0.7297\nEpoch 31/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6622 - acc: 0.6650 - val_loss: 1.6594 - val_acc: 0.7568\nEpoch 32/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6211 - acc: 0.6582 - val_loss: 1.6375 - val_acc: 0.7770\nEpoch 33/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5911 - acc: 0.6854 - val_loss: 1.6964 - val_acc: 0.7500\nEpoch 34/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5050 - acc: 0.7262 - val_loss: 1.8496 - val_acc: 0.6892\nEpoch 35/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6012 - acc: 0.6752 - val_loss: 1.7443 - val_acc: 0.7432\nEpoch 36/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5688 - acc: 0.6871 - val_loss: 1.6220 - val_acc: 0.7568\nEpoch 37/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4843 - acc: 0.7279 - val_loss: 1.6166 - val_acc: 0.7905\nEpoch 38/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4707 - acc: 0.7449 - val_loss: 1.6496 - val_acc: 0.7905\nEpoch 39/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4683 - acc: 0.7109 - val_loss: 1.6641 - val_acc: 0.7432\nEpoch 40/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4671 - acc: 0.7279 - val_loss: 1.6553 - val_acc: 0.7703\nEpoch 41/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4479 - acc: 0.7347 - val_loss: 1.6302 - val_acc: 0.7973\nEpoch 42/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4355 - acc: 0.7551 - val_loss: 1.6241 - val_acc: 0.7973\nEpoch 43/120\n588/588 [==============================] - 14s 25ms/step - loss: 2.4286 - acc: 0.7568 - val_loss: 1.6249 - val_acc: 0.7973\nEpoch 44/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4250 - acc: 0.7585 - val_loss: 1.6248 - val_acc: 0.7770\nEpoch 45/120\n588/588 [==============================] - 14s 25ms/step - loss: 2.4198 - acc: 0.7517 - val_loss: 1.6212 - val_acc: 0.7703\nEpoch 46/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4246 - acc: 0.7568 - val_loss: 1.6129 - val_acc: 0.7838\nEpoch 47/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4237 - acc: 0.7517 - val_loss: 1.6166 - val_acc: 0.7973\nEpoch 48/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4287 - acc: 0.7432 - val_loss: 1.6309 - val_acc: 0.8041\nEpoch 49/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4179 - acc: 0.7381 - val_loss: 1.6271 - val_acc: 0.7838\nEpoch 50/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4164 - acc: 0.7381 - val_loss: 1.6258 - val_acc: 0.7973\nEpoch 51/120\n588/588 [==============================] - 14s 24ms/step - loss: 2.1996 - acc: 0.7398 - val_loss: 1.3612 - val_acc: 0.7973\nEpoch 52/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1387 - acc: 0.8265 - val_loss: 1.4811 - val_acc: 0.7973\nEpoch 53/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1607 - acc: 0.8078 - val_loss: 1.5060 - val_acc: 0.7838\nEpoch 54/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1783 - acc: 0.8129 - val_loss: 1.4878 - val_acc: 0.8176\nEpoch 55/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1745 - acc: 0.8197 - val_loss: 1.4762 - val_acc: 0.8108\nEpoch 56/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1764 - acc: 0.8129 - val_loss: 1.4631 - val_acc: 0.7905\nEpoch 57/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1637 - acc: 0.8078 - val_loss: 1.4615 - val_acc: 0.7770\nEpoch 58/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1563 - acc: 0.8112 - val_loss: 1.4487 - val_acc: 0.7703\nEpoch 59/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1396 - acc: 0.8146 - val_loss: 1.4362 - val_acc: 0.7905\nEpoch 60/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1240 - acc: 0.8316 - val_loss: 1.4333 - val_acc: 0.8041\nEpoch 61/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1173 - acc: 0.8333 - val_loss: 1.4369 - val_acc: 0.8041\nEpoch 62/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1228 - acc: 0.8384 - val_loss: 1.4393 - val_acc: 0.8041\nEpoch 63/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1113 - acc: 0.8316 - val_loss: 1.4380 - val_acc: 0.8041\nEpoch 64/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1102 - acc: 0.8452 - val_loss: 1.4217 - val_acc: 0.8041\nEpoch 65/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0961 - acc: 0.8469 - val_loss: 1.4129 - val_acc: 0.7973\nEpoch 66/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0903 - acc: 0.8537 - val_loss: 1.4019 - val_acc: 0.8041\nEpoch 67/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0890 - acc: 0.8503 - val_loss: 1.3850 - val_acc: 0.8176\nEpoch 68/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0878 - acc: 0.8520 - val_loss: 1.4035 - val_acc: 0.7635\nEpoch 69/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0984 - acc: 0.8469 - val_loss: 1.4060 - val_acc: 0.8041\nEpoch 70/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0893 - acc: 0.8418 - val_loss: 1.3981 - val_acc: 0.7973\nEpoch 71/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0876 - acc: 0.8605 - val_loss: 1.3951 - val_acc: 0.8041__________________________________________________________________________________________________\nTrain on 588 samples, validate on 148 samples\nEpoch 1/120\n588/588 [==============================] - 16s 26ms/step - loss: 5.6355 - acc: 0.4932 - val_loss: 4.1086 - val_acc: 0.6216\nEpoch 2/120\n588/588 [==============================] - 15s 25ms/step - loss: 4.5977 - acc: 0.5748 - val_loss: 3.8252 - val_acc: 0.4459\nEpoch 3/120\n588/588 [==============================] - 15s 25ms/step - loss: 4.3815 - acc: 0.4575 - val_loss: 2.4087 - val_acc: 0.6622\nEpoch 4/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.7480 - acc: 0.6003 - val_loss: 2.0060 - val_acc: 0.6892\nEpoch 5/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.3019 - acc: 0.5408 - val_loss: 2.3176 - val_acc: 0.5676\nEpoch 6/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.1739 - acc: 0.5663 - val_loss: 2.2607 - val_acc: 0.6892\nEpoch 7/120\n588/588 [==============================] - 15s 25ms/step - loss: 3.2322 - acc: 0.6207 - val_loss: 1.8898 - val_acc: 0.7230\nEpoch 8/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.9777 - acc: 0.6020 - val_loss: 1.8401 - val_acc: 0.7500\nEpoch 9/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.8982 - acc: 0.6429 - val_loss: 1.8517 - val_acc: 0.7365\nEpoch 10/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.8342 - acc: 0.6344 - val_loss: 1.7941 - val_acc: 0.7095\nEpoch 11/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7426 - acc: 0.6327 - val_loss: 1.8495 - val_acc: 0.7162\nEpoch 12/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7340 - acc: 0.6531 - val_loss: 1.7652 - val_acc: 0.7162\nEpoch 13/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6680 - acc: 0.6616 - val_loss: 1.8097 - val_acc: 0.7365\nEpoch 14/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6922 - acc: 0.6786 - val_loss: 1.7143 - val_acc: 0.7500\nEpoch 15/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6161 - acc: 0.6786 - val_loss: 1.6960 - val_acc: 0.7568\nEpoch 16/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6054 - acc: 0.6905 - val_loss: 1.6779 - val_acc: 0.7297\nEpoch 17/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6072 - acc: 0.6684 - val_loss: 1.6750 - val_acc: 0.7703\nEpoch 18/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5907 - acc: 0.6871 - val_loss: 1.6774 - val_acc: 0.7432\nEpoch 19/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5854 - acc: 0.6718 - val_loss: 1.6609 - val_acc: 0.7770\nEpoch 20/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5621 - acc: 0.6905 - val_loss: 1.6709 - val_acc: 0.7365\nEpoch 21/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5515 - acc: 0.6854 - val_loss: 1.6904 - val_acc: 0.7703\nEpoch 22/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5749 - acc: 0.6837 - val_loss: 1.6862 - val_acc: 0.7297\nEpoch 23/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6212 - acc: 0.6514 - val_loss: 1.7215 - val_acc: 0.7568\nEpoch 24/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6532 - acc: 0.6633 - val_loss: 1.7105 - val_acc: 0.7230\nEpoch 25/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7300 - acc: 0.6344 - val_loss: 1.6870 - val_acc: 0.7432\nEpoch 26/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.7355 - acc: 0.6650 - val_loss: 1.6733 - val_acc: 0.7703\nEpoch 27/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6336 - acc: 0.6650 - val_loss: 1.6572 - val_acc: 0.7297\nEpoch 28/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6018 - acc: 0.6803 - val_loss: 1.7292 - val_acc: 0.7635\nEpoch 29/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5448 - acc: 0.7143 - val_loss: 1.8065 - val_acc: 0.7095\nEpoch 30/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5724 - acc: 0.6820 - val_loss: 1.8029 - val_acc: 0.7297\nEpoch 31/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6622 - acc: 0.6650 - val_loss: 1.6594 - val_acc: 0.7568\nEpoch 32/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6211 - acc: 0.6582 - val_loss: 1.6375 - val_acc: 0.7770\nEpoch 33/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5911 - acc: 0.6854 - val_loss: 1.6964 - val_acc: 0.7500\nEpoch 34/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5050 - acc: 0.7262 - val_loss: 1.8496 - val_acc: 0.6892\nEpoch 35/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.6012 - acc: 0.6752 - val_loss: 1.7443 - val_acc: 0.7432\nEpoch 36/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.5688 - acc: 0.6871 - val_loss: 1.6220 - val_acc: 0.7568\nEpoch 37/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4843 - acc: 0.7279 - val_loss: 1.6166 - val_acc: 0.7905\nEpoch 38/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4707 - acc: 0.7449 - val_loss: 1.6496 - val_acc: 0.7905\nEpoch 39/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4683 - acc: 0.7109 - val_loss: 1.6641 - val_acc: 0.7432\nEpoch 40/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4671 - acc: 0.7279 - val_loss: 1.6553 - val_acc: 0.7703\nEpoch 41/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4479 - acc: 0.7347 - val_loss: 1.6302 - val_acc: 0.7973\nEpoch 42/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4355 - acc: 0.7551 - val_loss: 1.6241 - val_acc: 0.7973\nEpoch 43/120\n588/588 [==============================] - 14s 25ms/step - loss: 2.4286 - acc: 0.7568 - val_loss: 1.6249 - val_acc: 0.7973\nEpoch 44/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4250 - acc: 0.7585 - val_loss: 1.6248 - val_acc: 0.7770\nEpoch 45/120\n588/588 [==============================] - 14s 25ms/step - loss: 2.4198 - acc: 0.7517 - val_loss: 1.6212 - val_acc: 0.7703\nEpoch 46/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4246 - acc: 0.7568 - val_loss: 1.6129 - val_acc: 0.7838\nEpoch 47/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4237 - acc: 0.7517 - val_loss: 1.6166 - val_acc: 0.7973\nEpoch 48/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4287 - acc: 0.7432 - val_loss: 1.6309 - val_acc: 0.8041\nEpoch 49/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4179 - acc: 0.7381 - val_loss: 1.6271 - val_acc: 0.7838\nEpoch 50/120\n588/588 [==============================] - 15s 25ms/step - loss: 2.4164 - acc: 0.7381 - val_loss: 1.6258 - val_acc: 0.7973\nEpoch 51/120\n588/588 [==============================] - 14s 24ms/step - loss: 2.1996 - acc: 0.7398 - val_loss: 1.3612 - val_acc: 0.7973\nEpoch 52/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1387 - acc: 0.8265 - val_loss: 1.4811 - val_acc: 0.7973\nEpoch 53/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1607 - acc: 0.8078 - val_loss: 1.5060 - val_acc: 0.7838\nEpoch 54/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1783 - acc: 0.8129 - val_loss: 1.4878 - val_acc: 0.8176\nEpoch 55/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1745 - acc: 0.8197 - val_loss: 1.4762 - val_acc: 0.8108\nEpoch 56/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1764 - acc: 0.8129 - val_loss: 1.4631 - val_acc: 0.7905\nEpoch 57/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1637 - acc: 0.8078 - val_loss: 1.4615 - val_acc: 0.7770\nEpoch 58/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1563 - acc: 0.8112 - val_loss: 1.4487 - val_acc: 0.7703\nEpoch 59/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1396 - acc: 0.8146 - val_loss: 1.4362 - val_acc: 0.7905\nEpoch 60/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1240 - acc: 0.8316 - val_loss: 1.4333 - val_acc: 0.8041\nEpoch 61/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1173 - acc: 0.8333 - val_loss: 1.4369 - val_acc: 0.8041\nEpoch 62/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1228 - acc: 0.8384 - val_loss: 1.4393 - val_acc: 0.8041\nEpoch 63/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1113 - acc: 0.8316 - val_loss: 1.4380 - val_acc: 0.8041\nEpoch 64/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.1102 - acc: 0.8452 - val_loss: 1.4217 - val_acc: 0.8041\nEpoch 65/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0961 - acc: 0.8469 - val_loss: 1.4129 - val_acc: 0.7973\nEpoch 66/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0903 - acc: 0.8537 - val_loss: 1.4019 - val_acc: 0.8041\nEpoch 67/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0890 - acc: 0.8503 - val_loss: 1.3850 - val_acc: 0.8176\nEpoch 68/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0878 - acc: 0.8520 - val_loss: 1.4035 - val_acc: 0.7635\nEpoch 69/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0984 - acc: 0.8469 - val_loss: 1.4060 - val_acc: 0.8041\nEpoch 70/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0893 - acc: 0.8418 - val_loss: 1.3981 - val_acc: 0.7973\nEpoch 71/120\n588/588 [==============================] - 15s 25ms/step - loss: 1.0876 - acc: 0.8605 - val_loss: 1.3951 - val_acc: 0.8041\n\nNotice how at first acc is lower than val_acc and later is greater than val_acc. Can someone please shed some light what could be happening here? Thank you\n", "type": 1, "id": "16914", "date": "2019-12-03T23:30:52.757", "score": 3, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks", "training", "cross-validation"], "title": "Relationship between training accuracy and validation accuracy", "answer_count": 1, "views": 186, "accepted_answer": "16925", "answers": {"16925": {"line": 11283, "body": "very interesting questions:\n1) what exactly is happening when training and validation accuracy change during training\n\nThe accuracy change after every batch computation. You have 588 batches, so loss will be computed after each one of these batches (let's say each batch have 8 images). However, the accuracy you see in the progress bar it is the accuracy of the current batch averaged with the accuracy of all the previous batches so far. See keras.utils.generic_utils.Progbar.\nThe val_acc is computed only at the end of one epoch and it is computed with all your validation dataset at once (considering it as a single batch, so if you have 100 images for validation it will compute accuracy as a single batch of 100 images)\n\n2) what do different behaviours imply\n\nThe acc and val_acc normally differ from each other due to different split sizes. \n\nTry same experiment with validation_split=0.01 and validation_split=0.4 and you will see how both accuracy and val_acc will change. \nNormally the greater the validation split, the more similar both metrics will be since the validation split will be big enough to be representative (let's say it has cats and dogs, not only cats), taking into account that you need enough data to train correctly. This explains why in some cases the val_acc is higher than accuracy and vice versa.\n\nOverfitting only occurs when the graph fashion or tendency changes and val_acc starts to drop and accuracy keeping increasing. This means that your model can not do any better with the validation dataset (non previously seen images).\n\nI work with loss and val_loss which are highly correlated with accuracy. Normally the loss is the inverse, so interpret the comments above in the inverse sense (sorry about the confusion but I'm taking this example from my current experiments) I hope it helps:\n\nThere are 2 experiments, orange and grey. \n\nIn both experiments, val_loss is always slightly higher than loss (because of my current validation split which it happens to be also 0.2, but normally is 0.01 and val_loss is even higher).\nOn both experiments the loss trend is linearly decreasing, this is because gradient descent works and the loss functions is well defined and it converges.\nOrange experiment is overfitting from epoch 20 onwards because, the val_loss won't drop any more and, on the contrary, it start increasing.\nGrey experiment is just right, both loss and val_loss are still decreasing, and although the val_loss might be greater than loss it is not overfitting because it is still decreasing. So that is why it is still training :) \n\nComplex concepts here, I hope I was able to explain myself clearly! Cheers\n", "type": 2, "id": "16925", "date": "2019-12-04T10:18:26.693", "score": 0, "comment_count": 4, "parent_id": "16914"}}}
{"line": 10151, "body": "I can do text classification with RNN, in which the last output of RNN (rnn_outputs[-1]) is used to matmul with output layer weight and plus bias. That is getting a word (class name) after the last T in the time dimension of RNN.\nThe matter is for text generation, I need a word somewhere in the middle of time dimension, eg.:\nt0  t1    t2  t3\nThe brown fox jumps\n\nFor this example, I have the first 2 words: The, brown.\nHow to get the next word ie. \"fox\" using RNN (LSTM)? How to convert the following text classification code to text generating code?\nSource code (text classification):\nimport tensorflow as tf;\ntf.reset_default_graph();\n\n#data\n'''\nt0      t1      t2\nbritish gray    is => cat (y=0)\n0       1       2\nwhite   samoyed is => dog (y=1)\n3       4       2 \n'''\nBsize = 2;\nTimes = 3;\nMax_X = 4;\nMax_Y = 1;\n\nX = [[[0],[1],[2]], [[3],[4],[2]]];\nY = [[0],           [1]          ];\n\n#normalise\nfor I in range(len(X)):\n  for J in range(len(X[I])):\n    X[I][J][0] /= Max_X;\n\nfor I in range(len(Y)):\n  Y[I][0] /= Max_Y;\n\n#model\nInputs   = tf.placeholder(tf.float32, [Bsize,Times,1]);\nExpected = tf.placeholder(tf.float32, [Bsize,      1]);\n\n#single LSTM layer\n#'''\nLayer1   = tf.keras.layers.LSTM(20);\nHidden1  = Layer1(Inputs);\n#'''\n\n#multi LSTM layers\n'''\nLayers = tf.keras.layers.RNN([\n  tf.keras.layers.LSTMCell(30), #hidden 1\n  tf.keras.layers.LSTMCell(20)  #hidden 2\n]);\nHidden2 = Layers(Inputs);\n'''\n\nWeight3  = tf.Variable(tf.random_uniform([20,1], -1,1));\nBias3    = tf.Variable(tf.random_uniform([   1], -1,1));\nOutput   = tf.sigmoid(tf.matmul(Hidden1,Weight3) + Bias3);\n\nLoss     = tf.reduce_sum(tf.square(Expected-Output));\nOptim    = tf.train.GradientDescentOptimizer(1e-1);\nTraining = Optim.minimize(Loss);\n\n#train\nSess = tf.Session();\nInit = tf.global_variables_initializer();\nSess.run(Init);\n\nFeed = {Inputs:X, Expected:Y};\nfor I in range(1000): #number of feeds, 1 feed = 1 batch\n  if I%100==0: \n    Lossvalue = Sess.run(Loss,Feed);\n    print(\"Loss:\",Lossvalue);\n  #end if\n\n  Sess.run(Training,Feed);\n#end for\n\nLastloss = Sess.run(Loss,Feed);\nprint(\"Loss:\",Lastloss,\"(Last)\");\n\n#eval\nResults = Sess.run(Output,Feed);\nprint(\"\\nEval:\");\nprint(Results);\n\nprint(\"\\nDone.\");\n#eof\n\n", "type": 1, "id": "15562", "date": "2019-09-20T15:57:15.957", "score": 0, "comment_count": 0, "tags": ["natural-language-processing", "classification", "tensorflow", "recurrent-neural-networks", "generative-model"], "title": "How to change this RNN text classification code to become text generation code?", "answer_count": 1, "views": 27, "accepted_answer": "15602", "answers": {"15602": {"line": 10175, "body": "I found out how to switch it (the code) to do text generation task, use 3D input (X) and 3D labels (Y) as in the source code below:\nSource code:\nimport tensorflow as tf;\ntf.reset_default_graph();\n\n#data\n'''\nt0       t1       t2\nbritish  gray     is  cat\n0        1        2   (3)  <=x\n1        2        3        <=y\nwhite    samoyed  is  dog\n4        5        2   (6)  <=x\n5        2        6        <=y \n'''\nBsize = 2;\nTimes = 3;\nMax_X = 5;\nMax_Y = 6;\n\nX = [[[0],[1],[2]], [[4],[5],[2]]];\nY = [[[1],[2],[3]], [[5],[2],[6]]];\n\n#normalise\nfor I in range(len(X)):\n  for J in range(len(X[I])):\n    X[I][J][0] /= Max_X;\n\nfor I in range(len(Y)):\n  for J in range(len(Y[I])):\n    Y[I][J][0] /= Max_Y;\n\n#model\nInput    = tf.placeholder(tf.float32, [Bsize,Times,1]);\nExpected = tf.placeholder(tf.float32, [Bsize,Times,1]);\n\n#single LSTM layer\n'''\nLayer1   = tf.keras.layers.LSTM(20);\nHidden1  = Layer1(Input);\n'''\n\n#multi LSTM layers\n#'''\nLayers = tf.keras.layers.RNN([\n  tf.keras.layers.LSTMCell(30), #hidden 1\n  tf.keras.layers.LSTMCell(20)  #hidden 2\n],\nreturn_sequences=True);\nHidden2 = Layers(Input);\n#'''\n\nWeight3  = tf.Variable(tf.random_uniform([20,1], -1,1));\nBias3    = tf.Variable(tf.random_uniform([   1], -1,1));\nOutput   = tf.sigmoid(tf.matmul(Hidden2,Weight3) + Bias3); #sequence of 2d * 2d\n\nLoss     = tf.reduce_sum(tf.square(Expected-Output));\nOptim    = tf.train.GradientDescentOptimizer(1e-1);\nTraining = Optim.minimize(Loss);\n\n#train\nSess = tf.Session();\nInit = tf.global_variables_initializer();\nSess.run(Init);\n\nFeed   = {Input:X, Expected:Y};\nEpochs = 10000;\n\nfor I in range(Epochs): #number of feeds, 1 feed = 1 batch\n  if I%(Epochs/10)==0: \n    Lossvalue = Sess.run(Loss,Feed);\n    print(\"Loss:\",Lossvalue);\n  #end if\n\n  Sess.run(Training,Feed);\n#end for\n\nLastloss = Sess.run(Loss,Feed);\nprint(\"Loss:\",Lastloss,\"(Last)\");\n\n#eval\nResults = Sess.run(Output,Feed).tolist();\nprint(\"\\nEval:\");\nfor I in range(len(Results)):\n  for J in range(len(Results[I])):\n    for K in range(len(Results[I][J])):\n      Results[I][J][K] = round(Results[I][J][K]*Max_Y);\n#end for i      \nprint(Results);\n\nprint(\"\\nDone.\");\n#eof\n\n", "type": 2, "id": "15602", "date": "2019-09-23T21:57:04.337", "score": 0, "comment_count": 0, "parent_id": "15562"}}}
{"line": 9953, "body": "I'm currently a student learning about AI Networks. I've came across a statement in one of my Professor's books that a FFBP (Feed-Forward Back-Propagation) Neural Network with a single hidden layer  can model any mathematic function with accuracy dependant on number of hidden layer neurons. Try as I might I cannot find any explanation as to why that occurs - could someone maybe explain the question why that is?\n", "type": 1, "id": "14320", "date": "2019-09-07T13:12:27.857", "score": 1, "comment_count": 1, "tags": ["neural-networks", "backpropagation", "hidden-layers", "feedforward-neural-networks"], "title": "How does a single neuron in hidden layer affect training accuracy", "answer_count": 1, "views": 71, "accepted_answer": null, "answers": {"24667": {"line": 16584, "body": "The claim that Neural Network with a single hidden layer can model any functions is proven in Cybenko's Approximation by superpositions of a sigmoidal function.\nhttps://link.springer.com/article/10.1007/BF02551274\ncheck also: https://en.wikipedia.org/wiki/Universal_approximation_theorem\nThe thing is that the neural network using sigmoidal functions, which are non-linear functions can.\n", "type": 2, "id": "24667", "date": "2020-11-17T12:55:04.903", "score": 0, "comment_count": 0, "parent_id": "14320"}}}
{"line": 9995, "body": "A single neuron will be able to do linear separation. For example, XOR simulator network:\nx1 --- n1.1\n   \\  /    \\\n    \\/      \\\n             n2.1 \n    /\\      /\n   /  \\    /\nx2 --- n1.2 \n\nWhere x1, x2 are the 2 inputs, n1.1 and n1.2 are the 2 neurons in hidden layer, and n2.1 is the output neuron.\nThe output neuron n2.1 does a linear separation. How about the 2 neurons in hidden layer? \nIs it still called linear separation (at 2 nodes and join the 2 separation lines)? or polynomial separation of degree 2?\nI'm confused about how it's called because there are curvy lines in this wiki article: https://en.wikipedia.org/wiki/Overfitting\n\n\n", "type": 1, "id": "15368", "date": "2019-09-11T07:11:04.483", "score": 0, "comment_count": 0, "tags": ["deep-learning", "math", "deep-neural-networks", "artificial-neuron", "linear-regression"], "title": "Is it still called linear separation with a layer of more than 1 neuron", "answer_count": 2, "views": 58, "accepted_answer": "15419", "answers": {"15369": {"line": 9996, "body": "What you have depicted is a nonlinear classificator. Although each stage does a linear separation, the sequential composition of linear separations is nonlinear. The nonlinearity of the neuron is key in this regard, as otherwise it would all be equivalent to a matrix multiplication, which is linear. You were right about the degree, although it's rarely called like that. People usually describe just the number of layers, and I guess the main reason is that it's not directly equivalent to such polynomials, as that depends on other factors (e.g. the activation function).\n", "type": 2, "id": "15369", "date": "2019-09-11T07:26:47.480", "score": 0, "comment_count": 0, "parent_id": "15368"}, "15419": {"line": 10039, "body": "I found out the curvy zigzag green line is not polynomial as if it were polynomial, a vertical line won't cut that curvy line more than 1 time. \nIt's the combination of straight lines (linear separation) of multiple neurons in the same layer. So it's linear separation ('linear' by previous_layer_output*weight, 'separation' by activation function), at multiple nodes.\n", "type": 2, "id": "15419", "date": "2019-09-13T10:31:15.020", "score": 0, "comment_count": 0, "parent_id": "15368"}}}
{"line": 10234, "body": "I'm interested in learning about Neural Networks and implementing them. I'm particularly interested in GANs and LSTM networks.\nI understand perceptrons and basic Neural Network configuration (sigmoid activation, weights, hidden layers etc). But what topics do I need to learn in order, to get to the point where I can implement GAN or LSTM.\nI intend to make an implementation of each in C++ to prove to myself that I understand. I haven't got a particularly good math background, but I understand most math-things when they are explained.\nFor example, I understand backpropagation, but I don't really understand it. I understand how reinforced learning is used with backpropagation, but not fully how you can have things like training without datasets (like tD-backgammon). I don't quite understand CNNs, especially why you might make a particular architecture.\nIf for each \"topic\" there was a book or website or something for each it would be great.\n", "type": 1, "id": "15683", "date": "2019-10-01T20:59:28.153", "score": 1, "comment_count": 8, "tags": ["neural-networks"], "title": "What order should I learn about Neural Networks?", "answer_count": 2, "views": 114, "accepted_answer": null, "answers": {"16306": {"line": 10756, "body": "Would personally recommend deeplearning.ai's course to begin with. There may be more comprehensive or better MOOC's for covering basic MLP's, CNN, RNN's, tuning and training of neural networks but this is probably the most common one and the one that I can personally vouch for.\nAfter this I'd recommend you get a physical or pdf copy of Deep Learning by Goodfellow et al. and use it as reference material for any new idea you'd want to learn. Personally would not recommend reading the whole book and its better as a reference material as it is quite comprehensive. \nThis should essentially be able to give you enough knowledge to be able to cover almost any paper/material on deep learning. The Course mentioned (most courses) would cover LSTM's as they are quite an old idea (~1997 I think) and GAN's are well covered in the book mentioned (The author invented them) since they are more of a recent advancement (2014). \nHope this was helpful! \n", "type": 2, "id": "16306", "date": "2019-11-06T12:06:55.977", "score": 0, "comment_count": 0, "parent_id": "15683"}, "15736": {"line": 10271, "body": "I think, once you are covered with the common stuff, you can probably go on and study all kinds of neural network variants. \n\nThe common stuff:\na) An undergraduate level Linear Algebra course -- covering matrix calculus. You might find this useful.\nb) An undergraduate level study in statistical inference. Concepts from this topic will come up most of the time and you might have hard time getting around even though you understand the rest of the math. I would recommend this.\nc) A starter book on neural networks. Ex- Neural networks by Raul Rojas.\n\nAfter all these are covered you will certainly be ready for learning the variants of neural networks with ease. For LSTM I would recommend Alex Graves. \n", "type": 2, "id": "15736", "date": "2019-10-05T12:36:49.023", "score": 1, "comment_count": 0, "parent_id": "15683"}}}
{"line": 11452, "body": "How can we train a competitive layer on non-normalized vectors using LVQ technique ?\nan example is given below from Neural Network Design (2nd Edition) book\nThe net input expression for LVQ networks calculates the distance between the input\nand each weight vector directly, instead of using the inner product. The result is that the\nLVQ network does not require normalized input vectors. This technique can also be\nused to allow a competitive layer to classify non-normalized vectors. Such a network is\nshown in figure below.\n\nUse this technique to train a two-neuron competitive layer on the (non-normalized)\nvectors below, using a learning rate $\\alpha=0.5$\n$p_1=\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}, p_2=\\begin{bmatrix}\n-1 \\\\\n2\n\\end{bmatrix}, p_3=\\begin{bmatrix}\n-2 \\\\\n-2\n\\end{bmatrix}$\nPresent the vectors in the following order : $p_1, p_2, p_3, p_2, p_3, p_1$\nInitial weights : $W_1=\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}, W_2=\\begin{bmatrix}\n1 \\\\\no\n\\end{bmatrix}$\n", "type": 1, "id": "17126", "date": "2019-12-16T12:43:30.297", "score": 2, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks"], "title": "Train a competitive layer on nonnormalized vectors using LVQ technique", "answer_count": 1, "views": 72, "accepted_answer": "20324", "answers": {"20324": {"line": 13316, "body": "We have 2 classes , 1 subclass for each class\n\\begin{equation}\n    W^2=\\begin{vmatrix}\n1 & 0\\\\ \n0 & 1\\\\\n\\end{vmatrix}\n\\end{equation}\n$p_1$:\n\\begin{equation}\n    \\alpha^1=compet(n^1)=compet\\begin{vmatrix}\n||w_1-p_1||\\\\ \n||w_2-p_1||\\\\\n\\end{vmatrix} = compet\\begin{vmatrix}\n||\\begin{vmatrix}\n0 & 1\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n1 & 1\\\\ \n\\end{vmatrix}^T||\\\\ \n||\\begin{vmatrix}\n1 & 0\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n1 & 1\\\\ \n\\end{vmatrix}^T||\\\\\n\\end{vmatrix}=\ncompet(\\begin{vmatrix}\n1\\\\ \n1\\\\\n\\end{vmatrix}) = \\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix})\n\\end{equation}\n\\begin{equation}\n   \\alpha^2=W^2\\cdot\\alpha^1= \\begin{vmatrix}\n1 & 0\\\\ \n0 & 1\\\\\n\\end{vmatrix}\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}\n\\end{equation}\n\\begin{equation}\n    W_1(1) = W_1(0) + \\alpha\\cdot(p_1-W_1(0))=\\begin{vmatrix}\n0\\\\ \n1\\\\\n\\end{vmatrix}+0.5\\cdot(\\begin{vmatrix}\n1\\\\ \n1\\\\\n\\end{vmatrix}-\\begin{vmatrix}\n0\\\\ \n1\\\\\n\\end{vmatrix})=\\begin{vmatrix}\n0\\\\ \n1\\\\\n\\end{vmatrix}+\\begin{vmatrix}\n0.5\\\\ \n0\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n0.5\\\\ \n1\\\\\n\\end{vmatrix}\n\\end{equation}\n$p_2$ :\n\\begin{equation}\n    \\alpha^1=compet(n^1)=compet\\begin{vmatrix}\n||w_1-p_2||\\\\ \n||w_2-p_2||\\\\\n\\end{vmatrix} = compet\\begin{vmatrix}\n||\\begin{vmatrix}\n0.5 & 1\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n1 & 2\\\\ \n\\end{vmatrix}^T||\\\\ \n||\\begin{vmatrix}\n1 & 0\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n1 & 2\\\\ \n\\end{vmatrix}^T||\\\\\n\\end{vmatrix}=\ncompet(\\begin{vmatrix}\n1.8027756377\\\\ \n2.8284271247\\\\\n\\end{vmatrix}) = \\begin{vmatrix}\n0\\\\ \n1\\\\\n\\end{vmatrix})\n\\end{equation}\n\\begin{equation}\n   \\alpha^2=W^2\\cdot\\alpha^1= \\begin{vmatrix}\n1 & 0\\\\ \n0 & 1\\\\\n\\end{vmatrix}\\begin{vmatrix}\n0\\\\ \n1\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n0\\\\ \n1\\\\\n\\end{vmatrix}\n\\end{equation}\nwrong class\n\\begin{equation}\n    W_2(1) = W_2(0) - \\alpha\\cdot(p_2-W_2(0))=\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}-0.5\\cdot(\\begin{vmatrix}\n-2\\\\ \n2\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n2\\\\ \n-1\\\\\n\\end{vmatrix}\n\\end{equation}\n$p_3$ :\n\\begin{equation}\n    \\alpha^1= compet\\begin{vmatrix}\n||\\begin{vmatrix}\n0.5 & 1\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n-2 & 2\\\\ \n\\end{vmatrix}^T||\\\\ \n||\\begin{vmatrix}\n2 & -1\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n-2 & 2\\\\ \n\\end{vmatrix}^T||\\\\\n\\end{vmatrix}=\ncompet(\\begin{vmatrix}\n70\\\\ \n5\\\\\n\\end{vmatrix}) = \\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix})\n\\end{equation}\n\\begin{equation}\n   \\alpha^2=W^2\\cdot\\alpha^1\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}\n\\end{equation}\nwrong class\n\\begin{equation}\n    W_1(2) = W_1(1) - \\alpha\\cdot(p_3-W_1(1))=\\begin{vmatrix}\n0.5\\\\ \n0\\\\\n\\end{vmatrix}-0.5\\cdot(\\begin{vmatrix}\n-2.5\\\\ \n1\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n1.75\\\\ \n0.5\\\\\n\\end{vmatrix}\n\\end{equation}\n$p_2$ :\n\\begin{equation}\n    \\alpha^1=compet(n^1)= compet\\begin{vmatrix}\n||\\begin{vmatrix}\n1.75 & 0.5\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n-1 & 2\\\\ \n\\end{vmatrix}^T||\\\\ \n||\\begin{vmatrix}\n2 & -1\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n-1 & 2\\\\ \n\\end{vmatrix}^T||\\\\\n\\end{vmatrix}=\ncompet(\\begin{vmatrix}\n3.13\\\\ \n4.24\\\\\n\\end{vmatrix}) = \\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix})\n\\end{equation}\n\\begin{equation}\n   \\alpha^2=W^2\\cdot\\alpha^1\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}\n\\end{equation}\n\\begin{equation}\n    W_1(3) = W_1(2) - \\alpha\\cdot(p_2-W_1(2))=\\begin{vmatrix}\n1.75\\\\ \n0.5\\\\\n\\end{vmatrix}+0.5\\cdot(\\begin{vmatrix}\n-2.75\\\\ \n1.5\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n0.375\\\\ \n1.25\\\\\n\\end{vmatrix}\n\\end{equation}\n$p_3$ :\n\\begin{equation}\n    \\alpha^1=compet(n^1)= compet\\begin{vmatrix}\n||\\begin{vmatrix}\n0.375 & 1.25\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n-2 & 2\\\\ \n\\end{vmatrix}^T||\\\\ \n||\\begin{vmatrix}\n2 & -1\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n-2 & 2\\\\ \n\\end{vmatrix}^T||\\\\\n\\end{vmatrix}=\ncompet(\\begin{vmatrix}\n2.95\\\\ \n5\\\\\n\\end{vmatrix}) = \\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix})\n\\end{equation}\n\\begin{equation}\n   \\alpha^2=W^2\\cdot\\alpha^1\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}\n\\end{equation}\nwrong class\n\\begin{equation}\n    W_1(4) = W_1(1) - \\alpha\\cdot(p_3-W_1(3))=\\begin{vmatrix}\n0.375\\\\ \n1.25\\\\\n\\end{vmatrix}-0.5\\cdot(\\begin{vmatrix}\n-2.375\\\\ \n0.75\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n1.5625\\ \n0.875\\\\\n\\end{vmatrix}\n\\end{equation}\np_1 :\n\\begin{equation}\n    \\alpha^1=compet(n^1)= compet\\begin{vmatrix}\n||\\begin{vmatrix}\n1.5625 & 0.875\\\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n1 & 1\\\\ \n\\end{vmatrix}^T||\\\\ \n||\\begin{vmatrix}\n2 & -1\\ \n\\end{vmatrix}^T-\\begin{vmatrix}\n1 & 1\\\\ \n\\end{vmatrix}^T||\\\\\n\\end{vmatrix}=\ncompet(\\begin{vmatrix}\n0.57\\\\ \n2.23\\\\\n\\end{vmatrix}) = \\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix})\n\\end{equation}\n\\begin{equation}\n   \\alpha^2=W^2\\cdot\\alpha^1\\begin{vmatrix}\n1\\\\ \n0\\\\\n\\end{vmatrix}\n\\end{equation}\n\\begin{equation}\n    W_1(5) = W_1(4) - \\alpha\\cdot(p_1-W_1(4))=\\begin{vmatrix}\n1.5625\\\\ \n0.875\\\\\n\\end{vmatrix}+(\\begin{vmatrix}\n-0.28125\\\\ \n0.0625\\\\\n\\end{vmatrix}=\\begin{vmatrix}\n1.28125\\ \n0.9375\\\\\n\\end{vmatrix}\n\\end{equation}\n", "type": 2, "id": "20324", "date": "2020-04-16T16:41:18.440", "score": 0, "comment_count": 0, "parent_id": "17126"}}}
{"line": 10190, "body": "I was following some examples to get familiar with TensorFlow's LSTM API, but noticed that all LSTM initialization functions require only the num_units parameter, which denotes the number of hidden units in a cell.\nAccording to what I have learned from the famous colah's blog, the cell state has nothing to do with the hidden layer, thus they could be represented in different dimensions (I think), and then we should pass at least 2 parameters denoting both #hidden and #cell_state. \nSo, this confuses me a lot when trying to figure out what the TensorFlow's cells do. Under the hood, are they implemented like this just for the sake of convenience or did I misunderstand something in the blog mentioned?\n\n", "type": 1, "id": "15621", "date": "2019-09-25T13:59:41.587", "score": 13, "comment_count": 3, "tags": ["neural-networks", "tensorflow", "recurrent-neural-networks", "long-short-term-memory"], "title": "What is the relationship between the size of the hidden layer and the size of the cell state layer in an LSTM?", "answer_count": 3, "views": 3864, "accepted_answer": null, "answers": {"15667": {"line": 10224, "body": "I had a very similar issue as you did with the dimensions. Here's the rundown:\nEvery node you see inside the LSTM cell has the exact same output dimensions, including the cell state. Otherwise, you'll see with the forget gate and output gate, how could you possible do an element wise multiplication with the cell state? They have to have the same dimensions in order for that to work.\nUsing an example where n_hiddenunits = 256:\nOutput of forget gate: 256\nInput gate: 256\nActivation gate: 256\nOutput gate: 256\nCell state: 256\nHidden state: 256\n\nNow this can obviously be problematic if you want the LSTM to output, say, a one hot vector of size 5. So to do this, a softmax layer is slapped onto the end of the hidden state, to convert it to the correct dimension. So just a standard FFNN with normal weights (no bias', because softmax). Now, also imagining that we input a one hot vector of size 5:\ninput size: 5\ntotal input size to all gates: 256+5 = 261 (the hidden state and input are appended)\nOutput of forget gate: 256\nInput gate: 256\nActivation gate: 256\nOutput gate: 256\nCell state: 256\nHidden state: 256\nFinal output size: 5\n\nThat is the final dimensions of the cell.\n", "type": 2, "id": "15667", "date": "2019-09-30T04:24:42.000", "score": 2, "comment_count": 1, "parent_id": "15621"}, "27939": {"line": 19221, "body": "Look at the equation for computing the hidden state as a function of the cell state and output gate:\n$$\nh_t = \\tanh(C_t)\\circ o_t\n$$\nThis equation implies that the hidden state and cell state have the same dimensionality.\n", "type": 2, "id": "27939", "date": "2021-05-24T10:13:34.653", "score": 0, "comment_count": 0, "parent_id": "15621"}, "27886": {"line": 19174, "body": "What I understand with a layer of LSTM composed of 4 cells is depicted in the following picture:\n\nThis would explain the fact that the hidden state of the whole layer has exactly the same dimension of the hidden states (or cells).\nHowever, what I still don't fully understand is the 'return sequence' between LSTM layers, which changes the shape from [hidden_states] to [x_dimension, hidden_states]. This is explained because usually we only care about the state of the last cell, and when connecting multiple layers, all the states of the cells are passed into the next layer. Nevertheless, I still cannot make sense of it graphically.\ne.g.\nmodel = keras.models.Sequential([ keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]), keras.layers.LSTM(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ])\n", "type": 2, "id": "27886", "date": "2021-05-20T17:36:07.507", "score": 1, "comment_count": 0, "parent_id": "15621"}}}
{"line": 10703, "body": "I generate some non-Gaussian data, and use two kinds of DNN models, one with BN and   the other without BN.\nI find that the model DNN with BN can't predict well.  \nThe codes is shown as follow:\nimport numpy as np\nimport scipy.stats\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation, BatchNormalization\n\nnp.random.seed(1)\n\n# generate non-gaussian data\ndef generate_data():\n    distribution = scipy.stats.gengamma(1, 70, loc=10, scale=100)\n    x = distribution.rvs(size=10000)\n    # plt.hist(x)\n    # plt.show()\n    print ('[mean, var, skew, kurtosis]', distribution.stats('mvsk'))\n\n    y = np.sin(x) + np.cos(x) + np.sqrt(x)\n    plt.hist(y)\n    # plt.show()\n    # print(y)\n    return x ,y \n\nx, y = generate_data()\n\nx_train = x[:int(len(x)*0.8)]\ny_train = y[:int(len(y)*0.8)]\nx_test = x[int(len(x)*0.8):]\ny_test = y[int(len(y)*0.8):]\n\n\ndef DNN(input_dim, output_dim, useBN = True):\n    '''\n    Ding Yi Yi Ge DNN model\n    '''\n    model=Sequential()\n\n    model.add(Dense(128,input_dim= input_dim))\n    if useBN:\n        model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(50))\n    if useBN:\n        model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(output_dim))\n    if useBN:\n        model.add(BatchNormalization())\n    model.add(Activation('relu'))\n\n    model.compile(loss= 'mse', optimizer= 'adam')\n    return model\n\nclf = DNN(1, 1, useBN = True)\nclf.fit(x_train, y_train, epochs= 30, batch_size = 100, verbose=2, validation_data = (x_test, y_test))\n\ny_pred = clf.predict(x_test)\ndef mse(y_pred, y_test):\n    return np.mean(np.square(y_pred - y_test))\nprint('final result', mse(y_pred, y_test))\n\nThe input x is like this shape:\n\nIf I add BN layers, the result is shown as follows:\nEpoch 27/30\n - 0s - loss: 56.2231 - val_loss: 47.5757\nEpoch 28/30\n - 0s - loss: 55.1271 - val_loss: 60.4838\nEpoch 29/30\n - 0s - loss: 53.9937 - val_loss: 87.3845\nEpoch 30/30\n - 0s - loss: 52.8232 - val_loss: 47.4544\nfinal result 48.204881459013244\n\nIf I don't add BN layers, the predicted result is better:\nEpoch 27/30\n - 0s - loss: 2.6863 - val_loss: 0.8924\nEpoch 28/30\n - 0s - loss: 2.6562 - val_loss: 0.9120\nEpoch 29/30\n - 0s - loss: 2.6440 - val_loss: 0.9027\nEpoch 30/30\n - 0s - loss: 2.6225 - val_loss: 0.9022\nfinal result 0.9021717561981543\n\nAnyone knows the theory about why BN is not suitable for non-gaussian data ?\n", "type": 1, "id": "16240", "date": "2019-11-03T04:36:24.793", "score": 3, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "deep-neural-networks", "batch-normalization"], "title": "Is batch normalization not suitable for non-gaussian input?", "answer_count": 1, "views": 140, "accepted_answer": null, "answers": {"16250": {"line": 10712, "body": "So batch-normalization helps descent based learning have an easier time traversing the loss manifold, but in your case you use it along with a relu as a final activation is problematic, it means the output is relatively associated with the other samples in the batch.  \nRemove that last BN and you get better results, but also understand BN is inherently problematic for this task. Think of DNNs as featurizers, and BN in this case takes out the 2 batch-wide statistics which if they don't align to the initial distribution will cause error, which will lead to error in the output. In theory if BN gets the perfect statistics of the gaussian it should not matter too much, so one thing I tried with your code was remove last BN and increase N to 100,000 while increasing the batch size to 10000 and you see a huge boost in performance.\n", "type": 2, "id": "16250", "date": "2019-11-03T18:21:23.000", "score": 0, "comment_count": 0, "parent_id": "16240"}}}
{"line": 10457, "body": "I have a neural network that should be able to classify documents to target label A. The problem is that the network is actually classifying label B, which is an easier task.\nTo make the problem more clear: I need to classify documents from different sources. In the training data each source occurs repeatedly, but the network should be able to work on unknown sources. All documents from a single source have the same class. In this case, it is easier to identify sources than the target label so in practice the network is not really identifying the target label, but the source.\nThe solution to this problem is making sure that the model is bad at identifying the sources in the training data, while still attaching the right target labels.\nI think the first step is to get two output layers, one for the target label and one for identifying which source it is from. My approach fails however at the training procedure: I want to minimize the loss on the target output, but maximize the loss on the non-target output. But if I maximize the loss on that non-target output, that does not mean that the network 'unlearns' the non-target labels. So the main question for the non-target output is:\nTLDR; How do I define a training procedure that minimizes the loss on a non-target output layer, and then maximizes that loss on all layers before it. My goal is to have a network that is good at classifying label A, but bad at a related label B. If anyone wants to give a code example, my prefered framework is PyTorch.\n", "type": 1, "id": "15976", "date": "2019-10-19T12:43:28.273", "score": 2, "comment_count": 0, "tags": ["neural-networks", "objective-functions"], "title": "Maximize loss on non-target variable", "answer_count": 2, "views": 48, "accepted_answer": null, "answers": {"15996": {"line": 10477, "body": "You can maximize the loss by doing 1-loss (if the range of the loss is between 1 and 0. However, I am not sure if that would help. The network can just propagate the weights of the source classifier to always output the wrong answer. a better approach is to hand craft features of the document to make it less identifiable as the source. If the document have stated the source, remove it. This may help. Also, another way is to use documents from more sources so the network cannot use the source to classify the label. Hope I can help you. \n", "type": 2, "id": "15996", "date": "2019-10-21T11:14:02.110", "score": 0, "comment_count": 0, "parent_id": "15976"}, "18727": {"line": 12810, "body": "A little more information about the documents will be helpful. I am guessing that your scenario has webpages from different websites, you're feeding html pages to the network and the page contains the website name or url, which the network is picking up on and using it to label. I am assuming you're using a RNN or similar network for the classification task. \nYour question is to identify when your network is using the source identifying feature (name or url), and instruct it to not use this feature. If you can do some preprocessing and identify where in the document the source name/url is, you may tag each word/character in the input with this information. You may use this tag to add a large regularizing loss term which brings down the weights connected to the tagged input words/characters in the first layer.\nHowever, a better strategy will be to remove all instance of the header / url which contains website info, as @clement-hui suggested. This is cleaner and easier to implement than above.\nPerhaps your network is identifying the document source slightly more indirectly, and above preprocessing is not sufficient/useful. In this case, you may want to make random chunks of your document, and give each chunk the label corresponding to that document. Here it is more likely that some chunks will not have the source identifying information, and the network will be forced to pick a strategy which labels using the content rather than the source. \nAt test time you may either feed the whole document or random chunks from the document and use majority voting to get the final answer.\n", "type": 2, "id": "18727", "date": "2020-03-19T20:30:22.150", "score": 0, "comment_count": 0, "parent_id": "15976"}}}
{"line": 11625, "body": "Let's assume an extreme case in which the kernel of the convolution layer takes only values 0 or 1. To capture all possible patterns in input of $C$ number of channels, we need $2^{C*K_H*K_W}$ filters, where $(K_H, K_W)$ is the shape of a kernel. So to process a standard RGB image with 3 input channels with 3x3 kernel, we need our layer to output $2^{27}$ channels. Do I correctly conclude that according to this, the standard layers of 64 to 1024 filters are only able to catch a small part of (perhaps) useful patterns?\n", "type": 1, "id": "17324", "date": "2020-01-01T21:01:39.787", "score": 2, "comment_count": 2, "tags": ["deep-learning", "convolutional-neural-networks", "computer-vision"], "title": "What is the reasoning behind the number of filters in the convolution layer?", "answer_count": 2, "views": 110, "accepted_answer": null, "answers": {"17339": {"line": 11636, "body": "Let $n=C*K_w*K_h$. Then you should only need $n$ filters. Not $2^n$ to keep all the information. If you just used the rows of the identity matrix as your filters than your convolution would just be making an exact copy so it definitely wouldn't be throwing away information. On the other hand, there will be a max pooling operation. To simplify the question let's suppose we have a 3 channels, and a 1 by 1 kernel. And then let's suppose it is just one convolution followed by global max pooling. Also, let's use your assumption that it's all binary. If you have $m$ filters then the final output will be $m$ dimensional no matter how many input points you have. So clearly information is being thrown away there. But that's not such a bad thing. Throwing away irrelevant information gets us closer to the features we need the problem at hand. The parts that get thrown away by max pooling correspond to features not being found in a particular part of the image.\n", "type": 2, "id": "17339", "date": "2020-01-02T21:27:04.163", "score": 0, "comment_count": 4, "parent_id": "17324"}, "21552": {"line": 14268, "body": "From mathematical point of view you are correct as are your calculations. To catch all the patterns you need that many filters, but this is where a whole idea of a training comes in. Main objective of the training in the CNNs is to find just a few good patterns from billions possible ones. \nSo the direct answer to your question is: The standard layers of 64 to 1024 filters are only able to catch a small part of (perhaps) useful patterns, yes but this is assuming no training taking place. If you conducted training on given data with given model, then 64 to 1024 filters could already extract a lot of useful patterns, perhaps more than needed. \n", "type": 2, "id": "21552", "date": "2020-06-01T06:20:25.060", "score": 1, "comment_count": 0, "parent_id": "17324"}}}
{"line": 9573, "body": "In a neural network, by how much does the # of neurons typically vary from layer to layer?\nPLEASE NOTE:\nI am NOT asking how to find the optimal # of neurons per lyr.\nAs a hardware design engineer with no practical experience programming neural networks, I would like to glean for example... \n\nBy how much does the number of neurons in hidden layers typically vary from that of the input layer?\nWhat is the maximum deviation in the # of hidden lyr neurons to the # of input layer neurons? \nHow commonly do you see a large spike in the number of neurons?\n\nIt likely depends on the application so I would like to hear from as many people as possible. Please tell me about your experience.\n", "type": 1, "id": "13824", "date": "2019-08-06T14:49:32.830", "score": 4, "comment_count": 2, "tags": ["neural-networks", "deep-learning"], "title": "Standard deviation in the # of neurons per layer", "answer_count": 2, "views": 113, "accepted_answer": null, "answers": {"13831": {"line": 9580, "body": "There is no right answer to this question. But, I would like to point you to an answer on CV that addresses the mean of your problem.\nTwo points from the accepted answer that I want to draw your attention to are:\n\na) There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'.\nb) In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers.\n\nOther answers in the thread are also very insightful. I will recommend you to go through the answers and figure out the standard deviation and just assume things are normal, and hence you would have your distribution.\n", "type": 2, "id": "13831", "date": "2019-08-06T18:00:51.283", "score": 0, "comment_count": 0, "parent_id": "13824"}, "13827": {"line": 9576, "body": "\nInput layers will always have the dimensionality of your input data(for every model I can think of).\nSee above, the deviation between hidden layers can be significant. For example, 128 in the first hidden and 64 in the rest(or vice versa).\nThis question in particular will always be problem dependent. It is decided via architecture search or intuition/experience combined with some exploratory search.\n\n", "type": 2, "id": "13827", "date": "2019-08-06T16:18:45.690", "score": 1, "comment_count": 0, "parent_id": "13824"}}}
{"line": 9208, "body": "I am using dropout of different values to train my network. The problem is, dropout is contributing almost nothing to training, either causing so much noise the error never changes, or seemingly having no effect on the error at all:\nThe following runs were seeded.\nkey: dropout = 0.3, means 30% chance of dropout\ngraph x axis: iteration\ny axis: error\ndropout=0\n\ndropout = 0.001\ndropout = 0.1\ndropout = 0.5\n\nI don't quite understand why dropout of 0.5 effectively kills the networks ability to train. This specific network here is rather small, a CNN of architecture:\n3x3x3                    Input image\n3x3x3                    Convolutional layer: 3x3x3, stride = 1, padding = 1\n20x1x1                   Flatten layer: 27 -> 20\n20x1x1                   Fully connected layer: 20\n10x1x1                   Fully connected layer: 10\n2x1x1                    Fully connected layer: 2\n\nBut I have tested a CNN with architecture:\n10x10x3                  Input image\n9x9x12                   Convolutional layer: 4x4x12, stride = 1, padding = 1\n8x8x12                   Max pooling layer: 2x2, stride = 1\n6x6x24                   Convolutional layer: 3x3x24, stride = 1, padding = 0\n5x5x24                   Max pooling layer: 2x2, stride = 1\n300x1x1                  Flatten layer: 600 -> 300\n300x1x1                  Fully connected layer: 300\n100x1x1                  Fully connected layer: 100\n2x1x1                    Fully connected layer: 2\n\novernight with dropout = 0.2 and it completely failed to learn anything, having an accuracy of just below 50%, whereas without dropout, its accuracy is ~85%. I would just like to know if there's a specific reason as to why this might be happening. My implementation of dropout is as follows:\nactivation = relu(val)*(random.random() > self.dropout)\nthen at test time:\nactivation = relu(val)*(1-self.dropout)\n", "type": 1, "id": "13380", "date": "2019-07-16T02:58:43.097", "score": 1, "comment_count": 6, "tags": ["neural-networks", "convolutional-neural-networks", "dropout", "regularization", "relu"], "title": "Dropout causes too much noise for network to train", "answer_count": 1, "views": 160, "accepted_answer": null, "answers": {"13416": {"line": 9236, "body": "why dropout of 0.5 effectively kills the networks ability to train.\n\nbecause that is too mutch normal values are like 0.15-0.05. Imagine, the 50% of input image is randomly set to 0, THEN it happens on next layer, means in average 25%  remains, etc... also if you have small dataset with too different images for each class, this + drouput wil confuse the network. \nAlso your CNN setup is not realy rational. Too much fc layers, replace one or two fc to convo layers.  And i d say the reason of using 4x4 is only with stride 2, else use 3x3. And you sould use batch normalisation and augmentation like small noice would be probably petter then dropout in your case.\n", "type": 2, "id": "13416", "date": "2019-07-18T10:23:17.247", "score": 0, "comment_count": 2, "parent_id": "13380"}}}
{"line": 10458, "body": "Is there a way to understand, for instance, a multi-layered perceptron without hand-waving about them being similar to brains etc?\nFor example: it is obvious that what a perceptron does is approximating a function; there might be many other ways, given a labelled dataset, to find the separation of the input area into smaller areas that correspond to the labels; however, these ways would probably be computationally rather ineffective, which is why they cannot be practically used. However, it seems that the iterative approach of finding such areas of separation may give a huge speed-up in many cases; then, natural questions arise why this speed-up may be possible, how it happens and in which cases.\nOne could be sure that this question was investigated. If anyone could shed any light on the history of this question, I would be very grateful. So, why are neural networks useful and what do they do? I mean, from the practical and mathematical standpoint, without relying on the concept of \"brain\" or \"neurons\" which can explain nothing at all.\n", "type": 1, "id": "15977", "date": "2019-10-19T18:23:15.347", "score": 12, "comment_count": 2, "tags": ["neural-networks", "theory", "history"], "title": "Is there a way to understand neural networks without using the concept of brain?", "answer_count": 3, "views": 169, "accepted_answer": null, "answers": {"15979": {"line": 10460, "body": "tl;dr I always like to think of Neural Networks as a generalization of logistic regression. \nI too don't like that, traditionally, when introducing Neural Networks, books start with biological neurons and synapses, etc. I think its more beneficial to start from statistics and linear regression, then logistic regression and then neural networks.\nA perceptron is essentially a simple binary logistic regressor (if you threshold the output). If you have many perceptrons that share the same input (i.e. a layer in a neural network), you can think of it as a multi-class logistic regressor. Now, by stacking one such layer after an other, you create a Multi-Layer Perceptron (MLP), which is a Neural Network with two layers. There is equivalent to two multi-class logistic regressors stacked one after the other. One notable thing that changes is the training technique here, i.e. backpropagation (because you don't have direct access to the targets from the hidden layer). Another thing that can change is the activation function (it's not always sigmoid in Neural Networks)\nIntroduce sparse connectivity and weight sharing and you get a Convolutional Neural Network. Add a connection from a layer to its self (for the next timestep) and you get a Recurrent Neural Network. Likewise, you can reproduce any Neural Network through this reasoning.\nI know this is an over-simplified way of presenting them, but I think you get the point.\n", "type": 2, "id": "15979", "date": "2019-10-20T01:53:34.830", "score": 11, "comment_count": 1, "parent_id": "15977"}, "16132": {"line": 10603, "body": "One way to view a neural network is as a series of linear transformations. \nYou take a bunch of data points and look at it from a different perspective from a different space. You apply some non linear function on the data points like, ReLU, sigmoid etc. Now you repeat the same process of looking from a different space.\nOur goal is to look at it from a point where things starts looking right for our tasks. These linear transformations is what the network has to optimise. \n", "type": 2, "id": "16132", "date": "2019-10-29T04:59:12.923", "score": 2, "comment_count": 1, "parent_id": "15977"}, "16134": {"line": 10605, "body": "A good way of looking at it would be understanding neural networks mathematically, i.e. purely on the basis of the fact that you're just trying to fit a function and solve an optimisation problem (apart from looking at it as multiple units of logistic regression). \nSay we want to approximate a function $y =f_w(x)$ with $x \\in D$, where $D$ is our domain-space. We want this function to map to $C$, our co-domain, with all the values the function ends up taking being the set $y \\in R$, our range. Essentially we frame $f(x)$ as a sequence of operations (What operation should be done where is got from common-practice, intuition, and insight mostly gained from experience) assuming that when the right parameters are used for these operations we will arrive at a very reasonable approximation of the function.\nWe initialise the parameters with whatever values we want initially (usually random), calling this parameter-space $W$. The essential idea would be frame another function $L(f_w(x), \\hat{y})$ called the loss function which we want to minimise. This acts as a test to how good our function is - since our function parameters were initially random, the error between the function approximations and the actual range values for known points (training set) are estimated. These estimated error values and its gradient is then used by back-propagation where $w_{init}\\in W$ is updated to another $w_{1}\\in W$, where $w_1$ is calculated by moving on $L$ in the direction of decreasing gradient, in hopes of reaching the loss functions minima.  \nSimplifying, essentially all you want to do is find a $y=f_w(x)$ where parameters $w$ are to be chosen such that $L(f_w(x), \\hat{y})$ is minimised for the training set.\nEven though this is a very rough idea of neural networks, such a direction in thinking can especially be useful when studying generative networks and other problems where the problem has to be formulated mathematically before being able to approach it. \n", "type": 2, "id": "16134", "date": "2019-10-29T06:19:31.340", "score": 0, "comment_count": 0, "parent_id": "15977"}}}
{"line": 13084, "body": "\nThis is a picture of a recurrent neural network (RNN) found on a udemy course (Deep Learning A-Z). The axis at the bottom is \"time\". \nIn a time series problem, each yellow row from left to right would represent a sequence of a feature. In this picture, then, there are 6 sequences from 6 different features that are being fed to the network.\nI am wondering if the arrows in this picture are completely accurate in an RNN. Shouldn't every yellow node also connect to every other blue node along its depth dimension? By depth dimension here I mean the third dimensional axis of the input tensor. \nFor example, the yellow node at the bottom left of this picture, which is closest to the viewer, should have an arrow pointing to all the blue nodes in the array of blue nodes that is at the very left, and not just to the blue node directly above it.\n", "type": 1, "id": "20055", "date": "2020-04-06T21:46:15.407", "score": 2, "comment_count": 4, "tags": ["neural-networks", "recurrent-neural-networks"], "title": "Is this a correct visual representation of a recurrent neural network (RNN)?", "answer_count": 1, "views": 62, "accepted_answer": "20064", "answers": {"20064": {"line": 13091, "body": "I will answer this question, leaving it open to challenge by anyone more knowledgeable. \nThe equations to update each layer of an RNN are\n$y_t = \\sigma(W_x x_t + b_x )$\nand\n$h_t = \\sigma(W_hx_t + b_h)$\nWhere $h_t$ is the hidden layer (in blue in the picture), and $y_t$ is the output layer (red in picture). This equations says that every single component in the hidden layer vector, i.e. every unit in the hidden layer, is a function of the linear combination of the $x_t$ vector, which is the first yellow row along the depth axis. In other words, all the first yellow input nodes on the bottom left in the picture. \nThus, technically this picture is not correct, all the yellow nodes should have points to all the blue nodes. Also, by similar reasoning, all the blue nodes in the subsequent steps of the hidden layer should be connected to all the blue nodes of the previous layer.\nOf course, that would make for a much uglier/harder picture to make, so I don't blame the authors, although this has given me a few hours of confused research, which I guess still meets their educational goal.\n", "type": 2, "id": "20064", "date": "2020-04-07T04:49:36.697", "score": 0, "comment_count": 0, "parent_id": "20055"}}}
{"line": 9555, "body": "I've heard somewhere that due to their nature of capturing spatial relations, even untrained CNNs can be used as feature extractors? Is this true? Does anyone have any sources regarding this I can look at?\n", "type": 1, "id": "13805", "date": "2019-08-06T03:02:23.953", "score": 3, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "computer-vision"], "title": "Untrained CNNs as feature extractors?", "answer_count": 2, "views": 233, "accepted_answer": "13811", "answers": {"13807": {"line": 9557, "body": "I'm not sure it's possible. Untrained CNN means it has random kernel values. Let's say you have a kernel with size 3x3 like below:\n0 0 0\n0 0 0\n0 0 1\n\nI don't think it is possible for that kernel to provide good information about the image. on the contrary, the kernel eliminates a lot of information. We cannot rely on random values for feature extraction.\nBut, if you use CNN with \"assigned\" kernel, then you don't need to train the convolutional layer. For example, you can start a CNN with a kernel that designed to extract vertical line:\n-1 2 -1\n-1 2 -1\n-1 2 -1\n\n", "type": 2, "id": "13807", "date": "2019-08-06T05:41:21.837", "score": 0, "comment_count": 5, "parent_id": "13805"}, "13811": {"line": 9560, "body": "Yes, it has been demonstrated that the main factor for CNNs to work is its architecture, which exploits locality during the feature extraction. A CNN with random weights will do a random partition of the feature space, but still with that spatial prior that works so well, so those random features are OK for classification (and sometimes even better than trained ones, as they don't introduce additional bias).\nYou can read more in these papers:\n\nD. Ulyanov et al. Deep Image Prior\nA. Rosenfeld and J. K. Tsotsos. Intriguing Properties of Randomly Weighted Networks: Generalizing While Learning Next to Nothing.\n\n", "type": 2, "id": "13811", "date": "2019-08-06T10:57:34.543", "score": 5, "comment_count": 1, "parent_id": "13805"}}}
{"line": 10309, "body": "I built a three-layer neural network (first is 1D convolutional and the remaining two are linear). It takes an input of 5 angles in radians, and outputs two numbers from 0 to 1, which are respectively the probability of failure or success. The NN is trained in a simulation.\nThe simulation goes this way: it takes 5 angles in radians and calculates the vector sum of 5 vectors having $x$ as module and $\\alpha$ as angles (taken from the input). It returns $1$ if the vector sum has a module greater than $y$, or $0$ if it is less than $y$.\nMy intention is to be able to tell sequences of radians that will generate vectors with a sum greater than $y$ in module from the ones which won't.\nWhich would be the best configuration to achieve this? Is the configuration I set up (1D convolution layer + 2 linear layers) efficient? If so, would it be easy to find the right size for the convolution? Or should I just remove it?\nI noticed that if I change the order of the input angles the output of the simulation will be the same. Is there a particular configuration you should use when dealing with these cases?\n", "type": 1, "id": "15792", "date": "2019-10-08T09:00:41.550", "score": 0, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "ai-design"], "title": "What is the best neural network architecture for this problem?", "answer_count": 1, "views": 195, "accepted_answer": null, "answers": {"15804": {"line": 10315, "body": "I'm not completely sure I understand your simulation. What I think you are doing is:\n\nGenerate 5 angles specified in radians (Are these always normalized to within (0, $2*\\pi$)?).\nInterpret each angle as a unit vector in a 2d space.\nAdd the unit vectors together, yielding a vector that lies somewhere inside a circle with radius 5.\nAsk whether the summed vector is more or less than a distance $y$ from the origin of the circle.\n\nIf you're doing that, your problem looks like trying to learn a separation of two concentric rings, which is a well known benchmarking problem for classification. \nI am reasonably certain you can learn this pattern with several layers of ReLU neurons. I'm not certain that convolutional layers will help you much here. The main patterns I'd expect the network to learn are:\n\nperhaps 2-3 layers to learn whether the point lies far away from the origin in each of several different directions.\n1 layer to learn where the decision boundary is in each of 4 directions away from the origin.\n1 layer to inclusive-OR the 4 decision boundaries together.\n\nMy guess is that this is fairly easy to learn with 4 layers of 8 ReLU neurons, or something like it.\n", "type": 2, "id": "15804", "date": "2019-10-08T23:17:00.987", "score": 0, "comment_count": 1, "parent_id": "15792"}}}
{"line": 11248, "body": "Suppose you want to predict the price of some stock. Let's say you use the following features.\nOpenPrice  \nHighPrice\nLowPrice\nClosePrice\n\nIs it useful to create new features like the following ones?\nBodySize = ClosePrice - OpenPrice  \n\nor the size of the tail    \nTailUp = HighPrice - Max(OpenPrice, ClosePrice)  \n\nOr we don't need to do that because we are adding noise and the neural network is going to calculate those values inside?    \nThe case of the body size maybe is a bit different from the tail, because for the tail we need to use a non-linear function (the max operation). So maybe is it important to add the input when it is not a linear relationship between the other inputs not if it's linear?     \nAnother example. Consider a box, with height $X$, width $Y$ and length $Z$.\nAnd suppose the real important input is the volume, will the neural network discover that the correlation is $X * Y * Z$? Or we need to put the volume as input too?    \nSorry if it's a dumb question but I'm trying to understand what is doing internally the neural network with the inputs, if it's finding (somehow) all the mathematically possible relations between all the inputs or we need to specify the relations between the inputs that we consider important (heuristically) for the problem to solve?\n", "type": 1, "id": "16883", "date": "2019-12-01T15:31:44.697", "score": 4, "comment_count": 0, "tags": ["neural-networks"], "title": "Does the neural network calculate different relations between inputs automatically?", "answer_count": 2, "views": 82, "accepted_answer": "16884", "answers": {"16891": {"line": 11254, "body": "The question is related to \"feature extraction\". Firstly, to tackle a regression problem like both the problems stated by you, you need to provide the neural network with the most relevant inputs that have a effect on the output. Eg. If you want your network to add x and y, you need to provide it training examples like  input(x=1, y=3)  and output (sum=4). This will make your network do exactly what you want.\nBut suppose you do not know whether what inputs should you train your network on, neural networks can take care of that too. Look at this example:\nLook at the first truth table. Notice that the output column is actually the first input column and the other two input columns are just random. Eventually, the network learns this relationship and provides the correct results. What we learnt: if you are unsure about which inputs should you choose for your network, just provide as many as possible, or as many combinations as possible. Neural networks excel in finding relationships in input data. \nNext, talking of the volume problem, this is what I have been doing recently. It's actually an example of function approximation. Usually, the problem has multiple inputs and a single output (just like the addition problem), but the inverse is also possible. i.e., input : sum and output: x & y. This comes under one to many function mapping and multivariate regression. So YES, you need to provide the volume as input and x,y and z as outputs while training. The recommended configuration is one neuron in input layer, at least 6 hidden neurons and 3 neurons in output layer For magical results, you can use a deeper neural network rather than the shallow one suggested by me. But remember, neural networks have been proved to be **Universal Approximators*\n", "type": 2, "id": "16891", "date": "2019-12-02T06:05:05.057", "score": 0, "comment_count": 0, "parent_id": "16883"}, "16884": {"line": 11249, "body": "On paper, one expects a complex enough network to determine any complicated function of a limited number of inputs, given a large enough dataset. But in practice, there is no limit to the possible difficulty of the function to be learnt, and the datasets can be relatively small on occasion. In such cases - or arguably in general - it is definitely a good idea to define some combination of the inputs depending on some heuristics as you suggested. If you think some combination of inputs is an important variable by itself, you definitely should include it in your inputs. \nWe can visualize this situation in TensorFlow playground. Consider the circular pattern dataset on top left corner with some noise. You can use the default setting: $x_1$ and $x_2$ as inputs with 2 hidden layers with 4 and 2 neurons respectively. It should learn the pattern in less than 100 epochs. But if you reduce the number of neurons in the second layer to 2, it is not going to get as good as before. So, you are making the model more complicated to get the correct answer.\nYou can experiment and see that one needs at least one 3 neuron layer to get the correct classification from just $x_1$ and $x_2$. Now, if we examine the dataset, we see the circles so we know that instead of $x_1$ and $x_2$, we can try $x_1^2$ and $x_2^2$. This will learn perfectly without any hidden layers as the function is linear in these parameters. The lesson to be learnt here is that, our prior knowledge of the circle ($x_1^2 + x_2^2 = r^2$)  and familiarity with the data helped us in getting a good result with a simpler model (smaller number of neurons), by using derived inputs.\nTake the spiral data at the lower right corner for a more challenging problem. For this one, if you do not use any derived features, it is not likely to give you the correct result, even with several hidden layers. Keep in mind that every extra neuron is a potential source of overfitting, on top of being a computational burden.\nOf course the problem here is overly simplified but I expect the situation to be more or less the same for any complicated problem. In practice, we do not have infinite datasets or infinite compute times and the model complexity is always a restriction, so if you have any reason to think some relation between your inputs is relevant for your final result, you definitely should include it by hand at the beginning. \n", "type": 2, "id": "16884", "date": "2019-12-01T16:47:54.820", "score": 3, "comment_count": 0, "parent_id": "16883"}}}
{"line": 10584, "body": "I'm just started to learn about meta learning and CNN and in most paper that I've read they mention to have one CNN to feature extraction. These features will help the another network.\nI don't know what is feature extraction (I don't know what are those features) but I'm wondering if I can use it on image segmentation.\nThe idea is to use the first network to feature extraction without doing image classification, and pass those features to the other network.\nMy question is:\nHow can I use feature extraction in CNN on image segmentation?\n", "type": 1, "id": "16112", "date": "2019-10-28T09:08:49.590", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "feature-extraction"], "title": "How can I use feature extraction in CNN with image segmentation?", "answer_count": 1, "views": 177, "accepted_answer": "16116", "answers": {"16116": {"line": 10588, "body": "Feature extraction is a way that people use pretrained model to extract information from input data. For example, image segmentation task may use the VGG network or other image classifying network for feature extraction. The output of the last convolution layer is taken. Then, the features are feed into the untrained network to get outputs. The bottom network for image segmentation usually consists of upsampling and convolutional layers. Then output of size of original image is resulted in teh main network. Hope I can help you\n", "type": 2, "id": "16116", "date": "2019-10-28T10:10:53.920", "score": 0, "comment_count": 0, "parent_id": "16112"}}}
{"line": 10642, "body": "I'm trying to replace the strided convolutions of Keras' MobileNet implementation with the ConvBlurPool operation as defined in the Making Convolutional Networks Shift-Invariant Again paper. In the paper, a ConvBlurPool is implemented as follows:\n$$\nRelu \\circ Conv_{k,s} \\rightarrow Subsample_s \\circ Blur_m \\circ Relu \\circ Conv_{k,1}\n$$\nwhere k is the convolution's output kernels, s is the stride, m is the blurring kernel size and the subsample+blur is implemented as a strided convolution with a constant kernel.\nMy issues start when batch normalization enter the picture.\nIn MobileNet, a conv block is defined as follows (omitting the zero-padding):\n$$\nRelu \\circ BatchNorm \\circ Conv_{k,s}\n$$\nI am leaning towards converting it to:\n$$\nSubsample_s \\circ Blur_m \\circ Relu \\circ BatchNorm \\circ Conv_{k,1}\n$$\ni.e., putting the BN before the activation as it's normally done. This is not equivalent though, because the first BN operates on the downsampled signal.\nAnother possibility would be:\n$$\nBatchNorm \\circ Subsample_s \\circ Blur_m \\circ Relu \\circ Conv_{k,1}\n$$\nwith the BN as last operation. This is also not equivalent, because now the BN comes after the ReLu.\nIs there any reason to prefer one option over the other? Are there any other options I'm not considering?\n", "type": 1, "id": "16173", "date": "2019-10-31T11:34:50.637", "score": 1, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks"], "title": "Positioning of batch normalization layer when converting strided convolution to convolution + blurpool", "answer_count": 1, "views": 48, "accepted_answer": "16174", "answers": {"16174": {"line": 10643, "body": "After finding the paper authors' Github, I saw that, although they only have a MobileNet V2 model implemented, they choose the Subsample-after-ReLu option (the first one in the question).\nAlthough this doesn't fully answer my question, I'll take \"the paper authors do it this way\" as enough reason to prefer this over the alternative.\n", "type": 2, "id": "16174", "date": "2019-10-31T12:18:04.080", "score": 0, "comment_count": 1, "parent_id": "16173"}}}
{"line": 11416, "body": "I recently bought a Jetson Nano and I'm amazed with everything about it. But I don't know what is happening, because I created a very simple neural network with keras and it's taking way to long. I know is taking to long, because I runned the same ANN in my PC's CPU and it was faster than the jetson nano.\nHere's the code:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndataset = pd.read_csv('Churn_Modelling.csv')\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nclassifier = Sequential()\n\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n\nI should mention that of course, I did the correct installation of TensorFlow GPU library and not the normal TensorFlow, in fact I used the resources in this link: TensorFlow GPU Jetson Nano\n", "type": 1, "id": "17083", "date": "2019-12-13T18:19:47.407", "score": 1, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "python", "keras"], "title": "Deep Learning models train really slow Jetson Nano", "answer_count": 1, "views": 340, "accepted_answer": null, "answers": {"17086": {"line": 11419, "body": "First of all, you mentioned that you installed the correct version of Tensorflow on Jetson. You can list the available Tensorflow devices with:\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\nAnd make sure that you see the GPU available. \nMore importantly, Jetson Nano has a 128-core NVIDIA Maxwell(tm) architecture-based GPU. This might not be a powerful GPU for training a neural network with. As you can see here, in NVIDIA's official benchmark, they only tested trained models (i.e. classifications, object detections and segmentations). You can consider training your Keras model using another hardware and use it for labeling, classification, or etc. in your Jetson. \n", "type": 2, "id": "17086", "date": "2019-12-13T20:35:17.437", "score": 0, "comment_count": 0, "parent_id": "17083"}}}
{"line": 9711, "body": "In a convolutional neural network, when we apply the convolution on a $5 \\times 5$ image with $3 \\times 3$ kernel, with stride $1$, we should get only one $4 \\times 4$ as output. In most of the CNN tutorials, we are having $4 \\times 4 \\times m$ as output. I don't know how we are getting a three-dimensional output and I don't know how we need to calculate $m$. How is $m$ determined? Why do we get a three-dimensional output after a convolutional layer?\n", "type": 1, "id": "13999", "date": "2019-08-16T05:47:03.300", "score": 3, "comment_count": 0, "tags": ["convolutional-neural-networks", "computer-vision", "image-processing", "convolution"], "title": "Why do we get a three-dimensional output after a convolutional layer?", "answer_count": 2, "views": 108, "accepted_answer": "14002", "answers": {"14002": {"line": 9714, "body": "If you have a $h_i \\times w_i \\times d_i$ input, where $h_i, w_i$ and $d_i$ respectively refer to the height, width and depth of the input, then we usually apply $m$ $h_k \\times w_k \\times d_i$ kernels (or filters) to this input (with the appropriate stride and padding), where $m$ is usually a hyper-parameter. So, after the application of $m$ kernels, you will obtain $m$ $h_o \\times w_o \\times 1$ so-called feature maps (also known as activation maps), which are usually concatenated along the depth dimension, hence your output will have a depth of $m$ (given that the application of a kernel to the input usually produces a two-dimensional output). For this reason, the output is usually referred to as output volume.\nIn the context of CNNs, the kernels are learned, so they are not constant (at least, during the learning process, but, after training, they usually remain constant, unless you perform continual lifelong learning). Each kernel will be different from any other kernel, so each kernel will be doing a different convolution with the input (with respect to the other kernels), therefore, each kernel will be responsible for filtering (or detecting) a specific and different (with respect to the other kernels) feature of the input, which can, for example, be the initial image or the output of another convolutional layer.\n", "type": 2, "id": "14002", "date": "2019-08-16T08:31:34.373", "score": 2, "comment_count": 0, "parent_id": "13999"}, "14062": {"line": 9764, "body": "\nWhy do we get a three-dimensional output after a convolutional layer?\n\nDuring a search for an optimal convolution kernel via gradient descent or some other method there must be at least one additional dimension to represent trials.  It is most often one.  If the input is in $\\mathcal{R}^n$ space, then the output of the convolution operation is $\\mathcal{R}^{n+1}$ space.  However, this is not often (or ever) the output of the learning system using a convolutional layer, since the final layer in the conventional deep network designs used today is not the convolution layer.\nIn the case in this question, $m$ represents the number of discrete kernels tried, usually in rapid succession in most algorithms and hardware acceleration scenarios.  It is using the results in this $\\mathcal{R}^{n+1}$ space that the corrective mechanism requires to converge on an optimum efficiently.\nBy corrective mechanism is meant whatever corrects the assumptions made for the next set of kernels to be tried.  This mechanism often involves gradient descent and back propagation in an artificial network design.  It is the learning algorithm or, in the case of hardware acceleration, the learning circuit.\nThe value of $m$ is not arbitrary, but is largely problem dependent and based on hardware and execution environment.  If $m$ is too large, then too much convolution work is performed before the correction is made.  If $m$ is too small, then whatever efficiency is gained by grouping kernel tries together in time is lost.  There may be a formula to find $m$, but it will be based on platform dependent metrics and can be found by trying several $m$ values and determining the $m$ providing the lowest convergence time.\nThe value of $m$ can also affect accuracy and reliability of convergence.  Predicting this effect is not straightforward.  Such prediction, which would allow the automated selection of hyper-parameters like $m$, is of interest to many researchers for obvious reasons.  It has been and will probably continue to be an objective of AI development to remove the need for human intervention in AI system applications.\n", "type": 2, "id": "14062", "date": "2019-08-20T11:34:19.907", "score": 0, "comment_count": 0, "parent_id": "13999"}}}
{"line": 11489, "body": "I found the terms front-end and back-end in the article (or blog post) How to Develop a CNN for MNIST Handwritten Digit Classification. What do they mean here? Are these terms standard in this context?\n", "type": 1, "id": "17168", "date": "2019-12-19T09:58:09.173", "score": 0, "comment_count": 3, "tags": ["convolutional-neural-networks", "terminology"], "title": "What do the terms \"front-end\" and \"back-end\" refer to in this article?", "answer_count": 2, "views": 597, "accepted_answer": "17192", "answers": {"17192": {"line": 11507, "body": "I do not think these are formally defined.\nThe distinction is just to facilitate discussion of the NN architecture: e.g., you may have a few convolutional layers with pooling as a front-end, and a different architecture as a back-end (in a text-book architecture, just a fully-connected layer. But to get wild, maybe LSTM? To really get wild, BERT?).\nIn the end (no pun intended), computers do not care if a layer is seen by humans as a front-end or a back-end.\n", "type": 2, "id": "17192", "date": "2019-12-20T22:55:36.550", "score": 1, "comment_count": 0, "parent_id": "17168"}, "17224": {"line": 11534, "body": "I think that front end refers to a high level API for a CNN framework (c++ front end, Python front end).\nThe back end can be understood as a more peculiar (low level) interface to specific libraries. \nYou can use different back ends but still manipulate training data and model building process the same way using the front end (use Keras with TensorFlow, caffe with Pytorch, or the other way round use Theano, tensorflow, .. . with Keras!).\nYou can find some more material at the following links :\n\nhttps://keras.io/backend/\nhttps://project.inria.fr/deeplearning/files/2016/05/DLFrameworks.pdf\n\nI don't think it refers to neural network layers structure. The term shallow or deep layers are usually prefered.\n", "type": 2, "id": "17224", "date": "2019-12-23T16:56:38.980", "score": 0, "comment_count": 0, "parent_id": "17168"}}}
{"line": 10985, "body": "I'm studying a master's degree and my final work is going to be about the convolutional neural network.\nI read a lot of books and I did Convolutional Network Standford's course, but I need more.\nAre there books or papers on the details of convolutional neural networks (in particular, convolutional layer)? \n", "type": 1, "id": "16576", "date": "2019-11-17T17:27:06.503", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "reference-request", "papers", "books"], "title": "What are examples of books or papers on the details of convolutional neural networks?", "answer_count": 4, "views": 106, "accepted_answer": "16587", "answers": {"16587": {"line": 10995, "body": "Chapter 9 of the book Deep Learning (2016), by Goodfellow et al., describes the convolutional (neural) network (CNN), its main operations (namely, convolution and pooling) and properties (such as parameter sharing).\nThere's also the article From Convolution to Neural Network, which first introduces the mathematical operation convolution and then describes its connection with signal processing (where images can be viewed as 2D signals) and, finally, describes the CNN.\n", "type": 2, "id": "16587", "date": "2019-11-17T22:31:59.970", "score": 0, "comment_count": 0, "parent_id": "16576"}, "16640": {"line": 11044, "body": "You can look at the paper Gradient-Based Learning Applied to Document\nRecognition (1998) by Yann LeCun et al., which reviews and compares various methods applied to handwritten character recognition and shows that CNNs outperform all other methods.\nAlso, I suggest Andrew Ng's CNN videos.\n", "type": 2, "id": "16640", "date": "2019-11-19T22:03:34.600", "score": 0, "comment_count": 0, "parent_id": "16576"}, "16577": {"line": 10986, "body": "Chris Olah's work is always inspired, and not too technical as one would expect. He has several papers on CNNs on his website. In particular, check the series titled \"Convolutional Neural Networks\" with four papers on the topic.\n", "type": 2, "id": "16577", "date": "2019-11-17T17:39:45.187", "score": 1, "comment_count": 0, "parent_id": "16576"}, "16586": {"line": 10994, "body": "I'm not sure if this is what you are looking for but I find Goodfellow's book a pretty good resource:\nGoodfellow, specifically Section 2, Chapter 9 deals with convolutional neural networks: https://www.deeplearningbook.org/\n'Pattern Recognition and Machine Learning' by Bishop Might contains a section (5.5.5, pg 267 onwards) as well as an exercise, and a general discussion about neural networks in image recognition.\nIf you edit your question to post a bit more detail, we can offer better answers, for example, what is about the convolutional layer?  How it's implemented?\nIf you are looking for a more basic introduction to convolutional layers I would also suggest:\nA Comprehensive Guide to Convolutional Neural Networks -- the ELI5 way gives a pretty general overview, starting at the difference between CNNs and ANNs and explains why CNNs are superior to ANNs (for certain problems).  It also gives some details about how the convolution actually works.\nDemystifying the transpose convolution explains the transpose convolution operation in the context of how a traditional convolution; this may not be relevant if you are strictly using CNNs and not transpose-CNNs.\nUnderstanding of Convolutional Neural Network (CNN) -- Deep Learning is quite similar to \"A Comprehensive...\" link above, but it also includes information about filtering and shows the effect that different filters have on an image, which is certainly very import to an understanding of why we use CNNs.\nBuilding a Convolutional Neural Network (CNN) in Keras (or one of the other thousand similar pages) are pretty good for just starting out and building your own CNN classifier.  You can also check out examples from Keras, e.g. CIFAR10 CNN, but these tend to give you a very little information about why they designed the network the way that they did.\nIf, on the other hand, you are looking for some more advanced resources, here are is one that springs to mind:\nDeep Residual Learning for Image Recognition by He et al., deals with a major advance in image recognition, using Residual Networks (ResNet).  This type of network has become pretty popular, so I highly recommend giving it a read.\n", "type": 2, "id": "16586", "date": "2019-11-17T22:30:06.063", "score": 0, "comment_count": 3, "parent_id": "16576"}}}
{"line": 12367, "body": "What's the difference between LSTM and RNN? I know that RNN is a layer used in neural networks, but what exactly is an LSTM? Is it also a layer with the same characteristics?\n", "type": 1, "id": "18198", "date": "2020-02-23T20:36:06.570", "score": 1, "comment_count": 0, "tags": ["neural-networks", "comparison", "recurrent-neural-networks", "long-short-term-memory", "recurrent-layers"], "title": "What's the difference between LSTM and RNN?", "answer_count": 1, "views": 2152, "accepted_answer": "18199", "answers": {"18199": {"line": 12368, "body": "RNNs have recurrent connections and/or layers\nYou can describe a recurrent neural network (RNN) or a long short-term memory (LSTM), depending on the context, at different levels of abstraction. For example, you could say that an RNN is any neural network that contains one or more recurrent (or cyclic) connections. Or you could say that layer $l$ of neural network $N$ is a recurrent layer, given that it contains units (or neurons) with recurrent connections, but $N$ may not contain only recurrent layers (for example, it may also be composed of feedforward layers, i.e. layers with units that contain only feedforward connections).\nIn any case, a recurrent neural network is almost always described as a neural network (NN) and not as a layer (this should also be obvious from the name).\nLSTM can refer to a unit, layer or neural network\nOn the other hand, depending on the context, the term \"LSTM\" alone can refer to an\n\nLSTM unit (or neuron),\nan LSTM layer (many LSTM units), or\nan LSTM neural network (a neural network with LSTM units or layers).\n\nPeople may also refer to neural networks with LSTM units as LSTMs (plural version of LSTM).\nLSTMs are RNNs\nAn LSTM unit is a recurrent unit, that is, a unit (or neuron) that contains cyclic connections, so an LSTM neural network is a recurrent neural network (RNN).\nLSTM units/neurons\nThe main difference between an LSTM unit and a standard RNN unit is that the LSTM unit is more sophisticated. More precisely, it is composed of the so-called gates that supposedly regulate better the flow of information through the unit.\nHere's a typical representation (or diagram) of an LSTM (more precisely, an LSTM with a so-called peephole connection).\n\nThis can actually represent both an LSTM unit (and, in that case, the variables are scalars) or an LSTM layer (and, in that case, the variables are vectors or matrices).\nYou can see from this diagram that an LSTM unit (or layer) is composed of gates, denoted by\n\n$i_t$ (the input gate: the gate that regulates the input into the unit/layer),\n$o_t$ (the output gate: the gate that regulates the output from the unit)\n$f_t$ (the forget gate: the gate that regulates what the cell should forget)\n\nand recurrent connections (e.g. the connection from the cell into the forget gate and vice-versa).\nIt's also composed of a cell, which is the only thing that a neuron of a \"vanilla\" RNN contains.\nTo understand the details (i.e. the purpose of all these components, such as the gates), you could read the paper that originally proposed the LSTM by S. Hochreiter and J. Schmidhuber. However, there may be other more accessible and understandable papers, articles or video lessons on the topic, which you can find on the web.\nLSTMs also have recurrent connections!\nGiven the presence of cyclic connections, any recurrent neural network (either an LSTM or not) may be represented as a graph that contains one or more cyclic connections. For example, the following diagram may represent both a standard/vanilla RNN or an LSTM neural network (or maybe a variant of it, e.g. the GRU).\n\nWhen should you use RNNs and LSTMs?\nRNNs are particularly suited for tasks that involve sequences (thanks to the recurrent connections). For example, they are often used for machine translation, where the sequences are sentences or words. In practice, an LSTM is often used, as opposed to a vanilla (or standard) RNN, because it is more computationally effective. In fact, the LSTM was introduced to solve a problem that standard RNNs suffer from, i.e. the vanishing gradient problem. (Now, for these tasks, there are also the transformers, but the question was not about them).\n", "type": 2, "id": "18199", "date": "2020-02-23T21:39:37.643", "score": 0, "comment_count": 0, "parent_id": "18198"}}}
{"line": 12574, "body": "I'm currently trying to predict 1 output value with 52 input values. The problem is that I only have around 100 rows of data that I can use. \nWill I get more accurate results when I use a small architecture than when I use multiple layers with a higher amount of neurons? \nRight now, I use 1 hidden layer with 1 neuron, because of the fact that I need to solve (in my opinion) a basic regression problem. \n", "type": 1, "id": "18439", "date": "2020-03-05T13:04:17.107", "score": 4, "comment_count": 0, "tags": ["neural-networks", "ai-design", "datasets", "regression", "architecture"], "title": "Is a basic neural network architecture better with small datasets?", "answer_count": 2, "views": 110, "accepted_answer": null, "answers": {"18450": {"line": 12584, "body": "It's harder to overfit it certainly!\nI mean practically speaking there has to be some assumptions on the generation model of your data, either explicit or implicit.\nI would try probably 1-2 layer network first(maybe your data is linearly separable if you're lucky). \n", "type": 2, "id": "18450", "date": "2020-03-05T21:34:18.527", "score": 0, "comment_count": 0, "parent_id": "18439"}, "27090": {"line": 18549, "body": "I'm not aware of a direct way for finding the best NN architecture for a given task, but the recommended way, as far as I know, is to devise a network that can overfit the training data, and then apply regularization on top of it.\nThat way, you can be almost sure you're not underfitting/underperforming due to network capacity.\n", "type": 2, "id": "27090", "date": "2021-03-31T02:51:52.447", "score": 1, "comment_count": 0, "parent_id": "18439"}}}
{"line": 10191, "body": "Let's assume I have a CNN model trained to categorize some objects on the images. By using this model I find more categorized images. If I now retrain this model on data set that consists old set and newly categorised images is there a chance that such new model will have higher accuracy? Or maybe because new data posses only information that could be found on initial set, model will have similar/lower accuracy?\nPlease let me know if something unclear. \n", "type": 1, "id": "15622", "date": "2019-09-25T14:20:06.840", "score": 3, "comment_count": 0, "tags": ["convolutional-neural-networks", "image-recognition", "classification"], "title": "Can a model, retrained on images classified previously by itself, increase its accuracy?", "answer_count": 1, "views": 50, "accepted_answer": "15635", "answers": {"15635": {"line": 10200, "body": "The most likely outcome of this approach is wasted time and very little effect on accuracy.\nThere will be changes to the model. Some will be beneficial and improve the model, but some will backfire making it worse.\nFor instance:\n\nThe model predicts with probability 0.4 that an image is in a certain class. It is the highest prediction, and actually true. It will be added to the training dataset with a \"ground truth\" of probability 1.0, so on balance more and better data has been added to the data set. This will improve generalisation, as whatever caused the relatively low 0.4 value initially - e.g. a pose or lighting variation - will now be covered correctly in the training set.\nThe model predicts with probability 0.4 that an image is in a certain class. It is the highest prediction, and actually false. It will be added to the dataset with \"ground truth\" of probability 1.0 for the wrong class. This will weaken associations to the correct class for similar input images, meaning for exaple that a certain pose or lighting difference that is already causing problems for the model will be used to incorrectly classify images in future.\n\nThese two scenarios will occur, on average, at a rate determined by the model's current test accuracy. So if your current model is 90% accurate, 1 in 10 images in your new training data will be mislabelled. This will  \"lock in\" the current errors at the same rate on average as they already occur.\nThe effect may be a drift up or down in accuracy as the model will definitely change due to the new training data. However you have little to no control over how this drift effect goes if you are not willing or able to oversee the automatic classifications generated on new data by the model.\nThere are a few ways to get some improvement unsupervised from new data. For instance:\n\nBuild an autoencoder from the early convolutional layers of your model and train it to re-generate all inputs as outputs. This should help it learn important features of the variations in data that you are using. Once this training is done, discard the decoder part of the auto-encoder and add your classifier back in to fine tune it. This may help if you have only a small amount of labelled data, but a lot of unlabelled data.\nUse a model that has better accuracy than yours to auto-label the data. This might seem a little chicken-and-egg, but you may be able to create such a model using ensemble techniques. The ensemble model could be too awkward to use in production, but may still be used in an auto-labeling pipeline to improve your training data.\n\nNote you may get even better results simply ignoring the extra unlabeled data, and instead fine tuning a high quality ImageNet-trained model on the labeled data you already have - saving yourself a lot of effort. Depends on the nature of the images, and how much labeled data you are already working with.\n", "type": 2, "id": "15635", "date": "2019-09-26T11:25:46.587", "score": 0, "comment_count": 2, "parent_id": "15622"}}}
{"line": 10641, "body": "I have an image classification task to solve, but based on quite simple/good terms:\n\nThere are only two classes (either good or not good)\nThe images always show the same kind of piece (either with or w/o fault)\nThat piece is always filmed from the same angle & distance\nI have at least 1000 sample images for both classes\n\nSo I thought it should be easy to come up with a good CNN solution - and it was. I created a VGG16-based model with a custom classifier (Keras/TF). Via transfer learning I was able to achieve up to 100% validation accuracy during model training, so all is fine on that end.\nOut of curiosity and because the VGG-based approach seems a bit \"slow\", I also wanted to try it with a more modern model architecture as the base, so I did with ResNet50v2 and Xception. I trained both similar to the VGG-based model, tried it with several hyperparameter modifications, etc. However, I was not able to achieve a better validation accuracy than 95% - so much worse than with the \"old\" VGG architecture.\nHence my question is: \n\nGiven these \"simple\" (always the same) images and only two classes, is the VGG model probably a better base than a modern network like ResNet or Xception? Or is it more likely that I messed something up with my model or simply got the training/hyperparameters not right?\n\n", "type": 1, "id": "16172", "date": "2019-10-31T10:19:48.970", "score": 1, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "residual-networks", "vgg"], "title": "Is a VGG-based CNN model sometimes better for image classfication than a modern architecture?", "answer_count": 3, "views": 663, "accepted_answer": "16181", "answers": {"16181": {"line": 10648, "body": "VGG is a more basic architecture which uses no residual blocks. Reset usually perform better then VGG due to it's more layers and residual approach. Given that resnet-50 can get 99% accuracy on MNIST and 98.7% accuracy on CIFAR-10, it probably should achieve better than VGG network. Also, the validation accuracy should not be 100%. You could try increasing the size of your validation set to improve accuracy on validation. VGG network should perform worst than ResNet in most scenario, but experimenting is the way to go. Try and experiment more to get a method that works for your data. Hope that I can help you and have a nice day!\n", "type": 2, "id": "16181", "date": "2019-10-31T16:04:59.380", "score": 0, "comment_count": 0, "parent_id": "16172"}, "20224": {"line": 13226, "body": "Below is a listing of Keras application models that can be used easily in transfer learning. Note VGG has on the order of 140 million parameters which is why it is slow.\n\n\nModel               Size     Top-1 Accuracy  Top-5 Accuracy  Parameters    1Depth\nXception             88 MB      0.790           0.945         22,910,480    126\nVGG16               528 MB      0.713           0.901        138,357,544    23\nVGG19               549 MB      0.713           0.900        143,667,240    26\nResNet50             98 MB      0.749           0.921         25,636,712    -\nResNet101           171 MB      0.764           0.928         44,707,176    -\nResNet152           232 MB      0.766           0.931         60,419,944    -\nResNet50V2           98 MB      0.760           0.930         25,613,800    -\nResNet101V2         171 MB      0.772           0.938         44,675,560    -\nResNet152V2         232 MB      0.780           0.942         60,380,648    -\nInceptionV3          92 MB      0.779           0.937         23,851,784    159\nInceptionResNetV2   215 MB      0.803           0.953         55,873,736    572\nMobileNet            16 MB      0.704           0.895          4,253,864    88\nMobileNetV2          14 MB      0.713           0.901          3,538,984    88\nDenseNet121          33 MB      0.750           0.923          8,062,504    121\nDenseNet169          57 MB      0.762           0.932         14,307,880    169\nDenseNet201          80 MB      0.773           0.936         20,242,984    201\nNASNetMobile         23 MB      0.744           0.919          5,326,716    -\nNASNetLarge         343 MB      0.825           0.960         88,949,818    -\n\nI tend to use the MobileNet model for transfer learning because it has about 4 million\nparameters so it much faster than most models. It should perform as well as VGG on your\ndata set. If it does not tuning the hyper parameters may be required. I find that using\nan adjustable learning such as the Keras ReduceLROnPlateau callback along with the\nModelCheckpoint callback both monitoring validation loss works very well. Documentation\nis [here][1].\nYou might also try the efficientNet model which comes in various sizes and has high \naccuracy. Documentation is [here][2]\n\n\n  [1]: https://keras.io/callbacks/\n  [2]: https://github.com/Tony607/efficientnet_keras_transfer_learning\n\n", "type": 2, "id": "20224", "date": "2020-04-13T14:32:50.753", "score": 0, "comment_count": 0, "parent_id": "16172"}, "24987": {"line": 16843, "body": "The newer models generally outperform older ones on the ImageNet challenge in their accuracy scores*. This does not necessarily mean that this difference in performance will be reflected in your particular classification problem.\nThe closer your problem is to the ImageNet one, the more likely that the relative model performances will be similar. However when you perform transfer learning you will often have to fine-tune the model to achieve a stronger performance, the better you tune the model will effect performance, and there will often be a difference in which model is performing best on a given task. You can see papers in various classification tasks where VGG may be performing best, or Inception, or even AlexNet. I believe the simplest models (AlexNet has only 8 layers) may be the easiest to fine tune, and also may require the smallest amount of data for good performance.\n*There are exceptions, MobileNet is more recent but the innovation is that it is a smaller model rather than the strongest model i.e. it is designed to be useable on mobile devices rather than running on the latest GPU.\n", "type": 2, "id": "24987", "date": "2020-12-04T05:04:52.403", "score": 0, "comment_count": 0, "parent_id": "16172"}}}
{"line": 10819, "body": "I am working on a problem in which I am attempting to find a stable region in a spiral galaxy. The PI I'm working with asked me to use machine learning as a tool to solve the problem. I have created some visualizations of my data, as bellow.\n\nIn this image, you can see there is a flat region between 0 and roughly 30 pixels, and between 90 pixels and 110 pixels. I have received suggestions to use an RNN LSTM model that can identify flat regions, but I wanted to hear other suggestions of other neural network models as well.\nThe PI I'm working with suggests to feed my data visualization images into a neural network and have the neural network identify said stable regions. Can this be done using a neural network, and what resources would I have to look at? Moreover, can this problem be solved with RNN LSTM? I think the premise of this was to treat the radius as some temporal dimension. I've been extensively looking for answers online, and I cannot quite seem to find any similar examples.\n", "type": 1, "id": "16383", "date": "2019-11-09T22:39:58.420", "score": 4, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "python", "keras", "long-short-term-memory"], "title": "Using a neural network to identify a stable region within a set of data?", "answer_count": 2, "views": 58, "accepted_answer": null, "answers": {"16390": {"line": 10826, "body": "In image processing CNNs are usually used to create weighted filters for focusing in on the image features which are most important for making predictions. Keras is one of the libraries used to examine images in this way. With this type of analysis you will need labeled and unlabeled data you want to create a network that inputs a photo extracts the flat black line regions and outputs those. The model will be generative, generating guesses of regions where the function is flat. This is all possible to do but in order to label the data you need to label them by hand or you need to create a function that manually labels them which would not be very difficult. The input nodes will take in the pixels of the picture and the output layer will be guesses at location along the graph of wether the section is flat or not. It seems overkill to do this with a neural network when it is possible to not use a NN and creating a labeling method will most likely be your first step. If you have any questions please ask.\n", "type": 2, "id": "16390", "date": "2019-11-10T02:27:50.810", "score": 0, "comment_count": 0, "parent_id": "16383"}, "16421": {"line": 10855, "body": "If you're really just trying to find long contiguous flat regions in a sequence, you do not need machine learning. Your PI is mistaken. You would be better off simply writing a short data processing program. Your program could find the finite differences between adjacent datapoints, and then count whether a long string of them are below some threshold to identify long flat regions. This will be faster, simpler, and perhaps more accurate than using ML on data visualizations for this task.\nIf you are trying to find something more complex than these long flat regions, you could instead train an LSTM on the raw sequential data that you are using to generate the images. Again, that will probably be more accurate than trying to train a CNN, or any non-sequential model, on the image data itself.\n", "type": 2, "id": "16421", "date": "2019-11-11T01:08:50.823", "score": 1, "comment_count": 0, "parent_id": "16383"}}}
{"line": 13059, "body": "After transforming timeseries into an image format, I get a width-height ratio of ~135. Typical image CNN applications involve either square or reasonably-rectangular proportions - whereas mine look nearly like lines:\n\nExample dimensions: (16000, 120, 16) = (width, height, channels).\nAre 2D CNNs expected to work well with such aspect ratios? What hyperparameters are appropriate - namely, in Keras/TF terms, strides, kernel_size (is 'unequal' preferred, e.g. strides=(16, 1))? Relevant publications would help.\n\nClarification: width == timesteps. The images are obtained via a transform of the timeseries, e.g. Short-time Fourier Transform. channels are the original channels. height is the result of the transform, e.g. frequency information. The task is binary classification of EEG data (w/ sigmoid output).\nRelevant thread\n", "type": 1, "id": "20026", "date": "2020-04-05T18:08:08.727", "score": 0, "comment_count": 7, "tags": ["convolutional-neural-networks", "tensorflow", "python", "keras", "image-processing"], "title": "How to handle extremely 'long' images?", "answer_count": 1, "views": 103, "accepted_answer": null, "answers": {"20029": {"line": 13062, "body": "I had recently used a slightly unorthodox method to process such images, which involved using RNNs.\nAssume the image dimensions to be (16000, 120, 16) = (width, height, channels), as in the question.\nApply a 2D convolution (or multiple such convolutions) of shape(1, k, c),\nsuch that the output of the convolutions becomes (16000, 1, c). So if you only use a single convolutional layer, k=120.\nThen, squeeze the extra dimension, to get the shape (16000, c). \nThe problem has now been transformed back into a sequence problem! You can use RNN variants for further processing.\n", "type": 2, "id": "20029", "date": "2020-04-06T03:53:15.877", "score": 0, "comment_count": 1, "parent_id": "20026"}}}
{"line": 10414, "body": "I have a game/simulation that takes a vector of encoded sequences of moves (up, down, left, right). Let's say that these are sequential step taken by an ant moving in a 2D space, starting from the origin. The moves are generated randomly. \nI want to know for any game, if the ant gets farther than a certain distance y from the origin (although it might even be closer than y at the end of the game). I would like to classify games into \"ant gets further away than y\" with value of one, or zero for \"ant does not get further away than y\". I don't need an AI for this task, I have set this objective as a training goal for myself. \nI am able to tell if the last position is past y or not, using a regular feed forward network, I believe it is easier because it is as easy as summing up all the moves, regardless of the order. But to tell if the ant got past y and then got back, that still needs to return one.\nI thought I might be able to reach my objective through an RNN, encoding the moves as a sequence of one-hot encoded sequential directions to move towards. Currently, I am using one hidden layer (I tried with different sizes ranging from 10 to 100), backpropagating the loss only at the last step of a single training on a vector, but it seems like the RNN total loss doesn't decrease at all.\nIs there any obivious flaw in my simulation, or in the neural network model? Is there a category of problems this could belong to?\n", "type": 1, "id": "15924", "date": "2019-10-15T20:52:04.270", "score": 1, "comment_count": 3, "tags": ["recurrent-neural-networks"], "title": "Recurrent Neural Network to track distance from origin", "answer_count": 1, "views": 72, "accepted_answer": "15928", "answers": {"15928": {"line": 10416, "body": "This kind of problem does not really have a name other than \"toy problem\" since no-one needs to teach an AI to add up, multiply or divide* - there are already far more reliable and far faster ways to achieve that on any computer. What you are doing here is essentially vector addition, applying a distance metric then setting a true/false value based on a comparison. It would be 2 or 3 lines of code in most high-level programming languages.\nNeural networks can learn any function though, so in theory what you want to do is possible. You should not expect results to be perfect, a statistical learner never actually learns the analytical form of a function or process, just the rough \"shape\" of it.\nI have not done your experiment. However, your idea to use a RNN seems reasonable. With the details you have given, I can offer a few pieces of general advice:\n\nUse a modern RNN gated architecture, either LSTM or GRU. That's because the point in the sequence of moves where you want to set a \"distance exceeded\" toggle could be many steps away from the end of a game. The simplest RNNs (with direct loop backs from output to input within a layer) can easily suffer from vanishing gradients in this situation, whilst LSTM and GRU architectures are designed to deal with it.\nGenerate a lot of training data. You will need many examples of both categories before any neural network will home in on what is causing one class or the other to occur. The learning is based on statistics, not reason.\nTake a look at related LTSM examples that learn to add binary numbers. Repeat those even simpler experiments first, then move on to your own problem. This will avoid some beginner mistakes with poor choices of hyper-parameters, bad implementations etc.\n\n\n* Humans of course do use intelligence, reasoning and logic to learn reliable procedures for addition, multiplication and division. No doubt someone could be interested in how an AI can replicate that, without starting from built-in capabilties of a CPU (which of course the humans designed and built those procedures into the system at a low level). However, that's at a higher level of AI research than we are dealing with here. \n", "type": 2, "id": "15928", "date": "2019-10-16T06:47:01.310", "score": 0, "comment_count": 1, "parent_id": "15924"}}}
{"line": 11076, "body": "I am looking to build a neural network that takes an input vector $\\mathbf{X}$ and outputs a vector $\\mathbf{Y}$ such at $f(\\mathbf{X}, \\mathbf{Y})$ is minimized, where $f$ is some function. The network will see many different $\\mathbf{X}$ during training to adjust its weights and biases; then I will test the network by using the test set $\\{x_1, \\dots, x_n \\}$ to calculate $\\sum(f(x_1, y), \\dots, f(x_n, y))$ to see if this sum is minimized.\nHowever, I have no labels for the output $\\mathbf{Y}$. The loss function I am trying to minimize is based on the input and output instead of the output and label. I tried many standard Keras and TensorFlow loss functions, but they are unable to do the job. Any thoughts on how this might be achieved? \n", "type": 1, "id": "16677", "date": "2019-11-20T22:24:13.327", "score": 5, "comment_count": 3, "tags": ["neural-networks", "optimization", "unsupervised-learning"], "title": "Unsupervised learning to optimize a function of the input", "answer_count": 2, "views": 119, "accepted_answer": null, "answers": {"16678": {"line": 11077, "body": "According to your description, you already know your function $f$ to be optimized. So you should use it directly instead of the standard loss functions. In this other post there is an explanation of how to use $f$ as a custom loss function in Keras.\n", "type": 2, "id": "16678", "date": "2019-11-21T00:32:28.807", "score": 1, "comment_count": 0, "parent_id": "16677"}, "27320": {"line": 18734, "body": "From your question, it sounds like your only training data is {1,...,} and the network has to magically come up with values {y1,...,y} such that an unknown function is minimized. How do you plan to give feedback to the network during training?\nYour situation appears to be something like this:\nX-->Model-->Y-->f(X,Y)\nwhere X is being copied from the first layer to the last layer using a non-sequential architecture.\nThe solution to this problem would be to add an extra layer to your network that implements f(X, Y). However, this will only be trainable using standard methods like gradient descent if f(X, Y) is differentiable. If f(X, Y) is not differentiable, then you may need to use a different optimization method for learning the weights of the model and it may be more difficult. Particle swarm optimization is one possibility here.\n", "type": 2, "id": "27320", "date": "2021-04-14T06:21:29.353", "score": 0, "comment_count": 0, "parent_id": "16677"}}}
{"line": 11538, "body": "I understood that we normalize to input features in order to bring them on the same scale so that weights won't be learned in arbitrary fashion and training would be faster. \nThen I studied about batch-normalization and observed that we can do the normalization for outputs of the hidden layers in following way:\nStep 1: normalize the output of the hidden layer in order to have zero mean and unit variance a.k.a. standard normal (i.e. subtract by mean and divide by std dev of that minibatch).\nStep 2: rescale this normalized vector to a new vector with new distribution having $\\beta$ mean and $\\gamma$ standard deviation, where both $\\beta$ and $\\gamma$ are trainable.\nI did not understand the purpose of the second step. Why can't we just do the first step, make the vector standard normal, and then move forward? Why do we need to rescale the input of each hidden neuron to an arbitrary distribution which is learned (through beta and gamma parameters)?\n", "type": 1, "id": "17228", "date": "2019-12-23T22:52:44.147", "score": 2, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "batch-normalization"], "title": "How does a batch normalization layer work?", "answer_count": 1, "views": 580, "accepted_answer": null, "answers": {"17232": {"line": 11540, "body": "Definition and Explaination\nFor how Batch Normalization works exactly, I'll suggest you to read the following papers:\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\nHow Does Batch Normalization Help Optimization?\n\nThe recent interpretation on How BN works is that it can reduce the high-order effect as mentioned in Ian Goodfellow's lecture. So it's not really about reducing the internal covariate shift.\nIntuition\nFor how it works intuitively, you can think that we want to normalize the intermediate outputs (zero mean and unit variance) if the normalization won't remove too much useful information.\nHowever, normalization may not be suitable for all intermediate outputs. So $\\beta$ and $\\gamma$ is introduced to provide additional flexibility, if normalization removes too much useful information then $\\beta$ and $\\gamma$ will learn to become the original mean and variance, making the BN layer an identity transformation, as if it doesn't exist.\nIn practice, $\\beta$ and $\\gamma$ won't become the original mean and variance, since all intermediate outputs can be normalized in some certain way without losing too much useful information. So you can think of it to be a customized normalization for each BN layer.\ntl;dr\nBN layer normalize the intermediate outputs in default, however, if the neural network find out that these intermediate outputs should not be normalized, then the neural network undos or provide more flexibility to the normalization.\n", "type": 2, "id": "17232", "date": "2019-12-24T10:47:30.020", "score": 0, "comment_count": 3, "parent_id": "17228"}}}
{"line": 13571, "body": "I wonder what happens to the 'channels' dimension (usually 3 for RGB images) after the first convolution layer in CNNs? \nIn books and other sources, it is always said that the depth of the output from convolutional layers is the number of kernels (filters) in that layer. \nBut, if the input image has 3 channels and we convolve each of them with $K$ kernels, shouldn't the depth of the output be $K * 3$? Are they somehow 'averaged' or in other way combined with each other?\n", "type": 1, "id": "20676", "date": "2020-04-27T11:03:21.857", "score": 0, "comment_count": 3, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "filters"], "title": "What happens to the channels after the convolution layer?", "answer_count": 1, "views": 224, "accepted_answer": "20737", "answers": {"20737": {"line": 13622, "body": "Answer to my question is that values obtained from convolutions among different channels sum up together, therefore 3 channels after convolution with one filter give one output.\nBest explanation delivered by Andrew: https://www.coursera.org/lecture/convolutional-neural-networks/convolutions-over-volume-ctQZz\n", "type": 2, "id": "20737", "date": "2020-04-28T16:18:51.220", "score": 0, "comment_count": 1, "parent_id": "20676"}}}
{"line": 11786, "body": "I am training a convolutional neural network for object detection. Apart from the learning rate, what are the other hyperparameters that I should tune? And in what order of importance? Besides, I read that doing a grid search for hyperparameters is not the best way to go about training and that random search is better in this case. Is random search really that good?\n", "type": 1, "id": "17512", "date": "2020-01-15T09:04:46.550", "score": 4, "comment_count": 6, "tags": ["convolutional-neural-networks", "training", "computer-vision", "optimization", "hyperparameter-optimization"], "title": "When training a CNN, what are the hyperparameters to tune first?", "answer_count": 1, "views": 330, "accepted_answer": null, "answers": {"17638": {"line": 11892, "body": "Firstly when you say an object detection CNN, there are a huge number of model architectures available. Considering that you have narrowed down on your model architecture a CNN will have a few common layers like the ones below with hyperparameters you can tweak:\n\nConvolution Layer:- number of kernels, kernel size, stride length, padding\nMaxPooling Layer:- kernel size, stride length, padding\nDense Layer:- size\nDropout:- Percentage to keep/drop\n\n", "type": 2, "id": "17638", "date": "2020-01-23T14:03:05.517", "score": 0, "comment_count": 1, "parent_id": "17512"}}}
{"line": 12970, "body": "I have read a lot of information about several notions, like batch_size, epochs, iterations, but because of explanation was without numerical examples and I am not native speaker, I have some kind of problem of understanding still about those terms, so I decided to work with data. Let us suppose we have the following data\n\nOf course, it is just subset of original data, and I want to build a neural network with three hidden layers, the first layer contains 500 nodes, it takes input three variable and on each node, there is sigmoid activation function, next layer contains 100 node and sigmoid activation, the third one contains 50 node and sigmoid again, and finally we have one output with sigmoid to convert the result into 0 or 1 that classify whether a person with those attributes is female or male.\nI trained the model using Keras Tensorflow with the following code\nmodel.fit(X_train,y_train,epochs=30)\n\nWith this data, what does mean epochs=30? Does it mean that all 177 rows  (with 3 input at times) will go to the model 30 times? What about batch_size=None in model.fit parameters? \n", "type": 1, "id": "19922", "date": "2020-04-01T22:48:55.417", "score": 0, "comment_count": 4, "tags": ["neural-networks", "deep-learning", "training", "keras", "datasets"], "title": "What does it mean to have epochs=30 in Keras' fit method given certain data?", "answer_count": 2, "views": 231, "accepted_answer": "19937", "answers": {"19937": {"line": 12979, "body": "Batch size and epochs are independent parameters - they serve very different purposes. Your main question as I understand it (and for general, non-library specific consumption) is what is an epoch and how is the data used for each epoch?\nSimply put, an epoch is a single iteration though the training data. Each and every sample from your training dataset will be used once per epoch, whether it is for training or validation. Therefore, the more epochs, the more the model is trained. The key is to identify the number of epochs that fits the model to the data without overfitting.\nYour explaination of how batch size affects the training process is correct but not relevant to the question since it has no relation to the epoch training iterations. That is not to say that these values should be considered independently since they have similar effects on the model training process.\n", "type": 2, "id": "19937", "date": "2020-04-02T14:19:12.057", "score": 1, "comment_count": 0, "parent_id": "19922"}, "19924": {"line": 12972, "body": "ok  so let me explain in my word how i  understood this process:\ni know that one sample mean one row, therefore if we have data with size(177,3), that means  we have 177 sample. because we have divided  X and y into training and test, therefore we have following pairs (X_train,y_train)  and (X_test, y_test) \nnow about batch size, if we have  let say 177 sample(177 row) and  2 batch_size , that means we have approximately  $177/2$ batch right?update process goes  like this: \nlet us suppose network takes 3 input and produce one output, from first sample of data, three data will go to the network and  output will be  generated, this output will be compared to the  first value of y_train and  cost function will be created, then next  sample will go(it means next three value) and compared to the second value of y_train, also second cost function will be generated, final cost function for first batch will be sum of those cost functions and using gradient method weights are updated, after that one new batches will go through the network and on the based on updated weights, new weights are generated, when all  $177/2$  batch will be finished , it will be our 1 epoch right? is that correct? \n", "type": 2, "id": "19924", "date": "2020-04-01T23:55:59.253", "score": 0, "comment_count": 0, "parent_id": "19922"}}}
{"line": 12855, "body": "I know they are not the same in working, but an input layer sends the input to $n$ neurons with a set of weights, based on these weights and the activation layer, it produces an output that can be fed to the next layer.\nAren't the filters the same, in the way that they convert an \"image\" to a new \"image\" based on the weights that are in that filter? And that the next layer uses this new \"image\"?\n", "type": 1, "id": "18785", "date": "2020-03-24T23:28:53.160", "score": 4, "comment_count": 0, "tags": ["convolutional-neural-networks", "multilayer-perceptrons", "artificial-neuron", "weights", "filters"], "title": "Can neurons in MLP and filters in CNN be compared?", "answer_count": 2, "views": 293, "accepted_answer": "18796", "answers": {"18796": {"line": 12863, "body": "tl;dr The equivalent to a neuron in a Fully-Connected (FC) layer is the kernel (or filter) of a Convolution layer\nDifferences\nThe neurons of these two types of layers have two key differences. These are that the convolution layers implement:\n\nSparse connectivity, i.e. each neuron is connected only to an area of the input, not the whole.\nWeight sharing, i.e. similar connections end up having the same weights. This is usually visualized as the same filter traversing the image.\n\nBesides these two key differences, there are some other technical details, e.g. how the biases are implemented. Other than that they perform the same operation.\nWhat causes some confusion is that the input of a CNN is usually 2 or 3-dimensional, while a FC is usually 1-dimensional. These aren't mandatory however. To better help visualize the differences between the two I made a couple of figures illustrating the differences between a conv-layer and a FC one, both in 1D.\nSparse connectivity\nOn the left are two FC neural networks. On the right, are layers with sparse connections.\n\nWeight sharing\nOn the left is a sparsely connected network. The colors represent the different values of the weights. On the right is the same network with weight-sharing. Note that similar weights (i.e. arrows with the same direction in each layer) have the same value.\n\n\nTo answer your other questions:\n\nAre filters not the same in the way that they convert an \"image\" to a new \"image\" based on the weights that are in that filter? And that the next layers use these new \"images\"?\n\nYes, if the input of a convolution layer is an image, so will the output. The next layer will also operate on an image.\nHowever, I'd like to note that not all convolution layers accept images as their inputs. There are 1D and 3D convolutional layers as well.\n", "type": 2, "id": "18796", "date": "2020-03-25T13:31:43.563", "score": 3, "comment_count": 0, "parent_id": "18785"}, "25850": {"line": 17579, "body": "The other answer gives a good overview of the differences between MLPs and CNNs, and it includes 2 diagrams that attempt to illustrate the main differences between MLPs and CNNs, i.e. sparse connectivity and weight sharing. However, these diagrams do not clarify what a neuron in a CNN could be. A better diagram, which illustrates what a neuron is in a CNN, from a CNN and MLP perspective, is the following (taken from the famous article on CNNs).\n\nHere, there are 2 main blocks (aka volumes): the orange block on the left (the input) and the blue/cyan volume on the right (the feature maps, i.e. the outputs of the convolutional layer, i.e. after the application of the convolutions with different kernels).\nThe circles in the visible stack of the cyan block represent the neurons (or, more precisely, their activations or outputs). We only see $k=5$ neurons stacked: this corresponds to the application of $k=5$ different kernels (i.e. weights) to that specific subset of the input (aka receptive field), hence the sparse connectivity of CNNs. So, these neurons, in the same stack, are looking at the same small subset of the input, but with different weights (i.e. kernels). The neurons, which are not shown in this diagram, that are on the same (vertical) 2d plane (known as feature map) of the same neuron (e.g. the first that we see from left to right) in the cyan volume are the neurons that share the same weights, i.e. we use the same kernel to produce their outputs.\nSo, in this biological/neuroscientific view of the CNN, when you apply the convolution (or cross-correlation) operation with 1 specific filter (or kernel), you are computing the activation (not to be confused with the activation function, which is used to compute the activation!) i.e. the output of multiple neurons, all of them share the same weights. You stack all these activations on the same 2d plane (known as feature map) of the output volume: note that this operation is just the convolution operation! When you compute the convolution with another kernel, you are again computing the activation of other multiple neurons, which share another different weight matrix, and so on and so forth.\nSome authors prefer to use the term convolutional networks, i.e. without the term neural, probably because of this issue, i.e. it's not clear, especially to newcomers, what a neuron would be in a CNN, so the neuroscientific/biological view of CNNs is not always clear, although it's important to emphasize that CNNs were inspired by the visual cortext, so this biological interpretation could (and should) be more widely known or less confusing/misunderstood.\nNow, let's address your question more directly.\n\nAren't the filters the same, in the way that they convert an \"image\" to a new \"image\" based on the weights that are in that filter? And that the next layer uses this new \"image\"?\n\nThe filters in a CNN correspond to the weights of an MLP.\nA neuron in a CNN can be viewed as performing exactly the same operation as a neuron in an MLP. The big differences between a CNN and an MLP (as explained also in the other answer) are\n\nWeight sharing: Some neurons (not all!) in the same convolutional layer share the same weights. The convolution (or cross-correlation) is the operation that implements this partial forward pass with the same weights for different neurons.\n\nNeurons in a CNN only look at a subset of the input and not all inputs (i.e. receptive field), which leads to some notion of sparse connectivity\n\nA convolutional layer, in a CNN, is composed of neurons in a 3d dimensional volume (or, more precisely, their activations are organized in a 3d volume), rather than a 1-dimensional one, as in an MLP.\n\nCNNs may use subsampling (aka pooling)\n\n\n", "type": 2, "id": "25850", "date": "2021-01-19T16:17:27.547", "score": 0, "comment_count": 0, "parent_id": "18785"}}}
{"line": 11198, "body": "I understand that in general an RNN is good for time series data and a CNN image data, and have noticed many blogs explaining the fundamental differences in the models.\nAs a beginner in machine learning and coding, I would like to know from the code perspective, what the differences between an RNN and CNN are in a more practical way.\nFor instance, I think most CNNs dealing with image data use Conv1D  or Conv2D and MaxPooling2D layers and require reshaping input data with code looks something like this Input(shape=(64, 64, 1)).\nWhat are some other things that distinguish CNNs from RNNs from a coding perspective?\n", "type": 1, "id": "16824", "date": "2019-11-27T23:08:12.747", "score": 2, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "comparison", "recurrent-neural-networks", "implementation"], "title": "From an implementation point of view, what are the main differences between an RNN and a CNN?", "answer_count": 1, "views": 73, "accepted_answer": "21981", "answers": {"21981": {"line": 14585, "body": "In Convolutional Neural Networks (CNNs) you have small kernels (or filters) that you slide over an input (e.g. image). The value resulting from the convolution of the filter with a subset of the image over which the filter is currently positioned is then put into its respective cell in the output of that layer. Essentially, training CNNs boils down to training small filters, for example for detecting edges or corners etc. in input data, which most frequently happens to be images indeed. The assumption here is that features can be detected locally in the input volume, which entails that the nature of the input data shall be coherent over the entire volume of input data.\nRecurrent Neural Networks (RNNs) do not work locally, but are applied to sequences of arbitrary input data, where one input node may receive sensor readings, while the next node receives the date on which the sensor reading was measured. Of such arbitrary data, you feed a sequence through the RNN, which always keeps its own internal state from processing the previous instance/sample in the sequence in memory while processing the next data point/sample in the sequence. Depending on the kind of recurrent cell type that is employed to construct a RNN layer, the memory of the previous internal state then affects the computation of the next internal state and/or output computed when working on the next data sample. So, information of past data points/samples is carried forward while iterating though a sequence.\nIn short, CNNs are meant to detect local features in volume data, while RNNs preserve information over their previous internal state while processing the next data sample.\nProbably one of the best online resources walking you through all the related concepts step by step is the following lecture series offered by the Stanford University.\n", "type": 2, "id": "21981", "date": "2020-06-18T00:49:40.253", "score": 0, "comment_count": 0, "parent_id": "16824"}}}
{"line": 12099, "body": "I have a question regarding features representation for graph convolutional neural network.\nFor my case, all nodes have a different number of features, and for now, I don't really understand how should I work with these constraints. I can not just reduce the number of features or add meaningless features in order to make the number of features on each node the same - because it will add to much extra noise to the network.\nAre there any ways to solve this problem? How should I construct the feature matrix?\nI'll appreciate any help and if you have any links to papers that solve this problem.\n", "type": 1, "id": "17883", "date": "2020-02-06T14:33:04.163", "score": 3, "comment_count": 3, "tags": ["machine-learning", "deep-learning", "convolutional-neural-networks", "geometric-deep-learning"], "title": "How to represent and work with the feature matrix for graph convolutional network (GCN) if the number of features for each node is different?", "answer_count": 2, "views": 262, "accepted_answer": null, "answers": {"17884": {"line": 12100, "body": "The simplest way I could come with is to pad with 0 each feature which is not present. You said that you're going to add too much noise to the network, but I don't see the problem (please correct me if I'm wrong). For example we have two nodes, the first one has only 2 features with the 3rd one missing and the second node has all features X=[[1,2,0], [3,4,5]]. Now we can project the nodes to a hidden representation (pretty common). I'm going to use a weight matrix of W=[[1], [1], [1]]. The output of XW will be [[3], [12]]. Now let's add a new feature to the second node X=[[1,2,0, 0], [3,4,5,6]] and apply the same transformation W=[[1], [1], [1], [1]] the output will be [[3], [18]] you can see that the first node is not affected by the number of missing features. \nAnother way you could achieve this if you don't want to use the projection could be using a mask. For example give the same example above we could create a mask M=[[1,1,0], [1,1,1]] where each entry represents if a specific feature is present in a specific node. Now usually a GCN layer is defined as H=f(AHW) where A is the adjacency matrix. We could change the propagation rule to H=f(AH*MW) where * is the pointwise multiplication. Like this if a node is missing a feature it can not \"access\" information from others that are having that feature.  \n", "type": 2, "id": "17884", "date": "2020-02-06T15:12:30.660", "score": 2, "comment_count": 1, "parent_id": "17883"}, "21481": {"line": 14211, "body": "My immediate suggestion would be to zero-fill the missing values, but I recalled the below comment suggesting a more sophisticated method:\n\nKarim: How to deal with different size of feature vectors?\nNabila: That's a problem I'm actually working on. I've seen that you can create separate networks for each type of node feature, and sort of project them - so train them separately, and project them to the same size.\nOr you can do concatenation so you don't have to worry about that, but at some point they all need to be the same size to do classification at the end.\n\n\nA Literature Review on Graph Neural Networks\n\n", "type": 2, "id": "21481", "date": "2020-05-26T22:25:38.333", "score": 0, "comment_count": 0, "parent_id": "17883"}}}
{"line": 10098, "body": "Usually for DNN, I have the training data of matching X (2D) to Y (2D), for example, XOR data:\nX = [[0,0],[0,1],[1,0],[1,1]];\nY = [[0],  [1],  [1],  [0]  ];\n\nHowever, RNN seems strange, I don't get it how to match X to Y, input of RNN layer is 3D and output is 2D (rightclick to open in new tab): https://colab.research.google.com/drive/17IgFuxOYgN5fNO9LKwDijEBkIeWNPas6\nimport tensorflow as tf;\n\nx = [[[1],[2],[3]], [[4],[5],[6]]];\nbsize = 2;\ntimes = 3;\n\n#3d input\ninput = tf.placeholder(tf.float32, [bsize,times,1]);\n\ncell  = tf.keras.layers.LSTMCell(20);\nrnn   = tf.keras.layers.RNN(cell);\nhid   = rnn(input);\n\nsess = tf.Session();\ninit = tf.global_variables_initializer();\nsess.run(init);\n\n#results in 2d\nprint(sess.run(hid, {input:x}));\n\nThe example data seen on https://www.tensorflow.org/tutorials/sequences/recurrent are:\n t=0  t=1    t=2  t=3     t=4\n[the, brown, fox, is,     quick]\n[the, red,   fox, jumped, high]\n\nHow to map these data from X (3D input for RNN layer) to Y (2D)? (Y is 2D because RNN layer output is 2D).\n", "type": 1, "id": "15502", "date": "2019-09-18T10:22:05.530", "score": 0, "comment_count": 4, "tags": ["deep-learning", "tensorflow", "recurrent-neural-networks", "long-short-term-memory", "deep-neural-networks"], "title": "How to map X to Y for TensorFlow RNN training data", "answer_count": 1, "views": 39, "accepted_answer": "15530", "answers": {"15530": {"line": 10125, "body": "I found out how to get 3D output from LSTMCell so that I can matmul with output weights + biases and subtract with expected values:\n\nInputs & expecteds should be: placeholder(,[times,batch_size,num_inp]) instead of batch_size first then times. However, tf.keras.layers.LSTM will ask for [batch_size,times,num_inp]\nUse tf.nn.static_rnn with a list of inputs, instead of 1 input\n\nSource code:\nimport tensorflow as tf;\n\nx = [[[1],[2],[3]],[[4],[5],[6]]];\ntimes = 2;\nbsize = 3;\n\n#3d input\ninputs = tf.placeholder(tf.float32, [times,bsize,1]);\n\ncell   = tf.nn.rnn_cell.BasicRNNCell(20);\nhids,_ = tf.nn.static_rnn(cell,tf.unstack(inputs,times),dtype=tf.float32);\n\nsess = tf.Session();\ninit = tf.global_variables_initializer();\nsess.run(init);\n\n#results in 2d\nprint(sess.run(hids, {inputs:x}));\n\n", "type": 2, "id": "15530", "date": "2019-09-19T08:49:04.100", "score": 0, "comment_count": 0, "parent_id": "15502"}}}
{"line": 12169, "body": "I have a data set with a positive bias (an image, where the values range from 0 to 1), that seems to be causing my network to calculate incorrect gradients.\nIf I just use the raw image as input, of shape (1,28,28), gradient checking fails  on the convolution layers, producing the following:\nconv0: 1.1387914962261193e-06 1.4299464517486544     77.73286194827031\nconv1: 7.635784666538102e-06  0.13917264577152977    2.5142484667483797e-06\nconv2: 5.935327486981425e-07  2.706137838441397      1.6290818256837689e-06\nfc0:   6.053556079013436e-07  1.1083181748299355e-05 2.0993768208698443e-06\nfc1:   2.5819794868905293e-08 4.499689509732119e-06  1.108312270578618e-05\n\nWhere the first value is the absolute difference between the calculated weight updates, and the gradient tested weight updates, second being bias', and third being the calculated gradient to be passed to the previous layer. If this value is less than 0.1, it's a good indication the gradients are correct, as I use an epsilon of 1e-5.\nStrangely if I use a randomly initialised input: inpt = np.random.randn(*im.shape)) using the same seed as above for network initialisation, then I get the following differences:\nconv0: 0.002403805867625952   1.1140792221336904e-05 0.0004023637406479723\nconv1: 0.0011835450640199086  6.134154765691235e-06  0.00014655404401104377\nconv2: 0.00021913017743990792 0.0032477812666108496  7.169014825140573e-05\nfc0:   0.04510183914723059    6.425385741260989e-06  1.709243452626911e-05\nfc1:   0.0014606421065230097  1.9235668613026775e-06 5.995613024721222e-06\n\nWould anyone have any idea what be causing this? I'm truly at a loss here, as I do the same procedure regardless of the numbers in the input, using the following to find the gradients to be passed back to the previous layer (for conv layers):\nt = np.pad(grad, ((0,0), (self.xpad[1],self.xpad[0]), (self.ypad[1],self.ypad[0])))\nself.gradient = np.zeros(self.incoming.output.shape)\nfor j in range(self.gradient.shape[0]):\n    for i in range(t.shape[0]):\n        self.gradient[j] += signal.convolve(t[i], self.kernels[i,j], mode='valid')\n\n(This would be signal.correlate if I was using a convolution on the forward pass, but I'm not. Important to note that a convolutional neural network actually uses correlation on the forward pass, and only convolves on the backward)\nEDIT:\nI have done some more testing and found that if I change epsilon even slightly, I get MASSIVELY different results for the tested gradient. It seems decreasing epsilon increases the magnitude of the gradients exponentially. This is very strange behaviour, and I'm really not sure what is causing it. Could it be a discontinuity in ReLU where somewhere along the way I happen upon exactly 0?\n", "type": 1, "id": "17961", "date": "2020-02-11T05:53:42.673", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "backpropagation"], "title": "Positive bias causes the calculation of incorrect gradients", "answer_count": 1, "views": 40, "accepted_answer": null, "answers": {"17998": {"line": 12202, "body": "Ok, so I was right in assuming it was the discontinuity at x=0 for relu(x). The positive bias actually had nothing to do with the incorrect gradients. As for why in my edit I was getting such massive differences between gradient values, it was because I initialised the input to be integers, and upon adding a decimal, no matter how small (> 0 though), numpy would jump entire integers, so -2 + 0.0001 = -1.\nThis is reproduced by:\n>>> a = np.random.randint(-3,3,(1,3,3))\n>>> a\narray([[[-2,  1, -2],\n        [-2, -1, -3],\n        [-3,  0,  1]]])\n>>> a[0][0][0] += 0.0001\n>>> a\narray([[[-1,  1, -2],\n        [-2, -1, -3],\n        [-3,  0,  1]]])\n\nThe problem with my input is large amounts of it were 0, so it produced a bunch of discontinuous gradients. As such, I simply check both sides of the graph (have epsilon positive then negative), and if they disagree, I set the value to 0, as in that case I'd just ignore any gradient passing through that node.\n", "type": 2, "id": "17998", "date": "2020-02-13T05:10:23.763", "score": 0, "comment_count": 0, "parent_id": "17961"}}}
{"line": 11387, "body": "I'd like to ask for any kind of assistance regarding the following problem:\nI was given the following training data: 100 numbers, each one is a parameter, they together define a number X(also given).This is one instance,I have 20 000 instances for training.Next, I have 5000 lines given, each containing the 100 numbers as parameters.My task is to predict the number X for these 5000 instances.\nI am stuck because I only know of the sigmoid activation function so far, and I assume it's not suitable for cases like this where the output values aren't either 0 or 1.\nSo my question is this : What's a good choice for an activation function and how does one go about implementing a neural network for a problem such as this?\n", "type": 1, "id": "17047", "date": "2019-12-11T19:12:22.267", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "backpropagation", "regression"], "title": "Regression using neural network", "answer_count": 4, "views": 165, "accepted_answer": null, "answers": {"17051": {"line": 11390, "body": "Usually you're normalizing the data first, meaning that your whole dataset will be in between 0 and 1. Afterwords after you're having the model predictions, when computing the cost function or evaluating the model, you can apply the inverse of the normalization function.\n", "type": 2, "id": "17051", "date": "2019-12-11T21:43:32.357", "score": 0, "comment_count": 2, "parent_id": "17047"}, "21074": {"line": 13873, "body": "Lets mock some data up.\n\n\"100 numbers, each one is a parameter, they together define a number X(also given)\"\n\n# i.e. size of X_train -> [n x d]\n# i.e. size of X_train -> [??? x 100]  , when d = 100\n\n# \"I have 20000 instances for training\"\n# i.e. size of X_train -> [20000 x 100], when n = 20000\n\nimport torch\nimport numpy as np\n\nX_train = torch.rand((20000, 100))\nX_train = np.random.rand(20000, 100) # Or using numpy\n\nBut what is your Y?\n# Since the definition of a regression task, \n# loosely means to predict an output real number\n# given an input of d dimension\n\n# So the appropriate Y_train would \n# be of dimension [n x 1] \n# and look like this:\n\ny_train = torch.rand((20000, 1))\n\ny_train = np.random.rand(20000, 1) # Or using numpy\n\nWhat is a linear perceptron?\nTaking definition from this tutorial\n\nThus, in picture:\n\nNext we need to define training routine,\nFor now take it as biblical truth that this is an okay routine to train a neural net model (this isn't the only way but easiest or supervised learning):\n\nIn code:\nimport math\nimport numpy as np\nnp.random.seed(0)\n\ndef sigmoid(x): # Returns values that sums to one.\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(sx): \n    # See https://math.stackexchange.com/a/1225116\n    # Hint: let sx = sigmoid(x)\n    return sx * (1 - sx)\n\ndef cost(predicted, truth):\n    return np.abs(truth - predicted)\n\n\nnum_epochs = 10000 # No. of times to iterate.\nlearning_rate = 0.03 # How large a step to take per iteration.\n\n# Lets standardize and call our inputs X and outputs Y\nX = np.array(torch.rand((20000, 100)))\nY = or_output\n\nfor _ in range(num_epochs):\n    layer0 = X\n\n    # Step 2a: Multiply the weights vector with the inputs, sum the products, i.e. s\n    # Step 2b: Put the sum through the sigmoid, i.e. f()\n    # Inside the perceptron, Step 2. \n    layer1 = sigmoid(np.dot(X, W))\n\n    # Back propagation.\n    # Step 3a: Compute the errors, i.e. difference between expected output and predictions\n    # How much did we miss?\n    layer1_error = cost(layer1, Y)\n\n    # Step 3b: Multiply the error with the derivatives to get the delta\n    # multiply how much we missed by the slope of the sigmoid at the values in layer1\n    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n\n    # Step 3c: Multiply the delta vector with the inputs, sum the product (use np.dot)\n    # Step 4: Multiply the learning rate with the output of Step 3c.\n    W +=  learning_rate * np.dot(layer0.T, layer1_delta)\n\nNow that we learn the model, i.e. the W. \nWhen we see the data points that we need to use the model on, we apply the same forward propagation step, i.e. layer1 = sigmoid(np.dot(X, W)) \nSince we have:\n\nI have 5000 lines given, each containing the 100 numbers as parameters.My task is to predict the number X for these 5000 instances.\n\nAnd in code:\n# If we mock up the data,\n# it should be the same internal dimension. \nX_test = np.random.rand(5000, 100)\n\n# The desired output just needs to pass through the W and the activation:\n# the shape of `output` -> [5000 x 1] , \n# where there's 1 output value for each input.\noutput = sigmoid(np.dot(X_test, W))\n\n", "type": 2, "id": "21074", "date": "2020-05-11T15:58:26.083", "score": 1, "comment_count": 2, "parent_id": "17047"}, "21052": {"line": 13856, "body": "for regression, you can use a hidden layer with sigmoid, then a LINEAR output layer, where the weighted sum goes straight through, without modification.\nthis way your output is not restricted to 0-1\n", "type": 2, "id": "21052", "date": "2020-05-10T15:53:43.380", "score": 0, "comment_count": 1, "parent_id": "17047"}, "21078": {"line": 13877, "body": "The quick answer is that you want to use an activation function on the output layer that does not compress values to $(0,1)$. Depending on your software, this might be called \"linear\" or \"identity\". It looks like Keras just wants you to leave off the activation function: model.add(Dense(1)).\nThe typical way of thinking of a neural network as a classifier (let's say a binary classifier) is just extending a logistic regression. In fact, when you use a sigmoid activation function on the output node, you're (sort of) running logistic regression on the final hidden layer. \nA logistic regression is one type of generalized linear model. The gist of GLM is that some transformation of the the value of interest is a linear function of the feature space.\nLet $X$ be the data matrix for the feature space. Let $\\beta$ be a parameter vector. Then $\\hat{y} = \\mathbb{E}[y] = X\\beta$ is the linear model, and $g(\\mathbb{E}[y]) = X\\beta$ is the generalized linear model (vectorized, so apply $g$ to each $y_i$).\nBut we could extend this to a nonlinear transformation, and when a neural network is a binary classifier, this is precisely what we're doing. Instead of the transformation of $X$ being given by $\\beta$ and thus linear, we apply some nonlinear transformation $f$ and get $g(\\mathbb{E}[y]) = f(X)$.\nThe terminology in GLM is \"link function\", but that is essentially the activation function on the final node(s) of the neural network. Consequently, all of the GLM link functions are in play, and one of those link functions is the identity function. For a GLM, that's just linear regression. For your neural network, it will be a neural network (nonlinear) regression, which sounds like what you want.\n", "type": 2, "id": "21078", "date": "2020-05-11T18:32:46.193", "score": 1, "comment_count": 0, "parent_id": "17047"}}}
{"line": 11586, "body": "I am practicing with Resnet50 fine-tuning for a binary classification task. Here is my code snippet.\nbase_model = ResNet50(weights='imagenet', include_top=False)\nx = base_model.output\nx = keras.layers.GlobalAveragePooling2D(name='avg_pool')(x)\nx = Dropout(0.8)(x)\nmodel_prediction = keras.layers.Dense(1, activation='sigmoid', name='predictions')(x)\nmodel = keras.models.Model(inputs=base_model.input, outputs=model_prediction)\nopt = SGD(lr = 0.01, momentum = 0.9, nesterov = False)\n \nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  #\n   \ntrain_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=False)\n  \ntest_datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = train_datagen.flow_from_directory(\n        './project_01/train',\n        target_size=(input_size, input_size),  \n        batch_size=batch_size,\n        class_mode='binary')    \n\nvalidation_generator = test_datagen.flow_from_directory(\n        './project_01/val',\n        target_size=(input_size, input_size),\n        batch_size=batch_size,\n        class_mode='binary')\n\nhist = model.fit_generator(\n        train_generator,\n        steps_per_epoch= 1523 // batch_size, # 759 + 764 NON = 1523\n        epochs=epochs,\n        validation_data=validation_generator,\n        validation_steps= 269 // batch_size)  # 134 + 135NON = 269\n\nI plotted a figure of the model after training for 50 epochs:\n\nYou may have noticed that train_acc and val_acc have highly fluctuated, and train_acc merely reaches 52%, which means that network isn't learning, let alone over-fitting the data.\nAs for the losses, I haven't got any insights.\nBefore training starts, network outputs:\nFound 1523 images belonging to 2 classes.\nFound 269 images belonging to 2 classes.\n\nIs my fine-tuned model learning anything at all?\nI'd appreciate if someone can guide me to solve this issue.\n", "type": 1, "id": "17282", "date": "2019-12-28T07:17:35.067", "score": 2, "comment_count": 2, "tags": ["convolutional-neural-networks", "training", "keras", "transfer-learning", "fine-tuning"], "title": "Is my fine-tuned model learning anything at all?", "answer_count": 1, "views": 77, "accepted_answer": null, "answers": {"17283": {"line": 11587, "body": "It's difficult to say without knowing what your data looks like but from the numbers it seems too less and the images might be too similar to one another or very different. In any case, I'd have checked using other networks like Inception and decreasing learning rate even further (say 0.0001) to not mess with the Imagenet weights if your data is not very different from Imagenet classes.\n", "type": 2, "id": "17283", "date": "2019-12-28T10:45:27.327", "score": 0, "comment_count": 3, "parent_id": "17282"}}}
{"line": 12176, "body": "I am developing a CNN model to recognize 24 hand-signs of American Sign Language. I have 2500 Images/hand-sign. The data split is:\nTraining = 1250 Images/hand-sign\nValidation = 625 Images/hand-sign\nTesting = 625 Images/hand-sign\nHow should I proceed with training the model?:\n1. Should I develop a model starting from fewer hand-signs (like 5) and then increase them gradually?\n2. Should I start models from scratch or use transfer learning (VGG16 or other)\nApplying data augmentation, I did some tests with VGG16 and added a dense classifier at the end and received these accuracies:\nTrain: 0.87610877\nValidation: 0.8867307\nTest: 0.96533334\nAccuracy and Loss Graph\nTest parameters:\nNUM_CLASSES = 5\nEPOCHS = 50\nSTEPS_PER_EPOCH = 125\nVALIDATION_STEPS = 75\nTEST_STEPS = 75\nFramework = Keras, Tensorflow\nOPTIMIZER = adam\nModel:\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Conv2D(512, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Flatten(),\n    Dense(512, activation='relu'),\n\n    Dense(NUM_CLASSES, activation='softmax')\n])\n\nIf I try images with slightly different background and predict the classes (predict_classes()), I do not get accurate results. Any suggestions on how to make the model robust?\n", "type": 1, "id": "17968", "date": "2020-02-11T20:44:56.113", "score": 2, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "classification", "keras"], "title": "Hand-Signs Recognition using Deep Learning Convolutional Neural Networks", "answer_count": 1, "views": 63, "accepted_answer": null, "answers": {"17984": {"line": 12190, "body": "I feel your problem might not be with the model itself but with the dataset. If you only have $2500$ images for $24$ labels (hand-signs) that gets you roughly $104$ images per label. This is very little for the models I train (~$80K$ images in the smallest of cases). In my view you got a really decent accuracy at validation and test time for the size of your dataset.\nBut answering your questions:\n\nStarting from few labels and extending is usually helpful when your model is too deep and suffers from convergence problems. Your model is simple enough not to suffer those problems so I would go learning the $24$ labels at once.\nTransfer learning can help a lot at reducing training times. For example, if you start from a VGG classifier that detects hands there is a good chance that the weights of your convolutional layers are already almost configured for your use case.\n\nGenerally speaking, the easiest way to increase the accuracy of your model is to use one of these 2 methods:\n\nIncrease the dataset: I am not sure if it is possible in your case, maybe you can use image augmentation (rotation, zooming in/out, changes in the color space...).\nIncrease the depth of your model (provided that you have a big dataset).\n\nIf you already fulfilled those items, then you can go and make changes to the model architecture or loss function. Looking at your model and from the top of my head I would try to add Batch-Normalization for the convolutional layers.\nHope this helps a bit :)\n", "type": 2, "id": "17984", "date": "2020-02-12T13:45:13.400", "score": 0, "comment_count": 2, "parent_id": "17968"}}}
{"line": 12419, "body": "I am trying to understand the spatial transformer network mentioned in this paper https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf. I am clear about the last two stages of the spatial transformer i.e. the grid generator and sampler. However I am unable to understand the localization network which outputs the parameters of the transformation that is applied to the input image. So here are my doubts.\n\nIs the network trained on various affine/projective transforms of the input or only the standard input with a standard pose?\nIf the answer to question 1 is no, then how does the regression layer correctly regress the values of the transformation applied to the image? In other words how does the regression layer know what transformation parameters are required when it has never seen those inputs before?\n\nThanks in advance. \n", "type": 1, "id": "18257", "date": "2020-02-26T04:56:38.503", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "regression", "transformer"], "title": "How does the regression layer in the localization network of a spatial transformer work?", "answer_count": 1, "views": 90, "accepted_answer": null, "answers": {"18258": {"line": 12420, "body": "\nLocalization network is not trained separately on special transform of input. It's just a part of feed forward network which is trained as a whole, with normal backpropagation.\nIt's just part of the network which affect the loss function. As in any backpropagation loss function propagate back gradient, which backpropagate through final part of the network, backpropagate through differentiable sampler(that is non-trivial part, which use transfromation produced by localization subnetwork) and after that backpropagate into localization part\nWhole approach of spatial transformers could be in doubt now. There were some anecdotal evidences that it was not working on less trivial tasks (private communications, was not working for me either)\n\n", "type": 2, "id": "18258", "date": "2020-02-26T06:58:08.810", "score": 0, "comment_count": 2, "parent_id": "18257"}}}
{"line": 11961, "body": "I trained a simple CNN on the MNIST database of handwritten digits to 99% accuracy. I'm feeding in a bunch of handwritten digits, and non-digits from a document.\nI want the CNN to report errors, so I set a threshold of 90% certainty below which my algorithm assumes that what it's looking at is not a digit. \nMy problem is that the CNN is 100% certain of many incorrect guesses. In the example below, the CNN reports 100% certainty that it's a 0. How do I make it report failure?\n\nMy thoughts on this:\nMaybe the CNN is not really 100% certain that this is a zero. Maybe it just thinks that it can't be anything else, and it's being forced to choose (because of normalisation on the output vector). Is there any way I can get insight into what the CNN \"thought\" before I forced it to choose?\nPS: I'm using Keras on Tensorflow with Python.\nEdit\nBecause someone asked. Here is the context of my problem:\nThis came from me applying a heuristic algorithm for segmentation of sequences of connected digits. In the image above, the left part is actually a 4, and the right is the curve bit of a 2 without the base. The algorithm is supposed to step through segment cuts, and when it finds a confident match, remove that cut and continue moving along the sequence. It works really well for some cases, but of course it's totally reliant on being able to tell if what it's looking at is not a good match for a digit. Here's an example of where it kind of did okay.\n\nMy next best option is to do inference on all permutations and maximise combined score. That's more expensive.\n", "type": 1, "id": "17721", "date": "2020-01-28T16:16:23.740", "score": 35, "comment_count": 5, "tags": ["convolutional-neural-networks", "tensorflow", "keras", "bayesian-deep-learning", "uncertainty-quantification"], "title": "Why do CNN's sometimes make highly confident mistakes, and how can one combat this problem?", "answer_count": 6, "views": 9746, "accepted_answer": "17722", "answers": {"17722": {"line": 11962, "body": "The concept you are looking for is called epistemic uncertainty, also known as model uncertainty. You want the model to produce meaningful calibrated probabilities that quantify the real confidence of the model.\nThis is generally not possible with simple neural networks as they simply do not have this property, for this you need a Bayesian Neural Network (BNN). This kind of network learns a distribution of weights instead of scalar or point-wise weights, which then allow to encode model uncertainty, as then the distribution of the output is calibrated and has the properties you want.\nThis problem is also called out of distribution (OOD) detection, and again it can be done with BNNs, but unfortunately training a full BNN is untractable, so we use approximations.\nAs a reference, one of these approximations is Deep Ensembles, which train several instances of a model in the same dataset and then average the softmax probabilities, and has good out of distribution detection properties. Check the paper here, in particular section 3.5 which shows results for OOD based on entropy of the ensemble probabilities.\n", "type": 2, "id": "17722", "date": "2020-01-28T16:40:30.243", "score": 33, "comment_count": 8, "parent_id": "17721"}, "17731": {"line": 11971, "body": "Your classifier is specifically learning the ways in which 0s are different from other digits, \nnot what it really means for a digit to be a zero.\nPhilosophically, you could say the model appears to have some powerful understanding when restricted\nto a tightly controlled domain, but that facade is lifted as soon as you throw any sort of wrench in \nthe works.\nMathematically, you could say that the model is simply optimizing a classification metric for data\ndrawn from a specific distribution, and when you give it data from a different distribution, all\nbets are off.\nThe go-to answer is to collect or generate data like the data you expect the model to deal with\n(in practice, the effort required to do so can vary dramatically depending upon the application). In\nthis case, that could involve drawing a bunch of random scribbles and adding them to your training\ndata set. At this point you must ask, now how do I label them? You will want a new \"other\" or \n\"non-digit\" class so that your model can learn to categorize these scribbles separately from digits.\nAfter retraining, your model should now better deal with these cases.\nHowever, you may then ask, but what if I gave it color images of digits?  Or color images of farm\nanimals?  Maybe pigs will be classified as zeros because they are round.  This problem is a\nfundamental property of the way deep learning is orchestrated.  Your model is not capable of higher\norder logic, which means it can seem to go from being very intelligent to very dumb by just throwing\nthe slightest curve ball at it.  For now, all deep learning does is recognize patterns in data that\nallow it to minimize some loss function.\nDeep learning is a fantastic tool, but not an all-powerful omnitool.  Bear in mind its limitations\nand use it where appropriate, and it will serve you well.\n", "type": 2, "id": "17731", "date": "2020-01-29T02:29:32.893", "score": 15, "comment_count": 1, "parent_id": "17721"}, "17754": {"line": 11989, "body": "I'm an amateur with neural networks, but I will illustrate my understanding of how this problem comes to be. \nFirst, lets see how trivial neural network classifies 2D input into two classes : \n\nBut in case of complex neural network, the input space is much bigger and the sample data points are much more clustered with big chunks of empty space between them:\n\nThe neural network then doesn't know how to classify the data in the empty space, so something like this is possible :\n\n\nWhen using the traditional ways of measuring quality of neural networks, both of these will be considered good. As they do classify the classes themselves correctly.\nThen, what happens if we try to classify these data points?\n\nReally, neural network has no data it could fall back on, so it just outputs what seems to us as random nonsense.\n", "type": 2, "id": "17754", "date": "2020-01-30T09:00:36.700", "score": 2, "comment_count": 0, "parent_id": "17721"}, "17729": {"line": 11969, "body": "Broken assumptions\nGeneralization relies on making strong assumptions (no free lunch, etc). If you break your assumptions, then you're not going to have a good time. A key assumption of a standard digit-recognition classifier like MNIST is that you're classifying pictures that actually contain a single digit. If your real data contains pictures that have non-digits, then that means that your real data is not similar to training data but is conceptually very, very different.\nIf that's a problem (as in this case) then one way to treat that is to explicitly break that assumption and train a model that not only recognizes digits 0-9 but also recognizes whether there's a digit at all, and is able to provide an answer \"that's not a digit\", so a 11-class classifier instead of a 10-class one. MNIST training data is not sufficient for that, but you can use some kind of 'distractor' data to provide the not-a-digit examples. For example, you could use some dataset of letters (perhaps omitting I, l, O and B) transformed to look similar to MNIST data.\n", "type": 2, "id": "17729", "date": "2020-01-29T01:48:25.913", "score": 7, "comment_count": 0, "parent_id": "17721"}, "17743": {"line": 11981, "body": "Apollys,\nThat's a very well thought out response. Particularly, the philosophical  discussion of the essence of \"0-ness.\"\nI haven't actually performed this experiment, so caveat emptor... I wonder how well an \"other\" class would actually work. The ways in which \"other\" differs from \"digit\" has infinite variability (or at least its only limitation is the cardinality of the input layer).\nThe NN decides whether something is more of one class or more of a different class. If there isn't an essence in common among other \"non-digits\", I don't believe it will do well at identifying \"other\" as the catch-all for everything that has low confidence level of classification.\nThis approach still doesn't identify what it is to be \"not-digit\". It identifies how all the things that are \"other\" differ from the other labeled inputs -- probably poorly, depending on the variability of the \"non-digit\" labeled data. (i.e. is it numerically exhaustive, many times over, of all random scribbles?) Thoughts?\n", "type": 2, "id": "17743", "date": "2020-01-29T14:07:54.923", "score": 3, "comment_count": 5, "parent_id": "17721"}, "17760": {"line": 11994, "body": "In your particular case, you could add a eleventh category to your training data: \"not a digit\".\nThen train your model with a bunch of images of incorrectly segmented digits, in addition to the normal digit examples. This way the model will learn to tell apart real digits from incorrectly segmented ones.\nHowever even after doing that, there will be an infinite number of random looking images that will be classified as digits. They're just far away from the examples of \"not a digit\" you provided.\n", "type": 2, "id": "17760", "date": "2020-01-30T13:35:42.510", "score": 0, "comment_count": 0, "parent_id": "17721"}}}
{"line": 11528, "body": "I try to solve some easy functions with a neuronal network (aforge-lib):\nThis is how I generate the dataset:\nconst int GesamtAnzahl = 200;\nfloat[,] tempData = new float[GesamtAnzahl, 2];\nfloat minX = float.MaxValue;\nfloat maxX = float.MinValue;\n\nRandom rnd = new Random();\nvar granzen = new List<int>() \n{\n    rnd.Next(1, GesamtAnzahl-1),\n    rnd.Next(1, GesamtAnzahl-1),\n    rnd.Next(1, GesamtAnzahl-1),\n    rnd.Next(1, GesamtAnzahl-1),\n};\ngranzen.Sort();\n\nfor (int i = 0; i < GesamtAnzahl; i++)\n{\n\n    var x = i;\n    var y = -1;\n    if ((i > granzen[0] && i < granzen[1]) ||\n        (i > granzen[2] && i < granzen[3]))\n    {\n        y = 1;\n    }\n    tempData[i, 0] = x;\n    tempData[i, 1] = y;\n}\n\nSo this is quite easy: The output is 1 if the input is between the 2 lower random generated \"borders\" or between the 2 higher numbers. Otherwise the output is 1.\nThe input values are standardices to fit between -1 and 1. So 0 is -1 and 200 is 1.\nAs a network I used a BackPropagationLearning with a BipolarSigmoidFunction and several configurations like:\nLearning Rate: 0,1\nMomentum: 0\nSigmoids alpha value: 2\nHidden Layer 1: 4 neurons\nHidden Layer 2: 2 neurons\n\n\nLearning Rate: 0,1\nMomentum: 0\nSigmoids alpha value: 2\nHidden Layer 1: 4 neurons\nHidden Layer 2: 2 neurons\nHidden Layer 3: 2 neurons\n\n\nLearning Rate: 0,2\nMomentum: 0\nSigmoids alpha value: 2\nHidden Layer 1: 4 neurons\nHidden Layer 2: 2 neurons\nHidden Layer 3: 2 neurons\n\nand so on. None of them worked. As described here: https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e it should be enough to have 2 hidden layers. The first one with 4 neurons and the second one with 2. \nThe configurations which worked best were:\nLearning Rate: 0,01\nMomentum: 0\nSigmoids alpha value: 2\nHidden Layer 1: 4 neurons\nHidden Layer 2: 4 neurons\nHidden Layer 3: 4 neurons\n\nLearning Rate: 0,02\nMomentum: 0\nSigmoids alpha value: 2\nHidden Layer 1: 4 neurons\nHidden Layer 2: 2 neurons\n\nThis solves the problem about 50 % of the times.\nAs this is a quite simple problem I wonder if I am doing something wrong. I think there has to be a configuration which has better results. \nWhat is the best configuration for this problem and why?\nAdditionally I tried:\n\nHaving more data does not help. I created 5000 a dataset of 5000 points ( GesamtAnzahl = 5000). Then the networks have a even worse sucess rate.\nI tried to add an extra constant input (always 1) to the dataset but this also lowered the sucess rate\n\n", "type": 1, "id": "17217", "date": "2019-12-23T08:55:09.907", "score": 3, "comment_count": 0, "tags": ["neural-networks"], "title": "What is a working configuration of a neuronal network (number of layers, lerning rate and so on) for a specific dataset?", "answer_count": 1, "views": 77, "accepted_answer": null, "answers": {"17405": {"line": 11694, "body": "I ran a lot of randomly created networks to solve this problem, but none of the structures where able to reliably \"solve\" this problem.\nOf course, some of them where able to solve it one time, some of them even twice but there where only one which solved it 3 times:\n\nLearningRate: 0,510141694690167\nMomentum: 0,962972165068133; \nLayer/Neuron-Count: 2 (14, 9)\nSigmundAlphaValue: 2;\n\n", "type": 2, "id": "17405", "date": "2020-01-07T16:23:56.563", "score": 0, "comment_count": 0, "parent_id": "17217"}}}
{"line": 12268, "body": "Is it\n\nnumber of units in a layer \nnumber of layers\noverall complexity of the network (both 1 and 2)\n\n", "type": 1, "id": "18083", "date": "2020-02-18T06:49:56.963", "score": 1, "comment_count": 1, "tags": ["neural-networks"], "title": "In neural networks, what does the term depth generally mean?", "answer_count": 1, "views": 31, "accepted_answer": "18086", "answers": {"18086": {"line": 12271, "body": "Very good question, I have heard it refered to all the 3 ways you described. I am not sure if there is a purely objective answer so I will answer with what I understand by \"depth\" generally:\n\nIf speaking of model architecture: the number of layers of the model.\nIf speaking of CNN layers: the number of filters (not to be confused with the size of the kernel).\nIf speaking of RNN layers: the number of time steps (temporal \"depth\").\n\nOn the other hand I have never heard about depth regarding to FC layers. As I said the term \"depth\" is very subjective depending on the speakers. It should not be this way, and maybe theoretically is not, but I speak from what I heard in my experience. \n", "type": 2, "id": "18086", "date": "2020-02-18T08:57:00.923", "score": 0, "comment_count": 0, "parent_id": "18083"}}}
{"line": 13487, "body": "I am currently using TensorFlow and have simply been trying to train a neural network directly against a large continuous data set, e.g. $y = [0.014, 1.545, 10.232, 0.948, ...]$ corresponding to different points in time. The loss function in the fully connected neural network (input layer: 3 nodes, 8 inner layers: 20 nodes each, output layer: 1 node) is just the squared error between my prediction and the actual continuous data. It appears the neural network is able to learn the high magnitude data points relatively well (e.g. Figure 1 at time = 0.4422). But the smaller magnitude data points (e.g. Figure 2 at time = 1.1256) are quite poorly learned without any sharpness and I want to improve this. I've tried experimenting with different optimizers (e.g. mini-batch with Adam, full batch with L-BFGS), compared reduce_mean and reduce_sum, normalized the data in different ways (e.g. median, subtract the sample mean and divide by the standard deviation, divide the squared loss term by the actual data), and attempted to simply make the neural network deeper and train for a very long period of time (e.g. 7+ days). But after approximately 24 hours of training and the aforementioned tricks, I am not seeing any significant improvements in predicted outputs especially for the small magnitude data points.\n\nFigure 1\n\n\nFigure 2\n\n\nTherefore, do you have any recommendations on how to improve training particularly when there are different data points of varying magnitude I am trying to learn? I believe this is a related question, but any explicit examples of implementations or techniques to handle varying orders of magnitude within a single large data set would be greatly appreciated.\n", "type": 1, "id": "20564", "date": "2020-04-22T22:51:37.287", "score": 1, "comment_count": 0, "tags": ["neural-networks", "tensorflow", "training", "optimization", "batch-normalization"], "title": "How to improve neural network training against a large data set of points with varying magnitude", "answer_count": 3, "views": 115, "accepted_answer": null, "answers": {"20690": {"line": 13583, "body": "One option is normalizing your data. In particular, min-max feature scaling to bring all values into the range [0,1] is particularly useful with gradient descent.\n", "type": 2, "id": "20690", "date": "2020-04-27T15:41:55.580", "score": 0, "comment_count": 0, "parent_id": "20564"}, "20709": {"line": 13600, "body": "It might be that the large labels dominate the loss value, so the model pays more attention to them. You could use an L1 loss rather than an L2 loss, such that the predictions get pulled towards each label equally rather than being pulled more strongly when the label is further away from the prediction.\nThere may also be a data imbalance, i.e. you train on more large labels than small labels. This would also cause the model to pay more attention to becoming good at predicting the large labels. If this is the case, you can either collect more small labels or train more often on the small labels you have.\nAnother possibility is that your labels are not evenly distributed. For example, the gaps between large data points may be larger than between small data points, so the small data points all look similar to the model. You can plot a histogram of your labels to find out and then transform your labels (e.g. using log) to make space them out more evenly.\nThe general rules for training artificial neural networks apply. For example, normalize your inputs and outputs by subtracting the mean and divide them by their standard deviation, estimated across the training set. It also seem that your network is likely too deep and and not wide enough. I'd try 4x100 rather than 8x20 and add a small amount of weight decay when you see overfitting.\n", "type": 2, "id": "20709", "date": "2020-04-28T03:35:07.517", "score": 0, "comment_count": 0, "parent_id": "20564"}, "20669": {"line": 13564, "body": "I think there is no special method for training the neural network in large datasets. But I can add some suggestion for you:\n1) Use the convolutional neural network for this dataset.\n2) You can use huber loss instead of squared loss and see what happens.\n3) See if you have enough small magnitude training data.\nAlso, please define your problem in more detail (like what those images represent and what you want to predict, what are the features etc).\n", "type": 2, "id": "20669", "date": "2020-04-27T06:22:23.013", "score": 0, "comment_count": 0, "parent_id": "20564"}}}
{"line": 13395, "body": "I am a newbie in deep learning and wanted to know if the problem I have at hand is a suitable fit for deep learning algorithms. I have thousands of fragments each of about 1000 bytes size (i.e. numbers in the range of 0 to 255). There are two classes in the fragments:\n\nSome fragments have a high frequency of two particular byte values appearing next to one another: \"0 and 100\". This kind of pattern roughly appears once every 100 to 200 bytes.\nIn the other class, the byte values are more randomly distributed. \n\nWe have the ability to produce as many numbers of instances of each class as needed for training purposes.  However, I would like to differentiate with a machine learning algorithm without explicitly identifying the \"0 and 100\" pattern in the 1st class myself. Can deep learning help us solve this? If so, what kind of layers might be useful?\nAs a preliminary experiment, we tried to train a deep learning network made up of 2 hidden layers of TensorFlow's \"Dense\" layers (of size 512 and 256 nodes in each of the hidden layers). However, unfortunately, our accuracy was indicative of simply a random guess (i.e. 50% accuracy). We were wondering why the results were so bad. Do you think a Convolutional Neural Network will better solve this problem? \n", "type": 1, "id": "20417", "date": "2020-04-20T16:29:54.137", "score": 2, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "tensorflow"], "title": "Finding patterns in binary files using deep learning", "answer_count": 1, "views": 284, "accepted_answer": null, "answers": {"20423": {"line": 13400, "body": "Your network is essentially memorizing data but not extracting features. You need to apply CNN. \nThat said the CNN architecture will need to be kind of unusual, each bit will need to be turned into representing a positional element. \n", "type": 2, "id": "20423", "date": "2020-04-20T20:18:17.217", "score": 0, "comment_count": 2, "parent_id": "20417"}}}
{"line": 12388, "body": "To give an example. Let's just consider the MNIST dataset of handwritten digits. Here are some things which might have an impact on the optimum model capacity:\n\nThere are 10 output classes\nThe inputs are 28x28 grayscale pixels (I think this indirectly affects the model capacity. eg: if the inputs were 5x5 pixels, there wouldn't be much room for varying the way an 8 looks)\n\nSo, is there any way of knowing what the model capacity ought to be? Even if it's not exact? Even if it's a qualitative understanding of the type \"if X goes up, then Y goes down\"?\nJust to accentuate what I mean when I say \"not exact\": I can already tell that a 100 variable model won't solve MNIST, so at least I have a lower bound. I'm also pretty sure that a 1,000,000,000 variable model is way more than needed. Of course, knowing a smaller range than that would be much more useful!\n", "type": 1, "id": "18220", "date": "2020-02-24T20:00:46.967", "score": 10, "comment_count": 0, "tags": ["neural-networks", "computational-learning-theory", "regularization", "vc-dimension", "capacity"], "title": "Are there any rules of thumb for having some idea of what capacity a neural network needs to have for a given problem?", "answer_count": 3, "views": 401, "accepted_answer": "18430", "answers": {"18295": {"line": 12451, "body": "Personally, when I begin designing a machine learning model, I consider the following points:\n\nMy data: if I have simple images, like MNIST ones, or in general images with very low resolution, a very deep network is not required.\n\nIf my problem statement needs to learn a lot of features from each image, such as for the human face, I may need to learn eyes, nose, lips, expressions through their combinations, then I need a deep network with convolutional layers.\n\nIf I have time-series data, LSTM or GRU makes sense, but, I also consider recurrent setup when my data has high resolution, low count data points.\n\n\nThe upper limit however may get decided by resources available on the computing device you are using for training.\nHope this helps.\n", "type": 2, "id": "18295", "date": "2020-02-28T03:45:52.887", "score": 0, "comment_count": 0, "parent_id": "18220"}, "18430": {"line": 12566, "body": "Theoretical results\nRather than providing a rule of thumb (which can be misleading, so I am not a big fan of them), I will provide some theoretical results (the first one is also reported in paper How many hidden layers and nodes?), from which you may be able to derive your rules of thumb, depending on your problem, etc.\nResult 1\nThe paper Learning capability and storage capacity of two-hidden-layer feedforward networks proves that a 2-hidden layer feedforward\nnetwork ($F$) with $$2 \\sqrt{(m + 2)N} \\ll N$$ hidden neurons can learn any $N$ distinct samples $D= \\{ (x_i, t_i) \\}_{i=1}^N$ with an arbitrarily small error, where $m$ is the required number of output neurons. Conversely, a $F$ with $Q$ hidden neurons can store at least $\\frac{Q^2}{4(m+2)}$ any distinct data $(x_i, t_i)$ with\nany desired precision.\nThey suggest that a sufficient number of neurons in the first layer should be $\\sqrt{(m + 2)N} + 2\\sqrt{\\frac{N}{m + 2}}$ and in the second layer should be $m\\sqrt{\\frac{N}{m + 2}}$. So, for example, if your dataset has size $N=10$ and you have $m=2$ output neurons, then you should have the first hidden layer with roughly 10 neurons and the second layer with roughly 4 neurons. (I haven't actually tried this!)\nHowever, these bounds are suited for fitting the training data (i.e. for overfitting), which isn't usually the goal, i.e. you want the network to generalize to unseen data.\nThis result is strictly related to the universal approximation theorems, i.e. a network with a single hidden layer can, in theory, approximate any continuous function.\nModel selection, complexity control, and regularisation\nThere are also the concepts of model selection and complexity control, and there are multiple related techniques that take into account the complexity of the model. The paper Model complexity control and statistical learning theory (2002) may be useful. It is also important to note regularisation techniques can be thought of as controlling the complexity of the model [1].\nFurther reading\nYou may also want to take a look at these related questions\n\nHow to choose the number of hidden layers and nodes in a feedforward neural network?\n\nHow to estimate the capacity of a neural network?\n\n\n(I will be updating this answer, as I find more theoretical results or other useful info)\n", "type": 2, "id": "18430", "date": "2020-03-05T06:46:54.817", "score": 2, "comment_count": 0, "parent_id": "18220"}, "18321": {"line": 12472, "body": "This may sound counter intuitive but one of the biggest rules of thumb for model capacity in deep learning:\nIT SHOULD OVERFIT. \nOnce you get a model to overfit, its easier to experiment with regularizations, module replacements, etc. But in general, it gives you a good starting ground.\n", "type": 2, "id": "18321", "date": "2020-02-29T03:47:05.843", "score": 2, "comment_count": 3, "parent_id": "18220"}}}
{"line": 11777, "body": "I'm struggling to fully understand the stochastic gradient descent algorithm. \nI know that gradient descent allows you to find the local minimum of a function. What I don't know is what exactly that function IS.\nMore specifically, the algorithm should work by initializing the network with random weights. Then, if I'm not mistaken, it forward-propagates $n$ times (where $n$ is the mini-batch size). At this point, I've no idea about what function should I search for, with over hundreds of neurons each having hundreds of parameters.\n", "type": 1, "id": "17502", "date": "2020-01-14T22:02:19.673", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "backpropagation", "objective-functions", "gradient-descent"], "title": "What's the function that SGD takes to calculate the gradient?", "answer_count": 3, "views": 913, "accepted_answer": null, "answers": {"17507": {"line": 11782, "body": "\nI know that gradient descent allows you to find the local minimum of a function. What I don't know is what exactly that function IS.\n\nIt's usually called the loss function (and, in general, objective function) and often denoted as $\\mathcal{L}$ or $L$ (or something like that, i.e. it is not really important how you denote it). The specific function used as a loss function depends on the problem (ask another question if you want to know the details). For example, in the case of regression, the loss function may be the mean squared error. In classification, the loss function may be the cross-entropy. However, the most important thing to note is that the loss function depends on the parameters of the neural network (NN), so you can differentiate it with respect to the parameters of the NN (i.e. you can take the partial derivative of the loss function with respect to each of the parameters of the NN). \nLet's take the example of the mean squared error function, which is defined as \n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) ={\\frac {1}{n}}\\sum _{i=1}^{n}(y_i-f_{\\boldsymbol{\\theta}}(x_i))^{2}.\n$$\nwhere $n$ is the number of training examples used (the batch size), $y_i$ is the true class (or target) of the input example $x_i$ and $f(x_i)$ is the prediction of the neural network $f_{\\boldsymbol{\\theta}}$ with parameters (or weights) $\\boldsymbol{\\theta} = [\\theta_i, \\dots, \\theta_M] \\in \\mathbb{R}^M$, where $M$ is the number of parameters.\nGiven the loss function $\\mathcal{L}(\\boldsymbol{\\theta})$, we can now take the derivative of $\\mathcal{L}$, with respect to $\\boldsymbol{\\theta}$, using the famous back-propagation (BP) algorithm, which essentially applies the chain rule of calculus. The BP algorithm produces the gradient of the loss function $\\mathcal{L}(\\boldsymbol{\\theta})$. The gradient can be denoted as $\\nabla \\mathcal{L}(\\boldsymbol{\\theta})$ and it contains the partial derivatives of $\\mathcal{L}(\\boldsymbol{\\theta})$ with respect to each parameter $\\theta_i$, that is, $\\nabla \\mathcal{L}(\\boldsymbol{\\theta}) = \\left[ \\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\theta_i}, \\dots, \\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\theta_M} \\right] \\in \\mathbb{R}^M$. (If you want to know the details of the back-propagation algorithm, you should ask another question, but make sure you get informed first, because it may not be easy to fully explain it in an answer.)\nAfterward, we just apply the gradient descent step\n$$\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\gamma \\nabla \\mathcal{L}(\\boldsymbol{\\theta})\n$$\nwhere $\\gamma \\in \\mathbb{R}$ is often called the learning rate and is used to weight the contribution of the gradient $\\nabla \\mathcal{L}(\\boldsymbol{\\theta})$ to the new values of the parameters, and $\\leftarrow$ represents an assignment (like in programming).\nIt is worth emphasizing that both $\\boldsymbol{\\theta}$ and $\\nabla \\mathcal{L}(\\boldsymbol{\\theta})$ are vectors and have the same dimensionality ($M$).\nHave also a look at this answer where I explain the difference between gradient descent and stochastic gradient descent. \n", "type": 2, "id": "17507", "date": "2020-01-14T23:48:43.667", "score": 3, "comment_count": 5, "parent_id": "17502"}, "17687": {"line": 11932, "body": "After I've learned a little bit more about the topic, I think I figured out the exact sequence of the algorithm. So, here's my own answer. Please, correct me if I'm wrong.\n\nGive an input, forward-propagate it, and generate an output\nFor each output neuron:\n  for each weight connected to the neuron:\n      Given the function C = f(w) (which represents the cost in function to the weight value), calculate the derivative of that function at the point where  the current weight actually is)\nCalculate the actual derivative of all the weights by combining all the partial derivatives in respect to the weight: now you have a gradient of the weights\nRepeat this process to calculate the gradient for each of the batch elements. If you have a batch size of 8, then you'll have 8 gradients.\nFind the average gradient ((gradient_1+gradient2+gradient3...)/n_gradients)\nMove the weights of that gradient\n\nAm I right? How does this apply to deeper layers?\n", "type": 2, "id": "17687", "date": "2020-01-26T20:58:17.610", "score": 0, "comment_count": 4, "parent_id": "17502"}, "17505": {"line": 11780, "body": "Welcome to AI Stack exchange!\nYou're right, as the network is initialised randomly, the resultant function is essentially impossible to get your head around. This is because most of the time the network has >4 dimensions (4 can be graphed with some effort and a lot of color), and as such is literally beyond human comprehension via graphing.\nSo what do we do? Well, conveniently, it is possible to find the gradient of segments of a function, without having to know the entirety of the function itself (it's worth noting that it actually is possible to find the resultant function and with a lot of effort find it's derivative. This proves to be much more work than it's worth though, as we don't need the general derivative that tells us what the gradient is for whatever input we give it, we only need the derivative at the specific input we just fed through the network).\nThis might be hard to understand, but if you're familiar with the chain rule, it might make a bit more sense. The chain rule essentially allows you to split a function into components, and find the gradient of those specific components. By combining all of that, you end up with some nice gradients at each weight/bias with respect to the loss function. Take the negative of the gradient, and you now have the change required to decrease the loss function.\nThis is obviously quite hard to understand without an example, so here's the best one I've ever found, that helped me very much.\nAlso, as a side note, the whole mini batch thing is used to minimise catastrophic forgetting (where the network begins to \"unlearn\" old inputs). To deal with minibatchs, what you do is take input individually, then find the gradient for all weights and bias' for that specific input and remember the changes you want to make. Do that for all inputs in the minibatch, then finally add all the changes together to get the resultant best change  for each weight and bias. Only then do you update the weights and bias'.\nLet me know if you have any further questions\n", "type": 2, "id": "17505", "date": "2020-01-14T23:34:17.723", "score": 3, "comment_count": 0, "parent_id": "17502"}}}
{"line": 13851, "body": "I am starting to get my head around convolutional neural networks, and I have been working with the CIFAR-10 dataset and some research papers that used it. In one of these papers, they mention a network architecture notation for a CNN, and I am not sure how to interpret that exactly in terms of how many layers are there and how many neurons in each.\nThis is an image of their structure notation.\n\n\nCan some give me an explanation as to what exactly this structure looks like?\n\nIn the CIFAR-10 dataset, each image is $32 \\times 32$ pixels, represented by 3072 integers indicating the red, green, blue values for each pixel.\nDoes that not mean that my input layer has to be of size 3072? Or is there some way to group the inputs into matrices and then feed them into the network?\n\n\n", "type": 1, "id": "21046", "date": "2020-05-10T13:01:32.923", "score": 2, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "architecture"], "title": "Can you explain me this CNN architecture?", "answer_count": 1, "views": 108, "accepted_answer": null, "answers": {"21115": {"line": 13909, "body": "While it would certainly help if the link to the paper could also be posted, I will give it a shot based on what I understand from this picture.\n1) For any convolutional layer, there are few important things to configure, namely, the kernel (or, filter) size, number of kernels, stride. Padding is also important but it is generally defined to be zero unless mentioned otherwise. Let us consider the picture block-by-block. \nThe first block contains 3 convolutional layers: (i) 2 conv layers with 96 filters each and the size of each filter is $ 3 \\times 3$ (and stride $=1$ by default since it is not mentioned), and (ii) another conv layer with same configurations as above but with stride $=2$.\nThe second block is pretty much the same as the previous except the number of filters is increased to 192 for each layer that is defined.\nThe only considerable change in the third block is the introduction of $ 1 \\times 1$ convolutional filters instead of $3 \\times 3$.\nAnd finally, a global average pooling layer is used (instead of a fully connected layer).\n2) As for your analysis, it is exactly the case in fully connected layers, wherein the number of units in the input layer must match the vectorized dimensions of the input data. But, in the case of CNN, we give the images directly as the input to the network. The whole idea of a CNN is to understand the spatial structure of the data by analyzing patches of the image at a time (which is what the filter size defines). This PyTorch tutorial should give an idea as to how exactly the input is given to CNN.\n", "type": 2, "id": "21115", "date": "2020-05-12T23:31:53.673", "score": 0, "comment_count": 0, "parent_id": "21046"}}}
{"line": 12089, "body": "Is it possible to estimate the capacity of a neural network model? If so, what are the techniques involved?\n", "type": 1, "id": "17870", "date": "2020-02-06T01:16:47.363", "score": 5, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "computational-learning-theory", "vc-dimension", "capacity"], "title": "How to estimate the capacity of a neural network?", "answer_count": 2, "views": 575, "accepted_answer": null, "answers": {"17875": {"line": 12094, "body": "Most methods for measuring the complexity of neural networks are fairly crude. One common measure of complexity is VC dimension, a discussion which can be found here and here. For example, neural networks have a VC dimension that is too large to give a strong upper bound on the number of training samples needed for a model (the upper bound provided by VC analysis is much higher than what we have observed neural networks to be able to generalize from).\nAnother common measure of capacity is the number of parameters. We see in the paper \"Understanding deep learning requires rethinking generalization\", published at ICLR with over 1400+ citations, that networks with more parameters than data often have the capacity to memorize the data. The paper provides compelling evidence that traditional approaches to generalization provided by statistical learning theory (VC dimension, Rademacher complexity) are unable to fully explain the apparent capacity of neural networks. In general, neural networks seem to have a large capacity, given the apparent good performance on certain tasks.\nBeyond these ideas, the universal approximation theorem tells us that the set of neural networks can approximate any continuous function arbitrarily well, which strongly suggests that any neural network has a large capacity.\n", "type": 2, "id": "17875", "date": "2020-02-06T07:51:57.057", "score": 0, "comment_count": 0, "parent_id": "17870"}, "17881": {"line": 12098, "body": "VC dimension\nA rigorous measure of the capacity of a neural network is the VC dimension, which is intuitively a number or bound that quantifies the difficulty of learning from data.\nThe sample complexity, which is the number of training instances that the model (or learner) must be exposed to in order to be reasonably certain of the accurateness of the predictions made given some data, is proportional to this number.\nThe paper VC Dimension of Neural Networks (1998) by Eduardo D. Sontag provides a good introduction to the VC dimension of neural networks (even though these concepts are quite abstract and you may need to read them several times to fully grasp them). The information in this answer is highly based on that paper.\nShattering and VC dimension\nIn section 2, Concepts and VC Dimension, he describes the basic concepts behind the VC dimension (not only for neural networks), such as the concept of shattering (i.e. what does it mean for a set of sets to shatter another set?), which is a well-known concept in computational learning theory and is used to define the VC dimension (see definition 2), so you definitely need to get familiar with this concept to understand the VC dimension and, therefore, the capacity of a neural network (calculated with the VC dimension).\nVC dimension of functions and neural networks\nHe then provides an equivalent definition of the VC dimension but for functions (equation 6). Given that neural networks represent functions, then we can also define the VC dimension of a neural network. A specific combination of weights of neural networks represents a specific function, for which the VC dimension can be defined. To be more precise, a parametrized function (and a neural network) can be denoted as\n$$\n\\beta : \\mathbb{W} \\times \\mathbb{U} \\rightarrow \\mathbb{R}\n$$\nwhere $\\mathbb{W} = \\mathbb{R}^p$ and $p$ is the number of weights (or parameters) of the neural network, $\\mathbb{U}$ is the input space and $\\mathbb{R}$ the output space. So, in this case, $\\beta$ can also represent a neural network, with a certain parameter space $\\mathbb{W}$, an input space $\\mathbb{U}$ and an output space $\\mathbb{R}$.\nThe vector $\\mathbf{w} = (w_1, \\dots, w_p) \\in \\mathbb{W}$ represents a specific combination of weights of the neural network, so it represents a specific function. The set of all functions for each choice of this weight vector can be denoted as\n$$\n\\mathcal{F}_{\\beta} = \\{ \\beta(\\mathbf{w}, \\cdot) \\mid \\mathbf{w} \\in \\mathbb{W} \\}\n$$\nThe VC dimension (VCD) of $\\beta$ can then be defined as\n$$\n\\text{VCD}(\\beta) := \\text{VCD}(\\mathcal{F}_{\\beta})\n$$\nTherefore, the VC dimension is a measure of the capacity of a neural network with a certain architecture. Moreover, the VC dimension is equivalently defined for a certain set of functions associated with a neural network.\nHow to calculate the VC dimension?\nTo calculate the actual VC dimension of a neural network, it takes a little bit of more creativity. Therefore, I will just report the VC dimension of some neural networks. For more details, you should fully read the cited paper (more than once) and other papers and books too (especially, the ones described in this answer, which provide an introduction to CLT concepts).\nVC dimension of a perceptron\nThe VC dimension of a perceptron is $m + 1$, where $m$ is the number of inputs. Given that a perceptron represents a linear and affine function, the VC dimension of the perceptron is also equal to the number of parameters. However, note that, even though the VC dimension of the perceptron is linear in the number of parameters and inputs, it doesn't mean the perceptron can learn any function. In fact, perceptrons can only represent linear functions. See section 3.1 of VC Dimension of Neural Networks for more details.\nVC dimension of a single hidden layer neural network\nLet $n$ be the number of hidden units, then the VC dimension of a single hidden layer neural network is less than or equal to $n+1$. See section 3.2 of VC Dimension of Neural Networks for more details.\nVC dimension of multi-layer neural networks with binary activations\nThe VC dimension of multi-layer neural networks (MLPs) with binary activations and $p$ weights (or parameters) is $\\mathcal{O}(p \\log p)$. See theorem 4 (and related sections) of the paper VC Dimension of Neural Networks for more details.\nVC dimension of MLPs with real-valued activations\nThe VC dimension of MLPs with real-valued activations is no longer bounded by $\\mathcal{O}(p \\log p)$ and can be exponential in the number of parameters. See section 5.3 of VC Dimension of Neural Networks.\nVC dimension of MLPs with linear activations\nThe VC dimension of MLPs with linear activations is $\\mathcal{O}(p^2)$.  See theorem 5 of the paper VC Dimension of Neural Networks.\nNotes\nThe VC dimension is often expressed as a bound (e.g. with big-O notation), which may not be strict.\nIn any case, the VC dimension is useful because it provides some guarantees. For example, if you use the VC dimension to describe an upper bound on the number of samples required to learn a certain task, then you have a precise mathematical formula that guarantees that you will not need more samples than those expressed by the bound in order to achieve a small generalization error, but, in practice, you may need fewer samples than those expressed by the bound (because these bounds may not be strict or the VC dimension may also not be strict).\nFurther reading\nThere is a more recent paper (published in 2017 in MLR) that proves new and tighter upper and lower bounds on the VC dimension of deep neural networks with the ReLU activation function: Nearly-tight VC-dimension bounds for piecewise linear neural networks. So, you probably should read this paper first.\nThe paper On Characterizing the Capacity of Neural Networks using Algebraic Topology may also be useful and interesting. See also section 6, Algebraic Techniques, of the paper I have been citing: VC Dimension of Neural Networks.\nThe capacity of a neural network is clearly related to the number of functions it can represent, so it is strictly related to the universal approximation theorems for neural networks. See Where can I find the proof of the universal approximation theorem?.\n", "type": 2, "id": "17881", "date": "2020-02-06T14:08:35.860", "score": 5, "comment_count": 1, "parent_id": "17870"}}}
{"line": 13540, "body": "I was reading the paper Learning to Prune Filters in Convolutional Neural Networks, which is about pruning the CNN filters using reinforcement learning (policy gradient). The paper says that the input for the pruning agent (the agent is a convolutional neural network) is a 2D array of shape (N_l, M_l), where N_l is the number of filters and M_l = m x h x w (m, l and h are filter dimensions), and the output is an array of actions (each element is 0 (unnecessary filter) or 1 (necessary)) and says in order to approximate gradients we have to sample the output M times (using the REINFORCE algorithm).\nSince I have one input, how can I sample the output distribution multiple times (without updating the CNN parameters)? \nIf I'm missing something, please, tell me where I'm wrong\n", "type": 1, "id": "20638", "date": "2020-04-26T01:16:07.933", "score": 3, "comment_count": 0, "tags": ["reinforcement-learning", "convolutional-neural-networks", "policy-gradients", "reinforce"], "title": "How can I sample the output distribution multiple times when pruning the filters with reinforcement learning?", "answer_count": 1, "views": 68, "accepted_answer": null, "answers": {"20677": {"line": 13572, "body": "I'm not sure what do you mean by one input. The input to the pruning agent is always the same, it's the convolutional layer $W$ of dimension $m \\times h \\times w$. The layer is taken from baseline model that is pretrained. The input doesn't change it's always the same. The output of the pruning agent is an array of probabilities to prune a specific filter. For example if you have $3$ filters in a layer, the output of the pruning agent will be array of $3$ elements . Let's say its\n\\begin{equation}\ny = [0.1, 0.6, 0.7]\n\\end{equation}\nEach of these elements represents probability of pruning filter $i$ in layer $W$. So $0.1$ would be probability to prune filter $1$, $0.6$ to prune filter $2$ and $0.7$ to prune filter $3$.\nLet's say you sample this distribution $2$ times and you get: $[0, 1, 1], [0, 0, 1]$. That means you would make 2 different models from the original baseline model. First model would have 2nd and 3rd filter pruned in layer $W$, and second model would have 3rd filter pruned. The you run those 2 new models on your train and validation set, calculate objective function $R$. Then you update parameters $\\theta$ of your pruning agent based on $R$. The original weights of layer $W$ stay untouched. Then you do another inference of the pruning model $\\pi$ with updated parameters $\\theta$ (the input is still original $W$). You will get another array of probabilities and you keep repeating previous steps that i described until parameters $\\theta$ converge. When they converge you make final pruning.\n", "type": 2, "id": "20677", "date": "2020-04-27T11:09:31.067", "score": 0, "comment_count": 6, "parent_id": "20638"}}}
{"line": 12978, "body": "I am trying to implement the back-propagation algorithm for the following neural network.\n2 input -> 3 input layer -> 1 output  \nActivation f(x) -> sigmoid\nLoss f(x) -> Yexpected-Yresult\n\nThis is the Matlab/Octave function for backpropagating an XOR ANN. However, it doesn't seem to work, and it converges to 0.5. Is it correct?  \nsigmoid = @(z) 1./(1 + exp(-z));\nsig_der = @(y) sigmoid(y).*(1-sigmoid(y));\n    function [w1, w2, b1, b2] = backprop(w1,w2,b1,b2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i)\n      delta2 = (sig_der(last_layer,sigmoid)).*cost;\n      delta1 = (sig_der(mid_layer,sigmoid)).*sum(delta2.*w2);\n      w2 = w2 + 0.001 .* mid_layer .* delta2;\n      w1 = w1 + 0.001 .* data(1:2,i) .* delta1.';\n      b1 = b1 + 0.001 .* delta1';\n      b2 = b2 + 0.001 .* delta2;\n    end\n\nHere is the code:\nclear\n\ngraphics_toolkit(\"gnuplot\")\nsigmoid = @(z) 1./(1 + exp(-z));\nsig_der = @(y) sigmoid(y).*(1-sigmoid(y));\n\n\nfunction [cost, mid_layer, last_layer] = forward(w1,w2,b1,b2,data,sigmoid,i)\n  mid_layer = sigmoid(sum(data(1:2,i).*w1)'-b1);\n  last_layer = sigmoid(sum(mid_layer.*w2)-b2);\n  cost = data(3,i)-last_layer;\nend\n\nfunction [w1, w2, b1, b2] = backprop(w1,w2,b1,b2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i)\n  delta2 = (sig_der(last_layer,sigmoid)).*cost;\n  delta1 = (sig_der(mid_layer,sigmoid)).*sum(delta2.*w2);\n  w2 = w2 + 0.00001 .* mid_layer .* delta2;\n  w1 = w1 + 0.00001 .* data(1:2,i) .* delta1';\n  b1 = b1 + 0.00001 .* delta1;\n  b2 = b2 + 0.00001 .* delta2;\nend\n\ndata(:,1)=[0; 0; 0];\ndata(:,2)=[1; 0; 1];\ndata(:,3)=[0; 1; 1];\ndata(:,4)=[1; 1; 0];\n\nw1=rand(2,3)./2.*(rand(2,3).*-2+1);\nw2=rand(3,1)./2.*(rand(3,1).*-2+1);\nb1=rand(1,3)./2;\nb2=rand(1,1)./2;\n\nfor j=1:20000\n  for i=1:4\n    [cost, mid_layer, last_layer] = forward(w1,w2,b1,b2,data,sigmoid,i);\n    [w1, w2, b1, b2] = backprop(w1,w2,b1,b2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i);\n    cost_mem(j,i)=cost;\n  end\nend\ntoc\n\n", "type": 1, "id": "19936", "date": "2020-04-02T12:31:44.487", "score": 0, "comment_count": 3, "tags": ["neural-networks", "algorithm", "backpropagation", "matlab", "sigmoid"], "title": "Is my backpropagation code correct?", "answer_count": 1, "views": 88, "accepted_answer": "19945", "answers": {"19945": {"line": 12984, "body": "Thanks for everyone's help. I have now solved the problem. I hadn't quite understood the back propagation algorithm. I invite people to take a look at this link: backpropagation @ AGH UST which has solved my problem.\nCode:  \nclear\npkg load image\ngraphics_toolkit(\"gnuplot\")\ntic\nsigmoid = @(z) 1./(1 + exp(-z));\nsig_der = @(y) sigmoid(y).*(1-sigmoid(y));\n\n\nfunction [cost, mid_layer, last_layer] = forward(w1,w2,data,sigmoid,i)\n  mid_layer(:,1)=sum(w1.*data(1:2,i));\n  mid_layer(:,2)=sigmoid(mid_layer(:,1));\n  last_layer(:,1)=sum(mid_layer(:,2).*w2);\n  last_layer(:,2)=sigmoid(last_layer(:,1));\n  cost = data(3,i)-last_layer(:,2);\nend\n\nfunction [w1, w2] = backprop(w1,w2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i)\n  delta(1) = cost;\n  delta(2:4) = cost .* w2;\n  w2 = w2 + 0.99 .* delta(1) .* mid_layer(:,2) .* sig_der(last_layer(:,1));\n  w1 = w1 + 0.99 .* delta(2:4) .* sig_der(mid_layer(:,1))' .* data(1:2,i);\n  %b2 = b2 + 0.01 .* \n  %b1 = b1 + 0.01 .*\nend\n\ntic\n\n\ndata(:,1)=[0; 0; 0];\ndata(:,2)=[1; 0; 1];\ndata(:,3)=[0; 1; 1];\ndata(:,4)=[1; 1; 0];\n\nw1=rand(2,3)./2.*(rand(2,3).*-2+1);\nw2=rand(3,1)./2.*(rand(3,1).*-2+1);\n%b1=ones(1,3);%rand(1,3)./2;\n%b2=ones(1,1);%rand(1,1)./2;\n\nfor j=1:10000\n  for i=[randperm(4)]\n    [cost, mid_layer, last_layer] = forward(w1,w2,data,sigmoid,i);\n    [w1, w2] = backprop(w1,w2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i);\n    cost_mem(j,i)=cost;\n  end\nend\n\ntoc\n\n%{\ndlmwrite(\"/tmp/weights_1\",w1);\ndlmwrite(\"/tmp/weights_2\",w2);\n%}\n\n", "type": 2, "id": "19945", "date": "2020-04-02T21:18:40.533", "score": 0, "comment_count": 0, "parent_id": "19936"}}}
{"line": 12533, "body": "I know dropout layers are used in neural networks during training to provide a form of regularisation in an attempt to mitigate over-fitting.\nWould you not get an increased fitness if you disabled the dropout layers during evaluation of a network?\n", "type": 1, "id": "18391", "date": "2020-03-03T15:43:38.180", "score": 1, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "dropout"], "title": "Does the performance of a model increase if dropout is disabled at evaluation time?", "answer_count": 3, "views": 491, "accepted_answer": null, "answers": {"18396": {"line": 12538, "body": "Dropout is usually disabled at test (or evaluation) time. For example, in Keras, dropout is disabled at evaluation time by default, although you can enable it, if you need to (see below). The purpose of dropout is to decorrelate the units (or feature detectors) so that they learn more robust representations of the data (i.e. a form of regularisation). \nHowever, there's also Monte Carlo (MC) dropout, i.e., you train the network with dropout and you also use dropout at test time in order to get stochastic outputs (i.e. you will get different outputs, for different forward passes, given the same inputs). MC dropout is an approximation of Bayesian inference in deep Gaussian processes, which means that MC dropout is roughly equivalent to a Bayesian neural network.\n\nDoes the performance of a model increase if dropout is disabled at evaluation time?\n\nYes, possibly. However, MC dropout provides an uncertainty measure, which can be useful in certain scenarios (e.g. medical scenarios), where a point estimate (i.e. a single prediction or classification) is definitely not appropriate, but you also need a measure of the uncertainty or confidence of the predictions.\n", "type": 2, "id": "18396", "date": "2020-03-03T17:43:42.483", "score": 2, "comment_count": 0, "parent_id": "18391"}, "18392": {"line": 12534, "body": "Dropout is a technique that helps to avoid overfitting during training. That is, one can use dropout only for training.\n\nunits may change in a way that they fix up the mistakes of the other\n  units. This may lead to complex co-adaptations. This, in turn, leads to\n  overfitting because these co-adaptations do not generalize to unseen\n  data.\n\nIf you want to evaluate your model, you should turn off all dropout layers. For example, PyTorch's model.eval() does this work.\nMore about dropout:\n\nImproving neural networks by preventing co-adaptation of feature detectors\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\n\n", "type": 2, "id": "18392", "date": "2020-03-03T16:16:01.083", "score": 2, "comment_count": 1, "parent_id": "18391"}, "18394": {"line": 12536, "body": "Keras turns off dropout when computing validation accuracy during training. Also when making predictions\n", "type": 2, "id": "18394", "date": "2020-03-03T17:23:06.297", "score": 0, "comment_count": 3, "parent_id": "18391"}}}
{"line": 12839, "body": "I have this simple neural network in Python which I'm trying to use to aproximation tanh function. As inputs I have x - inputs to the function, and as outputs I want tanh(x) = y. I'm using sigmoid function also as an activation function of this neural network. \nimport numpy\n# scipy.special for the sigmoid function expit()\nimport scipy.special\n# library for plotting arrays\nimport matplotlib.pyplot\n# ensure the plots are inside this notebook, not an external window\n%matplotlib inline\n\n# neural network class definition\nclass neuralNetwork:\n\n\n    # initialise the neural network\n    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n        # set number of nodes in each input, hidden, output layer\n        self.inodes = inputnodes\n        self.hnodes = hiddennodes\n        self.onodes = outputnodes\n\n        # link weight matrices, wih and who\n        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n        # w11 w21\n        # w12 w22 etc \n        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n\n        # learning rate\n        self.lr = learningrate\n\n        # activation function is the sigmoid function\n        self.activation_function = lambda x: scipy.special.expit(x)  \n\n        pass\n\n\n    # train the neural network\n    def train(self, inputs_list, targets_list):\n        # convert inputs list to 2d array\n        inputs = numpy.array(inputs_list, ndmin=2).T\n        targets = numpy.array(targets_list, ndmin=2).T\n\n        # calculate signals into hidden layer\n        hidden_inputs = numpy.dot(self.wih, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)\n\n        # calculate signals into final output layer\n        final_inputs = numpy.dot(self.who, hidden_outputs)\n        # calculate the signals emerging from final output layer\n        final_outputs = self.activation_function(final_inputs)\n\n        # output layer error is the (target - actual)\n        output_errors = targets - final_outputs\n        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n        hidden_errors = numpy.dot(self.who.T, output_errors) \n\n        # BACKPROPAGATION & gradient descent part, i.e updating weights first between hidden\n        # layer and output layer, \n        # update the weights for the links between the hidden and output layers\n        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n\n        # update the weights for the links between the input and hidden layers, second part of backpropagation.\n        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n        pass\n\n\n    # query the neural network\n    def query(self, inputs_list):\n        # convert inputs list to 2d array\n        inputs = numpy.array(inputs_list, ndmin=2).T\n\n        # calculate signals into hidden layer\n        hidden_inputs = numpy.dot(self.wih, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)\n\n        # calculate signals into final output layer\n        final_inputs = numpy.dot(self.who, hidden_outputs)\n        # calculate the signals emerging from final output layer\n        final_outputs = self.activation_function(final_inputs)\n\n        return final_outputs\n\nNow I try to query this network, This network has three input nodes one for each x, one node for each input. This network also has 3 output nodes, so It would classify the inputs to given outputs. Where outputs are y, y = tanh(x) function. \n# number of input, hidden and output nodes\ninput_nodes = 3\nhidden_nodes = 8\noutput_nodes = 3\nlearning_rate = 0.1\n\n# create instance of neural network\nn = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n\nrealInputs = []\nrealInputs.append(1)\nrealInputs.append(2)\nrealInputs.append(3)\n\n# for x in (-3, 3):\n#     realInputs.append(x)\n#     pass\n\nexpectedOutputs = []\nexpectedOutputs.append(numpy.tanh(1));\nexpectedOutputs.append(numpy.tanh(2));\nexpectedOutputs.append(numpy.tanh(3));\n\nfor y in expectedOutputs:\n    print(y)\n    pass\n\ntraining_data_list = []\n\n# epochs is the number of times the training data set is used for training\nepochs = 200\n\nfor e in range(epochs):\n    # go through all records in the training data set\n    for record in training_data_list:\n        # scale and shift the inputs\n        inputs = realInputs\n        targets = expectedOutputs\n        n.train(inputs, targets)\n        pass\n    pass\n\nn.query(realInputs)\n\nOutputs: desired vs ones from network with same data as training data:\n0.7615941559557649\n0.9640275800758169\n0.9950547536867305\n\n\narray([[-0.21907413],\n       [-0.6424568 ],\n       [-0.25772344]])\n\nMy results are completely wrong. I'm a beginner with neural networks so I wanted to build neural network without frameworks like tensor flow... Could someone help me? Thank you. \n", "type": 1, "id": "18762", "date": "2020-03-23T15:07:04.043", "score": 1, "comment_count": 2, "tags": ["neural-networks", "backpropagation"], "title": "Simple three layer neural network with backpropagation is not approximating tanh function", "answer_count": 1, "views": 295, "accepted_answer": null, "answers": {"18768": {"line": 12843, "body": "This is because of Vanishing Gradient Problem\nWhat is Vanishing Gradient Problem ?\nwhen we do Back-propagation i.e moving backward in the Network and calculating gradients of loss(Error) with respect to the weights , the gradients tends to get smaller and smaller as we keep on moving backward in the Network. This means that the neurons in the Earlier layers learn very slowly as compared to the neurons in the later layers in the Hierarchy. The Earlier layers in the network are slowest to train.\nReason\nSigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small. \nSolution:\nUse Activation function as ReLu\n\nReference:\nVanishing Gradient Solution\n", "type": 2, "id": "18768", "date": "2020-03-24T01:26:40.380", "score": -1, "comment_count": 2, "parent_id": "18762"}}}
{"line": 12332, "body": "Does a fully convolutional network share the same translation invariance properties we get from networks that use max-pooling?\nIf not, why do they perform as well as networks which use max-pooling? \n", "type": 1, "id": "18157", "date": "2020-02-21T00:18:55.033", "score": 3, "comment_count": 2, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "computer-vision", "fully-convolutional-networks"], "title": "Does a fully convolutional network share the same translation invariance properties we get from networks that use max-pooling?", "answer_count": 3, "views": 207, "accepted_answer": null, "answers": {"18159": {"line": 12334, "body": "All convolutional networks (with or without max-pooling) are translation-invariant (AKA spatially invariant) because their filters slide over every position in the image. This means that if a pattern that \"matches\" a filter is present anywhere in the image, then at least one neuron should activate.\nMax-pooling, on the other hand, has nothing to do with spatial invariance. It's simply a regularization technique to help reduce the number of parameters later in the network by downsizing activation layers within the network. This can help combat overfitting, although it's not strictly necessary. Alternatively, neural networks can achieve the same effect by using a convolutional layer with a stride of 2 instead of 1.\n", "type": 2, "id": "18159", "date": "2020-02-21T00:40:12.087", "score": 0, "comment_count": 6, "parent_id": "18157"}, "21862": {"line": 14495, "body": "FCNs can and typically have downsampling operations. For example, u-net has downsampling (more precisely, max-pooling) operations. The difference between an FCN and a regular CNN is that the former does not have fully connected layers. See this answer for more info. \nTherefore, FCNs inherit the same properties of CNNs. There's nothing that a CNN (with fully connected layers) can do that an FCN cannot do. In fact, you can even simulate a fully connected layer with a convolution (with a kernel that has the same shape as the input volume).\n", "type": 2, "id": "21862", "date": "2020-06-13T20:47:21.600", "score": 0, "comment_count": 0, "parent_id": "18157"}, "28596": {"line": 19787, "body": "Neural networks are not invariant to translations, but equivariant,\nInvariance vs Equivariance\nSuppose we have input $x$ and the output $y=f(x)$ of some map between spaces $X$ and $Y$. We apply transformation $T$ in the input domain. For general map,output will change in some complicated and unpredictable way. However, for certain class of maps, change of the output becomes very tractable.\nInvariance means that output doesn't change after application of the map $T$. Namely:\n$$\nf(T(x)) = f(x)\n$$\nFor CNN example of the map, invariant to translations, is the GlobalPooling operation.\nEquivariance means that symmetry transformation $T$ on the input domain leads to the symmetry transformation $T^{'}$ on the output. Here $T^{'}$ can be the same map $T$, identity map - which reduces to invariance, or some other kind of transformation.\nThis picture is illustration of translational equivariance.\n\nEquivariance of operations in CNN\n\nConvolutions with stride=1:\n$$ f(T(x)) = T f(x)\n$$\nOutput feature map is shifted in same direction and number of steps.\nDownsampling operations. Convolutions with stride=1, Pooling (non-global):\n$$ f(T_{1/s}(x)) = T_{1/s} f(x)\n$$\nThey are equivariant to the subgroup of translations, which involves translations with integer number of strides.\nGlobalPooling :\n$$ f(T(x)) = f(x)\n$$\nThese are invariant to arbitrary shifts, this property is useful in classification tasks.\n\nCombination of layers\nStacking multiple equivariant layers you obtain equivariant architecture a whole.\nFor classification layer it makes sense to put GlobalPooling in the end in order to for NN to output the same probabilities for the shifted image.\nFor segmentation or detection problem architecture should be equivariant with the same map $T$, in order to translate bounding boxes or segmentation masks by the same amount as the transform on the input.\nNon-global downsampling operations reduce equivariance to the subgroup with shifts integer multiples of stride.\n", "type": 2, "id": "28596", "date": "2021-07-09T04:38:46.813", "score": 2, "comment_count": 0, "parent_id": "18157"}}}
{"line": 12646, "body": "I have collected a set of pictures of people with a text explaining the characteristics of the person on the picture, for example, \"Big nose\" or \"Curly hair\". \nI want to train some type of model that takes in any picture and returns a description of the picture in terms of characteristics.\nHowever, I have a hard time figuring out how to do this. It is not like labeling \"dog\" or \"apple\" because then I can create a set of training data and then evaluate its performance, now I can not. If so I would probably have used a CNN and probably also VGG-16 to help me out.\nI only have two ML courses under my belt and have never really encountered a problem like this before. Can someone help me to get in the right direction?\nAs of now, I have a data set of 13000 labeled images I am very confident it is labeled well. I do not know of any pre-trained datasets that could be of help in this instance, but if you know of one it might help.\nWorth noting is that every label is or should at least be unique. If for example there exist two pictures with the same label of \"Big nose\" it is purely coincidental.\n", "type": 1, "id": "18526", "date": "2020-03-10T00:59:12.337", "score": 2, "comment_count": 9, "tags": ["neural-networks", "machine-learning", "image-recognition"], "title": "How can I train a neural network to describe the characteristics of a picture?", "answer_count": 5, "views": 125, "accepted_answer": "18554", "answers": {"18545": {"line": 12661, "body": "I would do as suggested in the comments. First select an encoding scheme. I think what is called a difference hash would work well for this application. Code for that is shown below. Now take your data set of images and run them through the encoder and save the result in a database. The database would contain the \"labeling\" text and the encoder result. Now for a new image you are trying to label, input the image into the encoder. Take the encoder result and compare it to the encoded values in the database. Search through the encoded values in the database and find the closest match. You can then use a \"threshold\" value to determine if you want to give a specific label for the image or if the distance is above the threshold declare there is no matching label. You can determine the best \"threshold\" value by running you data set images with the known labels and iterate the threshold level and select the threshold with the least errors. I would use something like a 56 or a 128 length hash.\nimport cv2\nimport os\n# f_path is the full path to the image file, hash length is an integer specifies length of the hash\ndef get_hash(f_path, hash_length):    \n    r_str=''    \n    img=cv2.imread(f_path,0)        # read image as gray scale image\n    img = cv2.resize(img, (hash_length+1, 1), interpolation = cv2.INTER_AREA)    \n    # now compare adjacent horizontal values in a row if pixel to the left>pixel toright result=1 else 0\n    for col in range (0,hash_length):\n        if(img[0][col]>img[0][col+1]):\n            value=str(1)\n        else:\n            value=str(0)\n        r_str=r_str + value\n    number=0\n    power_of_two=1\n    for char in r_str:        \n        number = number + int(char) * power_of_two\n        power_of_two=2 * power_of_two    \n    return ( r_str, number) \n# example on an image of a bird\nf_path=r'c:\\Temp\\birds\\test\\robin\\1.jpg'\nhash=get_hash ( f_path, 16) # 16 length hash on a bird image\nprint (' hash string ', hash[0], '   hash number ', hash[1])\n\n> results is\n hash string  1111111100000000    hash number  255\n\n\n\n", "type": 2, "id": "18545", "date": "2020-03-10T16:51:41.333", "score": 0, "comment_count": 0, "parent_id": "18526"}, "18554": {"line": 12668, "body": "The term you are looking for is multi-label classification, i.e. where you are making more than one classification on each image (one for each label). Most examples you'll find online are in the NLP domain but it is just as easy with CNNs since it's essentially defined by the structure of the output layer and the loss function used. It's not as complicated as it might sound if you are already familiar with CNNs. \nThe output layer of a neural network (for 3 or more classes) has as many units as there are targets. The network learns to associate each of those units with a corresponding class. A multi-class classifier normally applies a softmax activation function to the raw unit output, which yields a probability vector. To get the final classification, the max() of the probability vector is taken (the most probable class). The output would look like this: \n                 Cat    Bird   Plane   Superman  Ball   Dog   \nRaw output:      -1     2      3       6         -1     -1\nSoftmax:         0.001  0.017  0.046   0.934     0.001  0.001\nClassification:  0      0      0       1         0      0\n\nMulti-label classification typically uses a sigmoid activation function since the probabilities of a label occuring can be treated independently. The classification is then determined by the probability (>=0.5 for True). For your problem, this output could look like:\n                 Big nose  Long hair  Curly hair  Superman  Big ears  Sharp Jawline\nRaw output:      -1        -2         3           6         -1        10\nSigmoid:         0.269     0.119      0.953       0.998     0.269     1.000\nClassification:  0         0          1           1         0         1\n\nThe binary crossentropy loss function is normally used for a multi-label classifier since a n-label problem is essentially splitting up a multi-class classification problem into n binary classification problems.\nSince all you need to do to get from a multi-class classifier to a multi-label classifier is change the output layer, its very easy to do with pre-trained networks. If you get the pre-trained model from Keras its as simple as including include_top=False when downloading the model and then adding the correct output layer.\nWith 13000 images, I would recommend using Keras' ImageDataGenerator class with the flow_from_dataframe method. This allows you to use a simple pandas dataframe to label and feed in all your images. The dataframe would look like this:\nFilename  Big nose  Long hair  Curly hair  Superman  Big ears  Sharp Jawline\n0001.JPG  0         0          1           1         0         1\n0002.JPG  1         0          1           0         1         1\n   .      .         .          .           .         .         .\n\nflow_from_dataframe's class_mode parameter can be set to raw or multi_output along with x_col to 'Filename' and y_col to ['Big nose', 'Long hair', 'Curly hair', 'Superman', 'Big ears', 'Sharp Jawline'] (in this example). Check out the documentation for more details.\nThe amount of data you need for each label depends on many factors and is essentially impossible to know without trying. 13000 sounds like a good start but it also depends on how many labels you have and how evenly distributed they are between the labels. A decent guide (one of many) on how to set up a multi-label classifier and how to implement it with Keras can be found here. It also covers imbalances on label frequency and is well worth a read. I'd highly recommend that you become as intimately familiar with your dataset as possible before you start tuning your neural network architecture.\n", "type": 2, "id": "18554", "date": "2020-03-10T22:31:00.067", "score": 2, "comment_count": 0, "parent_id": "18526"}, "18575": {"line": 12687, "body": "You can try image captioning. You can train a CNN model for image, and then, on top of that, provide the model embedding to another LSTM model to learn the encoded characteristics. You can directly use the pre-trained VGG-16 model and use the second last layer to create your image embeddings. \nShow and Tell: A Neural Image Caption Generator is a really nice paper to start with. There is an implementation of it in TensorFlow: https://www.tensorflow.org/tutorials/text/image_captioning. The paper focuses on generating caption, but you can provide your 'characteristics' to LSTM, so that it can learn it for each image. \n", "type": 2, "id": "18575", "date": "2020-03-11T16:48:00.123", "score": 2, "comment_count": 0, "parent_id": "18526"}, "18559": {"line": 12672, "body": "You can use image captioning. Look at the article Captioning Images with CNN and RNN, using PyTorch. The idea is very profound. The model encodes the image to high dimensional space and then passes it through LSTM cells and LSTM cells produce linguistic output. \nSee also Image captioning with visual attention.\n", "type": 2, "id": "18559", "date": "2020-03-11T02:08:20.820", "score": 1, "comment_count": 0, "parent_id": "18526"}, "18537": {"line": 12655, "body": "From what you wrote, the problem sounds a bit like face recognition, where a camera takes a picture of your face and compares it with a bunch of pictures in a database, for example, one for each employee if its at a company's main gate. If you look \"similar\" to one of the pictures in the database, the door opens and your ID/Name is displayed on a terminal. \nThis kind of system generates an encoding for each picture and evaluates the distance between your encoded picture and the encoding of each picture in the database. If this is at most some minimum value, it's considered a match.\nSo, what you could do is figure out some way to encode your pictures (say sum the pixel values for a very simple example, ideally you would use some sort of vector here because distances make sense with vectors) and store this encoding together with the label of the picture.\nOnce your database is complete (i.e. you have a bunch of pictures saved as a pair of [encoding, label]), you can \"scan\" each new picture, calculate its encoding (using the same algorithm that calculated your database encodings) and find the one entry in your database which minimizes the \"encoding-distance\".\nIf this sounds like a way to solve your problem, you need to come up with a proper encoding (like \"run my images through a CNN and save the output of my last fully connected layer\") and apply this to all the images you want to use as \"training data\", before \"testing\" it on some of the leftover images.\n", "type": 2, "id": "18537", "date": "2020-03-10T14:09:54.577", "score": 0, "comment_count": 0, "parent_id": "18526"}}}
{"line": 14161, "body": "I am running into an issue in which the the target (label collums) of my dataset contain a mixture of binary label (yes/no) and some numeric value label. \n\nThe value of these numeric value (resource 1 and resource 2 collumns) experience a large variation margin. Sometime these numeric value can be like 0.389 but sometimes they can be 0.389 x 10^-4 or something.\nMy goal is to predict the binary decision and the amount of resource allocated to a new user who have input feature 1 (numeric) and input feature 2 (numeric).\nMy initial though would be that the output neuron corresponding to the 0-1 decision would use logistic regression activation function. But for the neuron that corresponding to the resource I am not quite sure.\nWhat would be the appropriate way to tackle such situation in term of network structure or data pre-processing  strategy ?\nThank you for your enthusiasm !\n", "type": 1, "id": "21415", "date": "2020-05-23T00:28:46.417", "score": 2, "comment_count": 0, "tags": ["neural-networks", "data-preprocessing"], "title": "How to train a neural network with a data set that in which the target is a mix of 0-1 label and numeric real value label?", "answer_count": 2, "views": 77, "accepted_answer": "21419", "answers": {"21419": {"line": 14164, "body": "Your question are missing some details and i will assume some scenarios.\n\nIf you have a classification problem: you can try group the values in intervals that make sense (you should analyze and decide for this setup), if its possible. For example: 0.000-0.250 (0), 0.251-0.500 (1), 0.501-0.750 (2) and so on. Note that neural networks are sensible for distance between values (1 is closer to 0 than 2, so 1 is more similar to 0 than 2 and so on). If that is not your case, you should binarize the values in One Hot Encode manner.\nIf you have a regression problem, you should be ok without anything else. You can try normalize your outputs and observe the results, but generally it's not necessary for regression problems.\nBe sure if your dataset are free of outliers and noisy data as much as possible. \nIt's important choose activations functions that are adequate for the range of values in your attributes and output. This can depend on how do you treat and setup your dataset, the range of values, normalization etc.\n\nUpdate after more details in question\nYour neural network should have 3 neurons in the output layer, with linear activation. As said before, normalization usually is not necessary in regression problems, but if your values are too diferent (like the range in resource 1 and resource 2) maybe some kind of adjustment (normalization, standardization etc) can be helpful. But you need try and see the results. \n", "type": 2, "id": "21419", "date": "2020-05-23T05:10:54.433", "score": 1, "comment_count": 2, "parent_id": "21415"}, "21417": {"line": 14162, "body": "In Neural Networks the function that provides the largest interval while activation is Tanh with a result between -1 and 1 \nYou can use it to train your model , when the label has value of false it should be -1 , and when true it should be 1\nIn prediction , you'll see where the value is more close , for example if you get 0.4 more close to 1 so it'll be true \n", "type": 2, "id": "21417", "date": "2020-05-23T01:24:33.303", "score": 0, "comment_count": 0, "parent_id": "21415"}}}
{"line": 12539, "body": "\nIs this due to my dropout layers being disabled during evaluation?\nI'm classifying the CIFAR-10 dataset with a CNN using the Keras library.\nThere are 50000 samples in the training set; I'm using a 20% validation split for my training data (10000:40000). I have 10000 instances in the test set.\n", "type": 1, "id": "18397", "date": "2020-03-03T17:49:14.887", "score": 1, "comment_count": 1, "tags": ["neural-networks", "tensorflow", "keras", "dropout"], "title": "Why is my validation/test accuracy higher than my training accuracy", "answer_count": 1, "views": 342, "accepted_answer": null, "answers": {"18460": {"line": 12591, "body": "It is a bit rare that the validation and test  accuracy exceed the training accuracy. One thing that could cause this is the selection of the validation and test data. Was the data for these two sets selected randomly or did you do the selection yourself? It is generally better to have these sets selected randomly from  the overall data  set. That way the probability distribution in these sets will closely match the distribution of the training set. Normally the training accuracy is higher (especially if you run enough epochs which I see you did) because there is always some degree of over fitting which reduces validation and test accuracy. The only other thing I can think of is the effect of Dropout layers. If you had dropout layers in your model and the drop out ratio was high that could cause this accuracy disparity. When the training accuracy is calculated it is done with drop being active. This can lower the training accuracy to some degree. However when evaluating validation accuracy and test accuracy drop out is NOT active so the model is actually more accurate. This increase in accuracy might be enough to overcome the decrease due to over fitting. Especially possible in this case since the accuracy differences appear to be quite small.\n", "type": 2, "id": "18460", "date": "2020-03-06T07:06:23.983", "score": 0, "comment_count": 1, "parent_id": "18397"}}}
{"line": 13333, "body": "To whomever can help out, I appreciate it.\nI am currently attempting to detect a signal from background noise.  The signal is pretty well known but the background has a lotttt of variability.  I've since come to know this problem as Open Set Recognition.  Another complicating factor is that the signal mixes with the background noise (think equivalent to a transparent piece of glass in-front of scenery for a picture, or picking out the sound of a pin drop in an office space).\nWhen I started this project, it seemed like the current state of the art in this space was generating Spectrograms and feeding them to a CNN and this is the path I've followed.  I'm at a place where I think I've overcome most of the initial problems you might encounter but I'm still not getting good enough results for a project solution.\nHere's the overall steps I've gone through:\n\nGenerate 17000 ground truth \"signals\" and 17000 backgrounds (negatives or other classes depending on what nn scheme I'm training)\nGenerate separate test samples (not training samples but external model validation samples: \"blind test\") where I take the backgrounds and randomly overlay the signal into it at various intensities.\nMy first attempt was with a pre-built library training solution (ImageAI) with resnet50 base model.  This solution is a multiclass classifier so I had 400 each of the signal + 5 other classes that were the background. It did not work well at classifying the signal. I don't think I ever got this off the ground for two reasons a) My spectrogram pictures were not optimised (waay to large) and b) I couldn't adjust the image input shape via the library. It mostly just ended up classifying one background class.\nI then started building my own neural nets.  The first reason to make sure my spectrogram input shape was matched in the input shape of the CNN.  The second reason was to test various neural net schemes to see what worked best.\nThe first net I built was a simple feed forward net with a couple of dense layers.  This trains to .9998 val_acc.  It (like the rest of what I try) produces poor results on my blind tests, in the range of 60% true positive.\n'''\ndef build(width, height, depth, classes):\n# initialize the model along with the input shape to be\n# \"channels last\" and the channels dimension itself\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\n\n# if we are using \"channels first\", update the input shape\n# and channels dimension\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\nmodel.add(Flatten())\nmodel.add(Dense(512, input_shape=(inputShape), activation=\"relu\"))\nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(Dense(32, activation=\"relu\"))\n# sigmoid classifier\nmodel.add(Dense(classes))\nmodel.add(Activation(\"sigmoid\"))\n\n# return the constructed network architecture\nreturn model'''\n\nI then try a \"VGG Light\" model.  Again, trains to .9999 but gives me only 62% true positive results on my blind tests\n'''\ndef build(width, height, depth, classes):\n# initialize the model along with the input shape to be\n# \"channels last\" and the channels dimension itself\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\n\n# if we are using \"channels first\", update the input shape\n# and channels dimension\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\n\n# CONV => RELU => POOL\nmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n    input_shape=inputShape))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\n\n# (CONV => RELU) * 2 => POOL\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# (CONV => RELU) * 2 => POOL\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(GaussianNoise(.05))\n\n# first (and only) set of FC => RELU layers\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.5))\nmodel.add(Dense(128))       \nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())     \nmodel.add(GaussianDropout(0.5))\n\n# sigmoid classifier\nmodel.add(Dense(classes))\nmodel.add(Activation(\"sigmoid\"))\n\n# return the constructed network architecture\nreturn model'''\n\nI then try a \"full VGG\" net.  This again trains to .9999 but only a blind test true positive result of 63%.\n'''\ndef build(width, height, depth, classes):\n# initialize the model along with the input shape to be\n# \"channels last\" and the channels dimension itself\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\n\n# if we are using \"channels first\", update the input shape\n# and channels dimension\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\n\n#CONV => RELU => POOL\nmodel.add(Conv2D(64, (3, 3), padding=\"same\",\n    input_shape=inputShape))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n#model.add(Dropout(0.25))\n\n# (CONV => RELU) * 2 => POOL\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#model.add(Dropout(0.25))\n\n# (CONV => RELU) * 2 => POOL\nmodel.add(Conv2D(256, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(256, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#model.add(Dropout(0.25))\n\n# (CONV => RELU) * 2 => POOL\nmodel.add(Conv2D(512, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(512, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#model.add(Dropout(0.25))\n\n# (CONV => RELU) * 2 => POOL\nmodel.add(Conv2D(1024, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(1024, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#model.add(Dropout(0.25))\nmodel.add(GaussianNoise(.1))\n\n# first (and only) set of FC => RELU layers\nmodel.add(Flatten())\nmodel.add(Dense(8192))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1024))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization()) \nmodel.add(GaussianDropout(0.5))\n\n# sigmoid classifier\nmodel.add(Dense(classes))\nmodel.add(Activation(\"sigmoid\"))\n\n# return the constructed network architecture\nreturn model '''\n\nAll of the above are binary_crossentropy trained in keras.\nI've tried multi-class with these models as well but when testing them on the blind test they usually pick the background rather than the signal.\nI've also messed around with Autoencoders to try and get the encoder to rebuild the signal well and then compare to known results but haven't been successful yet though I'd be willing to give it another try if everyone thought that might produce better results.\nIn the beginning I ran into unbalanced classification problems (I was noob) but under all the models shown above the classes all have the same number of samples.\n\nI'm at the point where the larger VGG models trained on 34,000 samples is taking days and I don't see any better results than a basic, feed forward NN that takes 4 minutes to train.  Does anyone see the path forward here?\nMy thanks in advance.\nMecho\np.s. sorry for the formatting flubs, not sure how to correct them.\n", "type": 1, "id": "20342", "date": "2020-04-17T00:58:31.843", "score": 2, "comment_count": 4, "tags": ["convolutional-neural-networks", "keras", "architecture"], "title": "Heavily mixing signal differentiation from Open Set of backgrounds via CNN", "answer_count": 2, "views": 51, "accepted_answer": null, "answers": {"20373": {"line": 13357, "body": "Thanks for the answers. If you are processing an audio signal I think the application of a low pass filter (lpf) would help to enhance the signal to noise ratio. This would help especially if the noise component occupies a large part of the spectrum. If the audio is human speech the majority of the energy is within the 300Hz to 3Khz region. Using a low pass filter with a cutoff frequency of 3Khz would eliminate noise that is in the higher part of the spectrum. You could implement the lpf as a pre-processing function. I am not knowledgeable on the implementation but a search should get you the info you need. I did find an article here. If I recall the process is to convert the time domain signal to the frequency domain using a FFT, then set a cutoff point and reconvert back to the time domain. I also know there are ways to implement that directly in the time domain.Hope this helps.\nI am also supersized that if you achieve a high validation accuracy that your test set accuracy is so low. Your validation data should be data the network has not seen before just like your test data. Only thing I can think off is that the test data has a very different probability distribution than the training and validation data. How were the various data sets (train, test, validate) selected? Best choice is to select these randomly using something like sklearn train_test_split or Keras ImageDataGenerator flow from directory. Hope this helps.\n", "type": 2, "id": "20373", "date": "2020-04-18T16:59:44.683", "score": 0, "comment_count": 1, "parent_id": "20342"}, "23233": {"line": 15534, "body": "To anyone who reads this, I still haven't solved this completely.  At the moment I'm doing a lot better with much cleaner data, using a loss metric that matches what I'm after (F1_Score), using a very deep learning model (a custom Inception Resnet V2 model), use a custom learning rate function that depends on the training round's F1 Score, and every training round computing a F1 Score for various dB of signal/noise test sets with which I compute a model wellness score with which I determine if the model is good enough.   Pretty close.\n", "type": 2, "id": "23233", "date": "2020-08-25T01:16:23.563", "score": 0, "comment_count": 0, "parent_id": "20342"}}}
{"line": 13382, "body": "I am trying to understand the PointNet network for dealing with point clouds and struggling with understanding the difference between FC and MLP: \n\n\"FC is fully connected layer operating on each point. MLP is\n  multi-layer perceptron on each point.\"\n\nI understand how fully connected layers are used to classify and I previously thought, was that MLP was the same thing but it seems varying academic papers have a differing definition from each other and from general online courses. In PointNet what is meant by a shared MLP different to a standard feedforward fully connected network?\n\n", "type": 1, "id": "20401", "date": "2020-04-19T21:07:24.320", "score": 2, "comment_count": 0, "tags": ["convolutional-neural-networks", "models", "pytorch"], "title": "What is the difference between FC and MLP in as used in PointNet?", "answer_count": 1, "views": 112, "accepted_answer": null, "answers": {"31705": {"line": 20711, "body": "An MLP is just a fully-connected feedforward neural net. In PointNet, a shared MLP means that you are applying the exact same MLP to each point in the point cloud.\nThink of a CNN's convolutional layer. There you apply the exact same filter at all locations, and hence the filter weights are shared or tied. If they were not shared, you'd have potentially different filters (MLPs) at each pixel (point), updating independently.\nAs an example, let $f_\\theta$ be an MLP with parameters $\\theta$. Say we have a 3D point cloud $[\\vec{x}_1,\\ldots,\\vec{x}_n]\\subseteq \\mathbb{R}^3$. If we apply $f_\\theta$ as a shared MLP in the way PointNet describes, the result would be $[f_\\theta(\\vec{x}_1),\\ldots,f_\\theta(\\vec{x}_n)]$.\n", "type": 2, "id": "31705", "date": "2021-09-16T03:19:26.487", "score": 0, "comment_count": 0, "parent_id": "20401"}}}
{"line": 13683, "body": "I am trying to understand the mathematics behind the forward and backward propagation of neural nets. To make myself more comfortable, I am testing myself with an arbitrarily chosen neural network. However, I am stuck at some point.\nConsider a simple fully connected neural network with two hidden layers. For simplicity, choose linear activation function (${f(x) = x}$) at all layer. Now consider that this neural network takes two $n$-dimensional inputs $X^{1}$ and $X^{2}$. However, the first hidden layer only takes $X^1$ as the input and produces the output of $H^1$. The second hidden layer takes $H^{1} $and $X^2$ as the input and produces the output $H^{2}$. The output layer takes $H^{2}$ as the input and produces the output $\\hat{Y}$. For simplicity, assume, we do not have any bias.\nSo, we can write that, $H^1 = W^{x1}X^{1}$\n$H^2 = W^{h}H1 + W^{x2}X^{2} = W^{h}W^{x1}X^{1} + W^{x2}X^{2}$ [substituting the value of $H^1$]\n$\\hat{Y} = W^{y}H^2$\nHere, $W^{x1}$, $W^{x2}$, $W^{h}$ and $W^{y}$ are the weight matrix. Now, to make it more interesting, consider a sharing weight matrix $W^{x} = W^{x1} = W^{x2}$, which leads, $H^1 = W^{x}X^{1}$ and $H^2 = W^{h}W^{x}X^{1} + W^{x}X^{2}$\nI do not have any problem to do forward propagation by my hand; however, the problem arises when I tried to make backward propagation and update the $W^{x}$.\n$\\frac{\\partial loss}{\\partial W^{x}} = \\frac{\\partial loss}{\\partial H^{2}} . \\frac{\\partial H^{2}}{\\partial W^{x}}$ \nSubstituting, $\\frac{\\partial loss}{\\partial H^{2}} = \\frac{\\partial Y}{\\partial H^{2}}. \\frac{\\partial loss}{\\partial Y}$ and $H^2 = W^{h}W^{x}X^{1} + W^{x}X^{2}$\n$\\frac{\\partial loss}{\\partial W^{x}}= \\frac{\\partial Y}{\\partial H^{2}}. \\frac{\\partial loss}{\\partial Y} . \\frac{\\partial}{\\partial W^{x}} (W^{h}W^{x}X^{1} + W^{x}X^{2})$\nHere I understand that, $\\frac{\\partial Y}{\\partial H^{2}} = (W^y)^T$ and $\\frac{\\partial}{\\partial W^{x}} W^{x}X^{2} = (X^{2})^T$ and we can also calculate $\\frac{\\partial Y}{\\partial H^{2}}$, if we know the loss function. But how do we calculate $\\frac{\\partial}{\\partial W^{x}} W^{h}W^{x}X^{1}$?\n", "type": 1, "id": "20820", "date": "2020-05-01T08:53:03.383", "score": 2, "comment_count": 0, "tags": ["convolutional-neural-networks", "backpropagation", "weights"], "title": "Backpropagation of neural nets with shared weight", "answer_count": 3, "views": 227, "accepted_answer": null, "answers": {"20825": {"line": 13688, "body": "The product rule of partial derivative:\n$\\frac{\\partial}{\\partial x} f g = g \\frac{\\partial}{\\partial x} f + f \\frac{\\partial}{\\partial x} g$ \nAccording to this:  $\\frac{\\partial}{\\partial W^{x}} W^{h}W^{x}X^{1} = W^{h}X^{1}$, because derivative of other term with respect to $W^{x}$ is zero. (I am not considering the transpose notation as it depends on how you organize your data.) \nHowever, Your assumption of giving $H^{1}$ and $X^{2}$ as input to second hidden layer is not valid(they are called hidden layer for that reason). The output of first hidden layer ($H^{1}$)  will be fed to the input of second hidden layer.\nYour output of second hidden layer would be $H^{2} = W^{h}  * H^{1}$.  \nYou have to fed your input $X^{1} X^{2}$ to your network at once by means of looping or vectorization.\n", "type": 2, "id": "20825", "date": "2020-05-01T13:23:52.473", "score": 0, "comment_count": 2, "parent_id": "20820"}, "20821": {"line": 13684, "body": "If we write $ H^2 = W^{h}H1 + W^{x}X^{2} $ then it will be better to understand the backward propagation step.\nNow,\n$\\frac{\\partial}{\\partial W^{x}} W^{h}W^{x}X^{1}$ can be written as:\n$\\frac{\\partial H^2}{\\partial H^1}\\frac{\\partial H^1}{\\partial W^{x}} $\n$\\frac{\\partial H^2}{\\partial H^1} =  (W^h)^T$ and \n$\\frac{\\partial H^1}{\\partial W^{x}} = (X^{1})^T $\nTherefore,\n$\\frac{\\partial}{\\partial W^{x}} W^{h}W^{x}X^{1} = (W^h)^T(X^{1})^T  $\nI hope it has solved your problem.\n", "type": 2, "id": "20821", "date": "2020-05-01T10:26:52.570", "score": 0, "comment_count": 2, "parent_id": "20820"}, "26963": {"line": 18456, "body": "I think your notations are unclear, but I can give an answer based on what you probably meant. For example, $\\frac{\\partial{L}}{\\partial{W^x}}$ should be replaced by $(\\nabla_{W^x_{j:}}L)_{j=1, ...,n}$ (assuming everything stays in $\\mathbb{R}^n$). Also your expression for $\\frac{\\partial{L}}{\\partial{W^x}}$ is wrong, even accounting for the notation.\nSince $W^x_{j:}$ affects the loss through $H_{1,j}$ and $H_{2,j}$, it would be better to treat the math in this way:\n$$\\nabla_{W^x_{j:}}L=\\frac{\\partial{L}}{\\partial{H_{1,j}}}\\nabla_{W^x_{j:}}H_{1,j}+\\frac{\\partial{L}}{\\partial{H_{2,j}}}\\nabla_{W^x_{j:}}H_{2,j}$$\nNow, $H_{1, j}$ affects the loss though $H_{2,k}\\ \\forall\\ k=1,...,n.$ So,\n$$\\frac{\\partial{L}}{\\partial{H_{1,j}}}=\\sum_{k=1}^{n}\\frac{\\partial{L}} {\\partial{H_{2,k}}}W^x_{kj}$$\nAnd,\n$$\\frac{\\partial{L}}{\\partial{H_{2,j}}}=\\sum_{k=1}^{n}\\frac{\\partial{L}} {\\partial{Y_{k}}}W^y_{kj}$$\nSimilarly, $\\nabla_YL$ can be computed.\n", "type": 2, "id": "26963", "date": "2021-03-23T14:24:49.047", "score": 0, "comment_count": 0, "parent_id": "20820"}}}
{"line": 15352, "body": "I'm following the guide as outlined at this link: http://neuralnetworksanddeeplearning.com/chap2.html\nFor the purposes of this question, I've written a basic network 2 hidden layers, one with 2 neurons and one with one neuron. For a very basic task, the network will learn how to compute an OR logic gate so the training data will be:\nX = [[0, 0], [0, 1], [1, 0], [1, 1]]\nY = [0, 1, 1, 1]\n\nAnd the diagram:\n\nFor this example, the weights and biases are:\nw = [[0.3, 0.4], [0.1]]\nb = [[1, 1], [1]]\n\nThe feedforward part was pretty easy to implement so I don't think I need to post that here. The tutorial I've been following summarises calculating the errors and the gradient descent algorithm with the following equations:\nFor each training example $x$, compute the output error $\\delta^{x, L}$ where $L =$ Final layer (Layer 1 in this case). $\\delta^{x, L} = \\nabla_aC_x \\circ \\sigma'(z^{x, L})$ where $\\nabla_aC_x$ is the differential of the cost function (basic MSE) with respect to the Layer 1 activation output, and $\\sigma'(z^{x, L})$ is the derivative of the sigmoid function of the Layer 1 output i.e. $\\sigma(z^{x, L})(1-\\sigma(z^{x, L}))$.\nThat's all good so far and I can calculate that quite straightforwardly. Now for $l = L-1, L-2, ...$, the error for each previous layer can be calculated as\n$\\delta^{x, l} = ((w^{l+1})^T \\delta^{x, l+1}) \\circ \\sigma(z^{x, l})$\nWhich again, is pretty straight forward to implement.\nFinally, to update the weights (and bias), the equations are for $l = L, L-1, ...$:\n$w^l \\rightarrow w^l - \\frac{\\eta}{m}\\sum_x\\delta^{x,l}(a^{x, l-1})^T$\n$b^l \\rightarrow b^l - \\frac{\\eta}{m}\\sum_x\\delta^{x,l}$\nWhat I don't understand is how this works with vectors of different numbers of elements (I think the lack of vector notation here confuses me).\nFor example, Layer 1 has one neuron, so $\\delta^{x, 1}$ will be a scalar value since it only outputs one value. However, $a^{x, 0}$ is a vector with two elements since layer 0 has two neurons. Which means that $\\delta^{x, l}(a^{x, l-1})^T$ will be a vector even if I sum over all training samples $x$. What am I supposed to do here? Am I just supposed to sum the components of the vector as well?\nHopefully my question makes sense; I feel I'm very close to implementing this entirely and I'm just stuck here.\nThank you\n[edit] Okay, so I realised that I've been misrepresenting the weights of the neurons and have corrected for that.\nweights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n\nWhich has the output\n[array([[0.27660583, 1.00106314],\n   [0.34017727, 0.74990392]])\narray([[ 1.095244  , -0.22719165]])\n\nWhich means that layer0 has a weight matrix with shape 2x2 representing the 2 weights on neuron01 and the 2 weights on neuron02.\nMy understanding then is that $\\delta^{x,l}$ has the same shape as the weights array because each weight gets updated indepedently. That's also fine.\nBut the bias term (according to the link I sourced) has 1 term for each neuron, which means layer 0 will has two bias terms (b00 and b01) and layer 1 has one bias term (b10).\nHowever, to calculate the update for the bias terms, you sum the deltas over x i.e $\\sum_x \\delta^{x, l}$; if delta has the size of the weight matrix, then there are too many terms to update the bias terms. What have I missed here?\nMany thanks\n", "type": 1, "id": "22985", "date": "2020-08-11T15:31:42.437", "score": 2, "comment_count": 0, "tags": ["neural-networks", "python", "gradient-descent"], "title": "Implementing Gradient Descent Algorithm in Python, bit confused regarding equations", "answer_count": 1, "views": 80, "accepted_answer": "22990", "answers": {"22990": {"line": 15354, "body": "There seems to be a mismatch between the weights you provide and your network diagram. Since w[0] (the yellow connections) is meant to transform $ x \\in \\mathbb{R}^2 $ into the layer 0 activations which are $ \\mathbb{R}^2 $, w[0] should be a matrix $ \\in \\mathbb{R}^{2 \\times 2} $, not a vector in $\\mathbb{R}^2 $ as you have. Likewise, your w[1] (the red connections) should be a vector $ \\in \\mathbb{R^2} $ and not a scalar. Finally, if you are indeed scaling the output of layer 1 (the blue connection), then you'll need an additional scalar value. However, the blue connection confuses me a bit as usually the activated output is used directly in the loss function, not a scaled version of it. Unless the blue connection stands for the loss function.\nIn short, I believe if you change the shapes of your weight matrices to actually represent your network diagram, your update equations will work. I'll go through the network below to make sure I illustrate my point.\n$ x \\in \\mathbb{R}^{2} $, an input example\n$ W^0 \\in \\mathbb{R}^{2 \\times 2} $, the yellow connections\n$ W^1 \\in \\mathbb{R}^2 $, the red connections\n$ z^0 = xW^0 \\in \\mathbb{R}^{2} $, the weighted inputs to the layer 0 nodes. The dimensions of this should match the number of nodes at layer 0.\n$ a^0 = \\sigma(z^0) \\in \\mathbb{R}^{2} $, the output of the layer 0 nodes. The dimensions of this should match the number of nodes at layer 0.\n$ z^1 = a^0 W^1 \\in \\mathbb{R} $, the weighted inputs to the layer 1 nodes. The dimensions of this should match the number of nodes at layer 1.\n$ a^1 = \\sigma(z^1) \\in \\mathbb{R} $, the output of the layer 1 nodes and thus the output of the network. The dimensions of this should match the number of nodes at layer 1.\nWeight Updates\nAs you say before your edit, $\\delta^1$, as the product of two scalars $\\nabla_a C$ and $\\sigma'(z^1)$, is also a scalar. Since $a^0$ is a vector in $\\mathbb{R}^2$, then\n$\\delta^1(a^0)^T$ is also a vector in $\\mathbb{R}^2$. This matches what we expect, as it should match the dimensions of $W^1$ to allow the element-wise subtraction in the weight update equation.\nNB. It is not the case, as you say in your edit, that the shape of $\\delta^l$ should match the shape of $W^l$. It should instead match the number of nodes, and it is the shape of $\\delta^l(a^{l-1})^T$ that should match the shape of $W^l$. You had this right in your original post.\nBias Updates\nThis brings us to the bias updates.\nThere should be one bias term per node in a given layer, so the shapes of your biases are correct (i.e. $\\mathbb{R}^2$ for layer 0 and $\\mathbb{R}$ for layer 1). Now, we saw above that the shape of $\\delta^l$ also matches the number of nodes in layer $l$, so again the element-wise subtraction in your original bias update equation works.\nI also tried using this book to learn backprop, but I had a hard time connecting the variables with the different parts of the network and the corresponding code. I finally understood the algorithm in depth only after deriving all the update equations by hand for a very small network (2 inputs, one output, no hidden layers) and working my way up to larger networks, making sure to keep track of the shapes of the inputs and outputs along the way. If you're having trouble with the update equations I highly recommend this.\nA final piece of advice that helped me: drop the $x$ and the summations over input examples from your formulations and just treat everything as matrices (e.g. a scalar becomes a matrix in $\\mathbb{R}^{1 \\times 1}$, $X$ is a matrix in $\\mathbb{R}^{N \\times D}$). First, this allows you to better interpret matrix orientations and debug issues such as a missing transpose operation. Second, this is (in my limited understanding) how backprop should actually be implemented in order to take advantage of optimized linalg libraries and GPUs, so it's perhaps a bit more relevant.\n", "type": 2, "id": "22990", "date": "2020-08-12T00:04:10.137", "score": 0, "comment_count": 5, "parent_id": "22985"}}}
{"line": 14253, "body": "I know how pooling works, and what effect it has on the input dimensions - but I'm not sure why it's done in the first place. It'd be great if someone could provide some intuition behind it - while explaining the following excerpt from a blog:\n\nA problem with the output feature maps is that they are sensitive to the location of the features in the input. One approach to address this sensitivity is to down sample the feature maps. This has the effect of making the resulting down sampled feature maps more robust to changes in the position of the feature in the image, referred to by the technical phrase \"local translation invariance.\"\n\nWhat's local translation invariance here?\n", "type": 1, "id": "21532", "date": "2020-05-30T10:16:56.763", "score": 3, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks"], "title": "What is the effect of using pooling layers in CNNs?", "answer_count": 2, "views": 407, "accepted_answer": "21534", "answers": {"21535": {"line": 14256, "body": "In addition in general it somewhat aides in detection as only the strongest feature feature filter is activated so in a sense it removes additional information.\nBut it obviously has draw backs resulting in combinations of features being detected which aren't actual.objects.\n", "type": 2, "id": "21535", "date": "2020-05-30T17:32:33.337", "score": 0, "comment_count": 0, "parent_id": "21532"}, "21534": {"line": 14255, "body": "Pooling has multiple benefits\n\nRobust feature detection.\nMakes it computationally feasible to have deeper CNNs\n\nRobust Feature Detection\nThink of max-pooling (most popular) for understanding this.\n Consider a 2*2 box/unit in one layer which is mapped to only 1 box/unit in the next layer (Basically pooling). Let's say the feature map (kernel) detects a petal of a flower. Then qualifying a petal if any of the 4 units of the previous layer is fired makes the detection robust to noise. There is no strict requirement that all 4 units should be fired to detect a petal. Thus, the next layer (after pooling) captures the features with noise invariance. We can also say it is local translation invariance (in a close spatial sense) as a shifted feature will also be captured. But also remember translation invariance in general is captured by the convolution with kernels in the first place. (See how 1 kernel is convolved with the whole image)\nComputational advantage\nThe dimensions of the inputs in image classification are so huge that the number of the multiplication operation is in billions even with very few layers. Pooling the output layer reduces the input dimension for the next layer thus saving computation. But also now one can aim for really deep networks(number of layers) with the same complexity as before. \n", "type": 2, "id": "21534", "date": "2020-05-30T16:04:22.527", "score": 5, "comment_count": 0, "parent_id": "21532"}}}
{"line": 14916, "body": "For an upcoming project, I am trying to build a neural network for classifying text from scratch, without the use of libraries. This requires an embedding layer, or a way to convert words to some vector representation. I understand the gist, but I can't find any deep explanations or tutorials that don't start with importing TensorFlow. All I'm really told is that it works by context using a few surrounding words, but I don't understand exactly how.\nIs it much different from a classic network, with weights and biases? How does it figure out the loss?\nIf someone could point me towards a guide to how these things work exactly I would be very grateful.\n", "type": 1, "id": "22413", "date": "2020-07-09T15:04:15.103", "score": 3, "comment_count": 0, "tags": ["neural-networks", "implementation", "word-embedding"], "title": "How can I create an embedding layer to convert words to a vector space from scratch?", "answer_count": 1, "views": 57, "accepted_answer": null, "answers": {"22417": {"line": 14920, "body": "Word2vec embedding are trained by simple auto-encoder model that takes a word and tries to predict one word form the window of surrounding words.\n\nYou could define it like this:\nnum_of_words = 50000\n# one hot encoded word\ninput = Input(num_of_words)\n# You could use non linear activation\nw2v = Dense(300, activation=\"linear\")(input)\noutput = Dense(num_of_words, activation=\"softmax\")(w2v)\n\nBut in practice, the model is redefined and takes two words as input and predicts next words. And it outputs a probability score for all the words it knows (the model's \"vocabulary\", which can range from a few thousand to over a million words).\n\nIt is trained both ways from beginning to end of sentence and reverse. Loss is a regular categorical_crossentropy Detailed explenation can be found here [http://jalammar.github.io/illustrated-word2vec/]\n", "type": 2, "id": "22417", "date": "2020-07-09T17:16:13.710", "score": 0, "comment_count": 2, "parent_id": "22413"}}}
{"line": 16378, "body": "I have read what the loss function is but I am not sure if I have understood it. For each neuron in the output layer the loss function is equal most usually to the square of the difference value of the neuron and the result we want. Is that correct? Most sites don't help me understanding so answer would be appreciated a lot.\n", "type": 1, "id": "24402", "date": "2020-11-03T14:18:22.270", "score": 0, "comment_count": 1, "tags": ["neural-networks", "objective-functions"], "title": "Loss function definition", "answer_count": 1, "views": 74, "accepted_answer": "24403", "answers": {"24403": {"line": 16379, "body": "A loss function is what helps you \"train\" your neural network to do what you want it to do. A better way to word it to begin with would be an \"objective\" function. This function describes what objective you'd like your neural network to fit to (or to be good at).\nThe loss function that you've described is \"squared error\", which, as the name suggests, is the squared difference between the expected output and the output from the neural network. This trains the network to match the expected output value.\nOther loss (or \"objective\") functions could train your network to look for different things. For example, training on cross entropy loss helps your network learn certain probabilities. That's why it's usually used for classification, like when you want to determine which digit from 0-9 was fed into your MNIST classifier.\n", "type": 2, "id": "24403", "date": "2020-11-03T14:26:17.320", "score": 0, "comment_count": 1, "parent_id": "24402"}}}
{"line": 12785, "body": "Given a neural network with 3 inputs, 4 hidden layers, and 1 output, should the output neuron be a vector or a scalar? I thought that at the end of the summation only one number between 0 and 1 would be left over for each neuron in the last output layer but my program returns a 1x4 vector.\nclass Network:\n    def __init__(self, inputs, layers):\n        # each neuron has n weights attached\n        # each neuron has a bias\n        self.weights = []\n        self.biases = [np.random.randn(y,1) for y in layers[1:]]\n        # did this to connect neurons excluding input layer and last neuron\n        # because last neuron isn't connecting anything in front\n        for x,y in zip(layers[:-1], layers[1:]):\n            self.weights.append(np.random.randn(y,x))\n\n    # add bias vector to previous vector matrix product\n    def forward(self, inputs):\n        for b,w in zip(self.biases, self.weights):\n            inputs = sigmoid(np.dot(w, inputs) + b)\n            print(\"layer\",inputs)\n        self.output = inputs\n        print(\"output\",self.output)\n\n\ninput = [random.randrange(256),random.randrange(256),\n                random.randrange(256)]\n\nnet = Network(input, [3,4,4,1])\nnet.forward(input)\n\nOutput\nlayer [[7.80671510e-176 2.86955013e-159 1.00000000e+000 9.10681919e-004]\n [8.91574010e-176 3.27719954e-159 1.00000000e+000 1.03991921e-003]\n [3.44313504e-176 1.26560896e-159 1.00000000e+000 4.01858875e-004]\n [1.43702955e-175 5.28215553e-159 1.00000000e+000 1.67506508e-003]]\nlayer [[0.79051612 0.79051612 0.91469971 0.79084515]\n [0.80073915 0.80073915 0.70932703 0.80058277]\n [0.35187686 0.35187686 0.00982275 0.35113385]\n [0.51881022 0.51881022 0.55334083 0.5188783 ]]\nlayer [[0.26291072 0.26291072 0.16592316 0.26264314]]\noutput [[0.26291072 0.26291072 0.16592316 0.26264314]]\n\nIf my output is supposed to decide whether the neuron fires, how could I use the vector to determine this?\n", "type": 1, "id": "18697", "date": "2020-03-17T18:51:19.347", "score": 1, "comment_count": 0, "tags": ["neural-networks", "python", "math"], "title": "In a single neuron output layer should the output be a scalar?", "answer_count": 1, "views": 356, "accepted_answer": null, "answers": {"18698": {"line": 12786, "body": "Look, your code says your network has many outputs.\nLook at the two lines below. This two lines says the output depends on the dimension of np.dot(w, inputs). In your case it's 4 diminutional vector. And in the last line you are assigning them as output. You can write self.output = sigmoid(np.dot(new_weihts, inputs)) instead of self.output = inputs. Must ensure that new_weights is a vector and has same shape as previous layer's output.\ninputs = sigmoid(np.dot(w, inputs) + b)\nself.output = inputs\n\nNote: Your network don't look like a 4 layers network. It looks like a one layer network with 4 unit of neutrons.\n", "type": 2, "id": "18698", "date": "2020-03-17T20:37:01.240", "score": 0, "comment_count": 2, "parent_id": "18697"}}}
{"line": 15177, "body": "For example, if I have the following architecture:\n\n\nEach neuron in the hidden layer has a connection from each one in the\ninput layer.\n3 x 1 Input Matrix and a 4 x 3 weight matrix (for the backpropagation we have of course the transformed version 3 x 4)\n\nBut until now, I still don't understand what the point is that a neuron has 3 inputs (in the hidden layer of the example). It would work the same way, if I would only adjust one weight of the 3 connections.\nBut in the current case the information flows only distributed over several \"channels\", but what is the point?\nWith backpropagation, in some cases the weights are simply adjusted proportionally based on the error.\nOr is it just done that way, because then you can better mathematically implement everything (with matrix multiplication and so on)?\nEither my question is stupid or I have an error in my thinking and assume wrong ideas. Can someone please help me with the interpretation.\nIn tensorflow playground for example, I cut the connections (by setting the weight to 0), it just compansated it by changing the other still existing connection a bit more:\n\n", "type": 1, "id": "22742", "date": "2020-07-29T13:29:55.170", "score": 3, "comment_count": 0, "tags": ["neural-networks", "backpropagation", "architecture", "fully-connected-layer"], "title": "Why does a neuron in a multi-layer network need several input connections?", "answer_count": 3, "views": 137, "accepted_answer": null, "answers": {"22759": {"line": 15191, "body": "There's a few reasons I can think of, though I have not read an explicit description of why it is done this way. It's likely that people just started doing it this way because it's most logical, and people who have attempted to try your method of having reduced connections have seen a performance hit and so no change was made.\nThe first reason is that if you allow all nodes from one layer to connect to all others in the next, the network will optimise unnecessary connections out. Essentially, the weighting of these connections will become 0. This, however, does not mean you can trim these connections, as ignoring them in this local minima might be optimal, but later it might be really important these connections remain. As such, you can never truly know if a connection between one layer and the next is necessary, so it's just better to leave it in case it helps improve network performance.\nThe second reason is it's just simpler mathematically. Networks are implemented specifically so it's very easy to apply a series of matrix calculations to perform all computations. Trimming connections means either:\n\nA matrix must contain 0 values, wasting computation time\nA custom script must be written to calculate this networks structure, which in the real world can take a very long time as it must be implemented using something like CUDA (on a GPU level, making it very complicated)\n\nOverall, it's just a lot simpler to have all nodes connected between layers, rather than on connection per node.\n", "type": 2, "id": "22759", "date": "2020-07-30T02:23:13.043", "score": 2, "comment_count": 0, "parent_id": "22742"}, "22757": {"line": 15190, "body": "If you adopt a slightly different point-of-view, then a neural network of this static kind is just a big function with parameters, $y=F(x,P)$, and the task of training the network is a non-linear fit of this function to the data set.\nThat is, training the network is to reduce all of the residuals $y_k-F(x_k,P)$ simultaneously. This is a balancing act, just tuning one weight to adjust one residual will in general worsen some other residuals. Even if that is taken into account, methods that adjust one variable at a time are usually much slower than methods that adjust all variables simultaneously along some gradient or Newton direction.\nThe usual back-propagation algorithm sequentializes the gradient descent method for the square sum of the residuals. Better variants improve that to a Newton-like method by some estimate of the Hessean of this square sum or following along the idea of the Gauss-Newton method.\n", "type": 2, "id": "22757", "date": "2020-07-29T22:35:51.733", "score": 0, "comment_count": 1, "parent_id": "22742"}, "22744": {"line": 15178, "body": "It doesn't.\nWhether or not this is useful is another story, but it is totally fine to do that neural net you have with just one input value. Perhaps you choose one pixel of the photo and make your classification based on the intensity in that one pixel (I guess I'm assuming a black-and-white photo), or you have some method to condense an entire photograph into one value that summarizes the photo. Then each neuron in the hidden layer only has one input connection.\nLikewise, you are allowed to decide that the top neuron in the hidden layer should have only one input connection; just drop the other two.\nAgain, this might not give useful results, but they're still neural networks.\n", "type": 2, "id": "22744", "date": "2020-07-29T15:06:25.903", "score": 0, "comment_count": 7, "parent_id": "22742"}}}
{"line": 14450, "body": "When I was learning about neural networks, I saw that a complex neural network can understand the MNIST dataset and a simple convolution network can also understand the same. So I would like to know if we can achieve a CNN's functionality with just using a simple neural network without the convolution layer and if we can then how to convert a CNN into an ANN. \n", "type": 1, "id": "21803", "date": "2020-06-11T16:35:16.173", "score": 1, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "comparison", "feedforward-neural-networks"], "title": "Can we achieve what a CNN can do with just a normal neural network?", "answer_count": 2, "views": 69, "accepted_answer": null, "answers": {"21805": {"line": 14451, "body": "The convolutional aspect of a CNN comes purely from the connections between layers. Instead of a fully-connected network, which can be difficult to train and tends to overfit more, the convolutional network utilizes hierarchical patterns in the data to limit the number of connections - a local edge detection feature in an image analysis network, for example, only needs input from a small number of local pixels, not the entire image. But in principle, you could assign weights to a fully-connected network to perfectly mimic a convolutional one - you just set the weights of the unneeded connections to zero. Because a general ANN has all the connections present in a CNN plus more, it can do anything a CNN can do plus more, although the training can be more difficult.\n", "type": 2, "id": "21805", "date": "2020-06-11T16:47:03.603", "score": 1, "comment_count": 2, "parent_id": "21803"}, "21814": {"line": 14458, "body": "It can be argued that CNN will outperform a fully connected network if they have the same structure (number of neurons).\nNormal neural networks can probably learn to detect things like CNNs, but the task would be a lot more computationally expensive. In a CNN, all neurons in a feature maps share the same parameters, so if CNN learns to recognize a pattern in one location, it can detect the pattern in any other location. Furthermore, CNNs take into account the fact that pixels that are closer in proximity with each other are more heavily related than the pixels that are further apart, this information is lost in a Normal neural network.\nRead More here.\n", "type": 2, "id": "21814", "date": "2020-06-12T07:59:24.990", "score": 0, "comment_count": 0, "parent_id": "21803"}}}
{"line": 14801, "body": "The associative property of multidimensional discrete convolution says that:\n$$Y=(x \\circledast h_1) \\circledast h_2=x\\circledast(h_1\\circledast h_2)$$\nwhere $h_1$ and $h_2$ are the filters and $x$ is the input.\nI was able to do exploit this property in Keras with Conv2D: first, I convolve $h_1$ and $h_2$, then I convolve the result with $x$ (i.e. the rightmost part of the equation above).\nUp to this point, I don't have any problem, and I also understand that convolution is linear.\nThe problem is when two Conv2D layers have a non-linear activation function after the convolution. For example, consider the following two operations\n$$Y_1=\\text{ReLU}(x \\circledast h_1)$$\n$$Y_2=\\text{ReLU}(Y_1\\circledast h_2)$$\nIt is possible to apply the associative property if the first or both layers have a non-linear activation function (in the case above ReLU, but it could be any activation function)? I don't think so. Any idea or related paper or some kind of approach?\n", "type": 1, "id": "22268", "date": "2020-06-30T13:37:32.867", "score": 2, "comment_count": 0, "tags": ["convolutional-neural-networks", "activation-function", "convolution"], "title": "Is it possible to apply the associative property of the convolution operation when it is followed by a non-linearity?", "answer_count": 1, "views": 105, "accepted_answer": null, "answers": {"22283": {"line": 14813, "body": "Yes, when you have non-linearity it is not possible to combine your convolution steps.\nHowever, you can approximate the two layers network with one layer net according to the Universal Approximation Theorem. You will probably need to use something like a knowledge distillation technique to do it. Note that, the theorem doesn't say anything about the number of neurons required or if the learning techniques that we usually use will work well.\nAlso, $ReLU(x)$ is a linear mapping when $x \\geq 0$, so if your input, weight, and biases are $\\geq 0$, the net can be exactly modeled with a single layer network.\n", "type": 2, "id": "22283", "date": "2020-07-01T01:13:05.083", "score": 0, "comment_count": 1, "parent_id": "22268"}}}
{"line": 16125, "body": "I am trying to do the standard MNIST dataset image recognition test with a standard feed forward NN, but my network failed pretty badly. Now I have debugged it quite a lot and found & fixed some errors, but I had a few more ideas. For one, I am using the sigmoid activation function and MSE as an error function, but the internet suggests that I should rather use softmax for the output layer, and cross entropy loss as an error function. Now I get that softmax is a nice activation function for this task, because you can treat the output as a propability vector. But, while being a nice thing to have, that's more of a convinience thing, isn't it? Easier to visualize?\nBut when I looked at what the derivative of softmax & CEL combined is (my plan was to compute that in one step and then treat the activation function of the last layer as linear, as not to apply the softmax derivative again), I found:\n$\\frac{dE}{di}$ = $t$ - $o$\n(With $i$ being the input of the last layer, $t$ the one hot target vector and $o$ the prediction vector).\nThat is the same as the MSE derivative. So what benefits does softmax + CEL actually have when propagating, if the gradients produced by them are exactly the same?\n", "type": 1, "id": "24020", "date": "2020-10-11T00:13:09.570", "score": 5, "comment_count": 1, "tags": ["neural-networks", "gradient-descent", "cross-entropy", "mean-squared-error", "softmax"], "title": "What is the advantage of using cross entropy loss & softmax?", "answer_count": 2, "views": 636, "accepted_answer": null, "answers": {"24026": {"line": 16128, "body": "If you look at the definition of the cross-entropy (e.g. here), you will see that it is defined for probability distributions (in fact, it comes from information theory). You can also show that the maximization of the (binomial/Bernoulli) log-likelihood is equivalent to the minimization of the cross-entropy, i.e. when you minimize the cross-entropy you actually maximize the log-likelihood of the parameters given your labelled data. Hence the use of the softmax is theoretically founded.\nRegarding the supposed derivative of the cross-entropy loss function preceded by the softmax, even if that derivative is correct (I didn't think about it and I don't want to think about it now), note that then $t - o$ is different depending on whether $o$ is a probability vector or an unnormalized vector (which can take arbitrarily large numbers). If $o$ is a probability vector and $t$ a one-hot encoded vector (i.e. also a probability vector), then all numbers of $t - o$ will be in the range $[-1, 1]$. However, if $o_i$ can be arbitrarily large, e.g. $o_i = 10$, then $t_i - o_i \\in [-10, -9]$. So, the propagated error would be different if $o$ was not a probability vector.\n", "type": 2, "id": "24026", "date": "2020-10-11T09:46:36.553", "score": -1, "comment_count": 0, "parent_id": "24020"}, "24039": {"line": 16138, "body": "Short answer: larger gradients\nThat is not the derivative of the softmax function. $t - o$ is the combined derivative of the softmax function and cross entropy loss. Cross entropy loss is used to simplify the derivative of the softmax function. In the end, you do end up with a different gradients. It would be like if you ignored the sigmoid derivative when using MSE loss and the outputs are different. Using softmax and cross entropy loss has different uses and benefits compared to using sigmoid and MSE. It will help prevent gradient vanishing because the derivative of the sigmoid function only has a large value in a very small space of it. It is similar to using a different cross entropy loss where the combined derivative of the loss and sigmoid is $t - o$.\nInformation on derivatives of cross entropy with sigmoid function and with softmax function. I would also suggest some more research on cross entropy loss functions beyond my links.\n", "type": 2, "id": "24039", "date": "2020-10-11T22:44:05.630", "score": 1, "comment_count": 2, "parent_id": "24020"}}}
{"line": 15275, "body": "I am quite new to neural networks. I am trying to implement in Python a neural network having only one hidden layer with $N$ neurons and $1$ output layer.\nThe point is that I am analyzing time series and would like to use the output layer as the input of the next unit: by feeding the network with the input at time $t-1$ I obtain the output $O_{t-1}$ and, in the next step, I would like to use both the input at time $t$ and $O_{t-1}$, introducing a sort of auto-regression. I read that recurrent neural network are suitable to address this issue.\nAnyway I cannot imagine how to implement a network in Keras that involves multilayer recurrence: all the references I found are linked to using the output of a layer as input of the same layer in the next step. Instead, I would like to include the output of the last layer (the output layer) in the inputs of the first hidden layer.\n", "type": 1, "id": "22880", "date": "2020-08-05T21:09:22.017", "score": 1, "comment_count": 6, "tags": ["neural-networks", "python", "keras", "recurrent-neural-networks"], "title": "Is there a neural network that accepts both the current input and previous output?", "answer_count": 2, "views": 90, "accepted_answer": "22885", "answers": {"22885": {"line": 15278, "body": "You could just do this; concatenate your input_vector with zero's vector that has the size of your output. Then in the first pass you concatenate with the output instaid of the zero's vector. After that repeat.. At the end just compare (compute the loss) your entire output from t0 to t1 to your target and backprop.\nYou might want to look into recurrent layers, these are layers that have connections back to themselves so that the network can learn what to \"remember\".\nThese have some problems with longer sequences, so the \"newer\" versions try to deal with that. (LSTM and GRU)\nYou can also use attention mechanisms if you're dealing with sequences. (basically you learn what parts of your input sequence to look at given a certain \"query\", in your case maybe the last timestep) (generally used in natural language processing) But it's a bit more exotic and complicated.\n", "type": 2, "id": "22885", "date": "2020-08-05T21:55:13.400", "score": 0, "comment_count": 0, "parent_id": "22880"}, "22883": {"line": 15277, "body": "You want to look at recurrent neural networks.\n", "type": 2, "id": "22883", "date": "2020-08-05T21:44:33.040", "score": 1, "comment_count": 0, "parent_id": "22880"}}}
{"line": 14512, "body": "I'm currently trying to understand the difference between a vanilla LSTM and a fully connected LSTM. In a paper I'm reading, the FC-LSTM gets introduced as\n\nFC-LSTM may be seen as a multivariate version of LSTM where the input, cell output and states are all 1D vectors\n\nBut is not really expanded further upon. Google also didn't help me much in that regard as I can't seem to find anything under that keyword. \nWhat is the difference between the two? Also, I'm a bit confused by the quote - aren't inputs, outputs, etc. of a vanilla LSTM already 1D vectors?\n", "type": 1, "id": "21883", "date": "2020-06-14T17:28:28.307", "score": 3, "comment_count": 0, "tags": ["deep-learning", "comparison", "recurrent-neural-networks", "long-short-term-memory"], "title": "What is the difference between LSTM and fully connected LSTM?", "answer_count": 1, "views": 757, "accepted_answer": "21945", "answers": {"21945": {"line": 14555, "body": "Based on the citations in the ConvLSTM paper I have come to the conclusion that they mean the Peephole LSTM when they say fully connected LSTM. In the paper that they have taken the encoder-decoder-predictor model from, where they refer to a fully connected LSTM, a Peephole LSTM is used. Also they take their fully connected LSTM definition from this paper, which again uses the Peephole LSTM.  \nWith that the difference would be the added \"peephole connections\", that lets the gate layers look at the cell states and access the constant error carousel.\n", "type": 2, "id": "21945", "date": "2020-06-16T13:49:37.890", "score": 0, "comment_count": 0, "parent_id": "21883"}}}
{"line": 16325, "body": "Let's assume I have a simple feedforward neural network whose input contains binary 0/1 features and output is also binary two classes.\nIs it better, worse, or maybe totally indifferent, for every such binary feature to be in just one column or maybe it would be better to split one feature into two columns in a way that the second column will have the opposite value, like that:\nfeature_x (one column scenario) \n\n[0]\n\n[1]\n\n[0]\n\n\nfeature_x (two columns scenario)\n\n[0, 1]\n\n[1, 0]\n\n[0, 1]\n\nI know this might seem a bit weird and probably it is not necessary, but I have a feeling like there might be a difference for a network especially for its inner workings and how neurons in the next layers see such data. Has anyone ever researched that aspect?\n", "type": 1, "id": "24327", "date": "2020-10-30T20:26:22.190", "score": 1, "comment_count": 0, "tags": ["deep-learning", "deep-neural-networks", "data-preprocessing", "geometric-deep-learning", "binary-classification"], "title": "Should binary feature be in one or two columns in deep neural networks?", "answer_count": 1, "views": 41, "accepted_answer": "24335", "answers": {"24335": {"line": 16333, "body": "You're simply adding a redundant feature by having it as two: $X_2 = 1 - X_1$. It would be equally useful to duplicate the first column. At best this will not improve your model, at worst it will decrease accuracy.\n", "type": 2, "id": "24335", "date": "2020-10-31T09:31:18.320", "score": 0, "comment_count": 0, "parent_id": "24327"}}}
{"line": 14598, "body": "Typically, people say that convolutional neural networks (CNN) perform the convolution operation, hence their name. However, some people have also said that a CNN actually performs the cross-correlation operation rather than the convolution. How is that? Does a CNN perform the convolution or cross-correlation operation? What is the difference between the convolution and cross-correlation operations?\n", "type": 1, "id": "21999", "date": "2020-06-18T17:13:00.543", "score": 3, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks", "comparison", "convolution", "cross-correlation"], "title": "Do convolutional neural networks perform convolution or cross-correlation?", "answer_count": 2, "views": 529, "accepted_answer": "22000", "answers": {"22000": {"line": 14599, "body": "Short answer\nTheoretically, convolutional neural networks (CNNs) can either perform the cross-correlation or convolution: it does not really matter whether they perform the cross-correlation or convolution because the kernels are learnable, so they can adapt to the cross-correlation or convolution given the data, although, in the typical diagrams, CNNs are shown to perform the cross-correlation because (in libraries like TensorFlow) they are typically implemented with cross-correlations (and cross-correlations are conceptually simpler than convolutions). Moreover, in general, the kernels can or not be symmetric (although they typically won't be symmetric). In the case they are symmetric, the cross-correlation is equal to the convolution.\nLong answer\nTo understand the answer to this question, I will provide two examples that show the similarities and differences between the convolution and cross-correlation operations. I will focus on the convolution and cross-correlation applied to 1-dimensional discrete and finite signals (which is the simplest case to which these operations can be applied) because, essentially, CNNs process finite and discrete signals (although typically higher-dimensional ones, but this answer applies to higher-dimensional signals too). Moreover, in this answer, I will assume that you are at least familiar with how the convolution (or cross-correlation) in a CNN is performed, so that I do not have to explain these operations in detail (otherwise this answer would be even longer).\nWhat is the convolution and cross-correlation?\nBoth the convolution and the cross-correlation operations are defined as the dot product between a small matrix and different parts of another typically bigger matrix (in the case of CNNs, it is an image or a feature map). Here's the usual illustration (of the cross-correlation, but the idea of the convolution is the same!).\n\nExample 1\nTo be more concrete, let's suppose that we have the output of a function (or signal) $f$ grouped in a matrix $$f = [2, 1, 3, 5, 4] \\in \\mathbb{R}^{1 \\times 5},$$ and the output of a kernel function also grouped in another matrix $$h=[1, -1] \\in \\mathbb{R}^{1 \\times 2}.$$  For simplicity, let's assume that we do not pad the input signal and we perform the convolution and cross-correlation with a stride of 1 (I assume that you are familiar with the concepts of padding and stride).\nConvolution\nThen the convolution of $f$ with $h$, denoted as $f \\circledast h = g_1$, where $\\circledast$ is the convolution operator, is computed as follows\n\\begin{align}\nf \\circledast h = g_1\n&=\\\\\n[(-1)*2 + 1*1, \\\\\n(-1)*1 + 1*3, \\\\\n(-1)*3 + 1*5, \\\\\n(-1)*5+1*4] \n&=\\\\ \n[-2 + 1, -1 + 3, -3 + 5, -5 + 4]\n&=\\\\\n[-1, 2, 2, -1] \\in \\mathbb{R}^{1 \\times 4}\n\\end{align}\nSo, the convolution of $f$ with $h$ is computed as a series of element-wise multiplications between the horizontally flipped kernel $h$, i.e. $[-1, 1]$, and each $1 \\times 2$ window of $f$, each of which is followed by a summation (i.e. a dot product). This follows from the definition of convolution (which I will not report here).\nCross-correlation\nSimilarly, the cross-correlation of $f$ with $h$, denoted as $f \\otimes h = g_2$, where $\\otimes$ is the cross-correlation operator, is also defined as a dot product between $h$ and different parts of $f$, but without flipping the elements of the kernel before applying the element-wise multiplications, that is\n\\begin{align}\nf \\otimes h = g_2\n&=\\\\\n[1*2 + (-1)*1, \\\\ \n1*1 + (-1)*3, \\\\ \n1*3 + (-1)*5, \\\\ \n1*5 + (-1)*4] \n&=\\\\ \n[2 - 1, 1 - 3, 3 - 5, 5 - 4]\n&=\\\\\n[1, -2, -2, 1] \\in \\mathbb{R}^{1 \\times 4}\n\\end{align}\nNotes\n\nThe only difference between the convolution and cross-correlation operations is that, in the first case, the kernel is flipped (along all spatial dimensions) before being applied.\n\nIn both cases, the result is a $1 \\times 4$ vector. If we had convolved $f$ with a $1 \\times 1$ vector, the result would have been a $1 \\times 5$ vector. Recall that we assumed no padding (i.e. we don't add dummy elements to the left or right borders of $f$) and stride 1 (i.e. we shift the kernel to the right one element at a time). Similarly, if we had convolved $f$ with a $1 \\times 3$, the result would have been a $1 \\times 3$ vector (as you will see from the next example).\n\nThe results of the convolution and cross-correlation, $g_1$ and $g_2$, are different. Specifically, one is the negated version of the other. So, the result of the convolution is generally different than the result of the cross-correlation, given the same signals and kernels (as you might have suspected).\n\n\nExample 2: symmetric kernel\nNow, let's convolve $f$ with a $1 \\times 3$ kernel that is symmetric around the middle element, $h_2 = [-1, 2, -1]$. Let's first compute the convolution.\n\\begin{align}\nf \\circledast h_2 = g_3\n&=\\\\\n[(-1)*2 + 1*2 + (-1) * 3,\\\\ (-1)*1 + 2*3 + (-1) * 5,\\\\ (-1)*3 + 2*5 + (-1) * 4] \n&=\\\\ \n[-2 + 2 + -3, -1 + 6 + -5, -3 + 10 + -4] \n&=\\\\\n[-3, 0, 3]\n\\in \\mathbb{R}^{1 \\times 3}\n\\end{align}\nNow, let's compute the cross-correlation\n\\begin{align}\nf \\otimes h_2 = g_4\n&=\\\\\n[(-1)*2 + 1*2 + (-1) * 3, \\\\ (-1)*1 + 2*3 + (-1) * 5, \\\\ (-1)*3 + 2*5 + (-1) * 4] \n&=\\\\\n[-3, 0, 3]\n\\in \\mathbb{R}^{1 \\times 3}\n\\end{align}\nYes, that's right! In this case, the result of the convolution and the cross-correlation is the same. This is because the kernel is symmetric around the middle element. This result applies to any convolution or cross-correlation in any dimension. For example, the convolution of the 2d Gaussian kernel (a centric-symmetric kernel) and a 2d image is equal to the cross-correlation of the same signals.\nCNNs have learnable kernels\nIn the case of CNNs, the kernels are the learnable parameters, so we do not know beforehand whether the kernels will be symmetric or not around their middle element. They won't probably be. In any case, CNNs can perform either the cross-correlation (i.e. no flip of the filter) or convolution: it does not really matter if they perform cross-correlation or convolution because the filter is learnable and can adapt to the data and tasks that you want to solve, although, in the visualizations and diagrams, CNNs are typically shown to perform the cross-correlation (but this does not have to be the case in practice).\nDo libraries implement the convolution or correlation?\nIn practice, certain libraries provide functions to compute both convolution and cross-correlation. For example, NumPy provides both the functions convolve and correlate to compute both the convolution and cross-correlation, respectively. If you execute the following piece of code (Python 3.7), you will get results that are consistent with my explanations above.\nimport numpy as np \n\nf = np.array([2., 1., 3., 5., 4.])\n\nh = np.array([1., -1.])\nh2 = np.array([-1., 2., 1.])\n\ng1 = np.convolve(f, h, mode=\"valid\")\ng2 = np.correlate(f, h, mode=\"valid\")\n\nprint(\"g1 =\", g1) # g1 = [-1.  2.  2. -1.]\nprint(\"g2 =\", g2) # g2 = [ 1. -2. -2.  1.]\n\nHowever, NumPy is not really a library that provides out-of-the-box functionality to build CNNs.\nOn the other hand, TensorFlow's and PyTorch's functions to build the convolutional layers actually perform cross-correlations. As I said above, although it does not really matter whether CNNs perform the convolution or cross-correlation, this naming is misleading. Here's a proof that TensorFlow's tf.nn.conv1d actually implements the cross-correlation.\nimport tensorflow as tf # TensorFlow 2.2\n\nf = tf.constant([2., 1., 3., 5., 4.], dtype=tf.float32)\nh = tf.constant([1., -1.], dtype=tf.float32)\n\n# Reshaping the inputs because conv1d accepts only certain shapes.\nf = tf.reshape(f, [1, int(f.shape[0]), 1])\nh = tf.reshape(h, [int(h.shape[0]), 1, 1])\n\ng = tf.nn.conv1d(f, h, stride=1, padding=\"VALID\")\nprint(\"g =\", g) # [1, -2, -2, 1]\n\nFurther reading\nAfter having written this answer, I found the article Convolution vs. Cross-Correlation (2019) by Rachel Draelos, which essentially says the same thing that I am saying here, but provides more details and examples.\n", "type": 2, "id": "22000", "date": "2020-06-18T17:13:00.543", "score": 7, "comment_count": 0, "parent_id": "21999"}, "22007": {"line": 14605, "body": "Just as a short and quick answer to build of nbros:\nThe way CNNs are typically taught, they are taught using a correlation on the forward pass, rather than a convolution. In reality, Convolutional neural networks is a bit of a misleading name, but not entirely incorrect.\nCNNs do in fact use convolutions every time they are trained and run. If a correlation is used on the forward pass, a convolution is used on the backward pass. The opposite is true if a convolution is used on the forward pass (which is equally valid as using a correlation). I couldn't seem to find this information anywhere, so had to learn it myself the hard way.\nSo to summarise, a typical CNN goes like this: Correlation forward, convolution backward.\n", "type": 2, "id": "22007", "date": "2020-06-19T04:59:34.363", "score": 0, "comment_count": 5, "parent_id": "21999"}}}
{"line": 14683, "body": "I'm building a denoising autoencoder. I want to have the same input and output shape image.\nThis is my architecture:\ninput_img = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))  \n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n\n\nx = Conv2D(32, (3, 3), activation='relu', padding='valid')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\n\n# decodedSize = K.int_shape(decoded)[1:]\n\n# x_size = K.int_shape(input_img)\n# decoded = Reshape(decodedSize, input_shape=decodedSize)(decoded)\n\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nMy input shape is: 1169x827\nThis is Keras output:\nModel: \"model_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 1169, 827, 1)]    0         \n_________________________________________________________________\nconv2d_30 (Conv2D)           (None, 1169, 827, 32)     320       \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 585, 414, 32)      0         \n_________________________________________________________________\nconv2d_31 (Conv2D)           (None, 585, 414, 64)      18496     \n_________________________________________________________________\nmax_pooling2d_13 (MaxPooling (None, 293, 207, 64)      0         \n_________________________________________________________________\nconv2d_32 (Conv2D)           (None, 291, 205, 32)      18464     \n_________________________________________________________________\nup_sampling2d_12 (UpSampling (None, 582, 410, 32)      0         \n_________________________________________________________________\nconv2d_33 (Conv2D)           (None, 582, 410, 32)      9248      \n_________________________________________________________________\nup_sampling2d_13 (UpSampling (None, 1164, 820, 32)     0         \n_________________________________________________________________\nconv2d_34 (Conv2D)           (None, 1162, 818, 1)      289       \n===============================================================\n\nHow can I have the same input and output shape?\n", "type": 1, "id": "22106", "date": "2020-06-23T13:06:31.737", "score": 2, "comment_count": 0, "tags": ["convolutional-neural-networks", "tensorflow", "keras", "autoencoders"], "title": "How can I have the same input and output shape in an auto-encoder?", "answer_count": 2, "views": 399, "accepted_answer": null, "answers": {"22114": {"line": 14690, "body": "If you look at Keras' output, there are various steps which lose pixels:\nMax pooling on odd sizes will always lose one pixel. Conv2D using 3x3 kernels will also lose 2pixels, although I'm puzzled that it doesn't seem to happen in the downsampling steps.\nIntuitively, padding the original images with enough border pixels to compensate for the pixel loss due to the various layers would be the simplest solution. At the moment I can't calculate how much it should be, but I suspect rounding up to a multiple of 4 should take care of the max pooling layers. For denoising, borders could be just copied from the outermost pixels, probably with some sort of low pass filtering to avoid artefacts.\n", "type": 2, "id": "22114", "date": "2020-06-23T15:24:46.133", "score": 1, "comment_count": 2, "parent_id": "22106"}, "22135": {"line": 14707, "body": "I don't know if this is the right way of doing it but I solved the problem.\nFollowing the code from above I've added:\nimg_size = K.int_shape(input_img)[1:]\n\nresized_image_tensor = tf.image.resize(decoded, list(img_size[:2]))****\n\n\nautoencoder = Model(input_img, resized_image_tensor)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nI used tf.image.resize to synchronize the shape of reconstructed image and input image.\nHope it helps.\n", "type": 2, "id": "22135", "date": "2020-06-24T08:16:35.593", "score": 0, "comment_count": 0, "parent_id": "22106"}}}
{"line": 15342, "body": "I am trying to determine the complexity of the neural network we use. The neural network is a U-net generator with an input shape of NxN (not an image but image-like data) and output of the same shape. There is 7x downsampling and 7x upsampling. Downsampling is a simple convolutional layer, where I have no problem to determine complexity as stated here:\n$$\nO\\left(\\sum_{l=1}^{d} n_{l-1} \\cdot s_{l}^{2} \\cdot n_{l} \\cdot m_{l}^{2}\\right)\n$$\nI however cannot find what is big O complexity for the upsampling stage, where the UpSampling2D layer is used before convolution.\nAny idea what is the time complexity of the upsampling convolutional layer, or where I might find information? Thanks in advance!\n", "type": 1, "id": "22969", "date": "2020-08-10T13:42:03.550", "score": 1, "comment_count": 2, "tags": ["convolutional-neural-networks", "time-complexity", "u-net", "computational-complexity", "pooling"], "title": "What is the time complexity of the upsampling stage of the U-net?", "answer_count": 1, "views": 216, "accepted_answer": "23079", "answers": {"23079": {"line": 15425, "body": "After further investigating the problem I have found the answer:\nU-net generators' up-sampling stage consists of two steps:\n\nUse UpSampling2D layer\nApply convolution on the output\n\nThe UpSampling2D layer is in the keras documentation described as:\nRepeats the rows and columns of the data by size[0] and size[1] respectively.\nFrom this information, we can calculate the time cost for UpSampling2D alone. Lets set size to (2,2), as is set in basic configuration of the U-net generator. The output of the UpSampling2D is then doubled. In case we started with (4,4,3), where the last index corresponds to number of channels, the output shape will be 8,8,3. We can see that each row and column need to be copied twice in each channel. From this we can define time complexity of a single up-sampling as:\n$$\nO\\left(2 \\cdot c \\cdot n \\cdot s\\right)\n$$\nWhere c corresponds to number of channels, n corresponds to input length (one side of a matrix) and s is equal to filter size. Assuming that length and filter size have square shape, the complexity is multiplied by 2. Since in this case the the filter size is known, equal to (2,2), the notation can be simplified to:\n$$\nO\\left(4 \\cdot c \\cdot n \\right) = O\\left(c \\cdot n \\right)\n$$\nIn my case, with only 1 channel, the complexity is simply\n$$\nO\\left(n \\right)\n$$\nWhich means the up-sampling stage is linear, and the only important feature is input size, which is negligible to the complexity of the following convolutional layer and can be ignored.\n", "type": 2, "id": "23079", "date": "2020-08-17T07:51:00.977", "score": 0, "comment_count": 0, "parent_id": "22969"}}}
{"line": 14725, "body": "Why is non-linearity desirable in a neural network?\nI couldn't find satisfactory answers to this question on the web. I typically get answers like \"real-world problems require non-linear solutions, which are not trivial. So, we use non-linear activation functions for non-linearity\".\n", "type": 1, "id": "22166", "date": "2020-06-25T11:39:14.180", "score": 0, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "activation-function"], "title": "Why is non-linearity desirable in a neural network?", "answer_count": 2, "views": 136, "accepted_answer": "22168", "answers": {"22168": {"line": 14726, "body": "Consider what happens if you intend to train a linear classifier on replicating something trivial as the XOR function. If you program/train the classifier (of arbitrary size) such that it outputs XOR condition is met whenever feature a or feature b are present, then the linear classifier will also (incorrectly) output XOR condition is met whenever both features together are present. That is because linear classifiers simply sum up contributions of all features and work with the total weighted inputs they receive. For our example, that means that when the weighted contribution of either feature is sufficient already to trigger the classifier to output XOR condition is met, then obviously also the summed contributions of both features are sufficient to trigger the same response.\nTo get a classifier that is capable of outputting XOR condition is met if and only if the summed contributions of all input features are above a lower threshold and below an upper threshold, commonly non-linearities are introduced. You could of course also try to employ a quadratic function to solve the two-feature problem, but as soon as the number of variables/features increases again, you run into the same problem again, only in higher dimensions. Therefore, the most general approach to solving this problem of learning non-linear functions, like the XOR, is by setting up large models with enough capacity to learn a given task and equipping them with non-linearities. That simplifies training since it allows for using stochastic gradient descent for training the system/classifier, preventing one from having to solve Higher-Degree polynomial equations analytically (which can get computationally quite expensive quite quickly) to solve some task.\nIn case you are interested, here's one paper analyzing and dealing with the XOR problem (as one concrete instance of a problem where purely linear models fail to solve some task).\nEDIT:\nYou can consider a layer in a network as a function $y = f(x)$, where $x$ is the input to some layer $f$ and $y$ is the output of the layer. As you propagate $x$, being the network's input, through the network, you get something like $y = p(t(h(g(f(x)))))$, where $f$ is the input layer and $p$ constitutes the output layer, i.e. a set of weights, by which the input to that respective layer gets multiplied.\nIf $h$, for example, is some non-linear activation function, like ReLU or sigmoid, then $y$, being the network's output, is a non-linear function of input $x$.\n", "type": 2, "id": "22168", "date": "2020-06-25T13:06:44.700", "score": 0, "comment_count": 9, "parent_id": "22166"}, "27071": {"line": 18533, "body": "For a regressor, it can work fine to have an output layer that is linear.\nThe composition of two linear functions is also linear, so in a deep neural net, if all layers are linear it can only learn a linear function. As Daniel B explains, XOR is a good example of a function with no useful linear approximation.\n", "type": 2, "id": "27071", "date": "2021-03-29T16:03:52.067", "score": 0, "comment_count": 0, "parent_id": "22166"}}}
{"line": 16520, "body": "I am trying understand machine learning inferece, and i would like to know what exactly is the difference between Google Coral USB and Movidius Intel Neural Compute Stick 2. From what i could gather the google coral USB speeds up the frame rate, but that doesn't look clear to me. My questions are:\nWhat exactly is the benefit from both of them in units? Like, is it frame rate? prediction speed? Are both visual processing units? And lastly, do i need to keep my neural network in a single computer board for training or can i have it at a cloud?\n", "type": 1, "id": "24595", "date": "2020-11-14T19:48:38.607", "score": 1, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks", "inference"], "title": "Difference between Neural Compute Stick 2 and Google Coral USB for edge computing", "answer_count": 1, "views": 792, "accepted_answer": "24596", "answers": {"24596": {"line": 16521, "body": "I do not know much about the Neural Compute Stick but I can tell you a little bit about the Coral Edge TPU, since I used it myself (which I think also applies to the Neural Compute Stick).\nThe Edgte TPU is a specialized ASIC which is very efficient at the main calculations (convolutions, relu etc.) for neural network inferencing. That means it cannot be used for training* a neural network but for deploying a neural network in production after it has been trained and optimized/quantized (precision reduced from float32 to int8). But I think you already new that, as it seems from your question.\nNow to your actual question in terms of speed: You cannot really compare speed of such chip in terms of framerate alone, nor can you call it a visual processing unit or not. The google coral is a general ASIC that is very fast at doing convolutions/relus etc. For what you gonna use your neural network (like Image recognition, or maybe stock predicitons) is complitly up to you, and especially up to your neural network for which task it was trained for. The only limitation you have here is regarding the supported layer operations. E.g. it is not possible to do operations like 3D convolutions or some fancy new non-linear activation functions (There was an overview of supported operations in the docs that I cannot find right now).\nFurthermore the framerate also depends on your NN architecture, image resolution etc. so a comparison here is completly misleading. If you want to have a general indication of speed look at how many int8 operations it can handle per second (TOPS) also under what energy consuption (Watts), if you care.\nThe main advantage of this unit compared to a GPU that is usually used for training+inferencing is much lower energy consumption and unit cost. With roughly 4 Watts on the Edge TPU (as far as I can remember) compared to e.g. 250 Watts GPU. Energy consumption also dictates the necessary cooling solution, which can just be a passive for the Edge Tpu in many cases. Regarding the unit costs, I guess you can easily sees that yourself, if you keep in mind that you can get \"nearly\" similar inferencing speeds than with a full workstation class GPU. Furthermore, such unit has a much smaller formfactor and weight, which makes it perfect for applications at the edge. (I should also add that the efficiency gain is also a lot due to the precision reduction to int8 (quantization). Quantization is also possible on GPUs.)\n\nAnd lastly, do i need to keep my neural network in a single computer board for training or can i have it at a cloud?\n\nI am not quite sure what you mean with this. The general workflow would look like this: you use a GPU for training your neural network, after training you optimize/quantize your network, and lastly you deploy your neural network to the \"computer board\" of your choice that just needs to have a CPU and the Edge TPU and do inferencing operations/predictions for your task.\n*Transfer learning is possible of e.g. the last layer. But no backpropagation for full NN training.\n", "type": 2, "id": "24596", "date": "2020-11-14T23:47:32.630", "score": 0, "comment_count": 0, "parent_id": "24595"}}}
{"line": 13570, "body": "I am using the following architechture:\n3*(fully connected -> batch normalization -> relu -> dropout) -> fully connected\n\nShould I add the batch normalization -> relu -> dropout part after the last fully connected layer as well (the output is positive anyway, so the relu wouldn't hurt I suppose)? \n", "type": 1, "id": "20675", "date": "2020-04-27T11:02:50.867", "score": 1, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "architecture"], "title": "Should batch-normalization/dropout/activation-function layers be used after the last fully connected layer?", "answer_count": 1, "views": 104, "accepted_answer": null, "answers": {"20713": {"line": 13604, "body": "No, the activation of the output layer should instead be tailored to the labels you're trying to predict. The network prediction can be seen as a distribution, for example a categorical for classification or a Gaussian (or something more flexible) for regression. The output of your network should predict the sufficient statistics of this distribution. For example, a softmax activation on the last layer ensures that the outputs are positive and sum up to one, as you would expect for a categorical distribution. When you predict a Gaussian with mean and variance, you don't need an activation for the mean but the variance has to be positive, so you could use exp as activation for that part of the output.\n", "type": 2, "id": "20713", "date": "2020-04-28T04:05:37.513", "score": 0, "comment_count": 1, "parent_id": "20675"}}}
{"line": 13620, "body": "I'm trying to create a text recognition project using CNN. I need help regarding the text detection task. \nI have the training images and bounding box details for them. But I'm unable to figure out how to create the loss function. \nCan anyone help me by telling how to take the output from the CNN model and compare it to the bounding box labels?\n", "type": 1, "id": "20735", "date": "2020-04-28T15:03:52.557", "score": 2, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "classification", "image-recognition", "optical-character-recognition"], "title": "How should I define the loss function for a multi-object detection problem?", "answer_count": 1, "views": 120, "accepted_answer": null, "answers": {"24881": {"line": 16766, "body": "Okay so your CNN model is taking an Image and outputting the bounding boxes for them. That means the last layer of CNN model must be having four outputs which are generating real numbers. This is a regression problem.\nIn that case, you can take L1 loss (mean absolute error) or L2 loss (mean square error) as your loss function. I have created a similar project, I have used L1 loss.\nSuppose your input image is x and predicted_bb is the model output and real_bb is your original bounding box for image x. Then you should proceed as follows\npredicted_bb = model(x)\n\n# CALCULATE LOSS BETWEEN THE ACTUAL & PREDICTED BB COORDINATES\n\nloss_bb = torch.nn.functional.l1_loss(predicted_bb, real_bb, reduction=\"none\").sum(1)\n\n# SET GRADIENTS TO ZERO\n\noptimizer.zero_grad()\n\n# BACKPROPOGATE THE LOSS\n\nloss.backward()\n\nFor tensorflow keras\npredicted_bb = Dense(4, activation='relu')(x)\n\nmodel = Model(inputs = image_input, outputs = [predicted_bb])\nmodel.compile(loss=['mae'], optimizer='adam', metrics =['accuracy'])\n\n", "type": 2, "id": "24881", "date": "2020-11-28T02:46:55.200", "score": 0, "comment_count": 0, "parent_id": "20735"}}}
{"line": 16612, "body": "Background\nFrom my understanding (and following along with this blog post), (deep) neural networks apply transformations to the data such that the data's representation to the next layer (or classification layer) becomes more separate. As such, we can then apply a simple classifier(s) to the representation to chop up the regions where the different classes exist (as shown by this blog post).\nIf this is true and say we have some noisy data where the classes are not easily separable, would it make sense to push the input to a higher dimension, so we can more easily separate it later in the network?\nFor example, I have some tabular data that is a bit noisy, say it has 50 dimensions (input size of 50). To me, it seems logical to project the data to a higher dimension, such that it makes it easier for the classifier to separate. In essence, I would project the data to say 60 dimensions (layer out dim = 60), so the network can represent the data with more dimensions, allowing us to linearly separate it. (I find this similar to how SVMs can classify the data by pushing it to a higher dimension).\nQuestion\nWhy, if the above is correct, do we not see many neural network architectures projecting the data into higher dimensions first then reducing the size of each layer thereafter?\nI learned that if we have more hidden nodes than input nodes, the network will memorize rather than generalize.\n", "type": 1, "id": "24702", "date": "2020-11-18T19:52:55.390", "score": 2, "comment_count": 0, "tags": ["neural-networks", "overfitting", "multilayer-perceptrons", "generalization"], "title": "Why don't neural networks project the data into higher dimensions first, then reduce the size of each layer thereafter?", "answer_count": 2, "views": 198, "accepted_answer": "24703", "answers": {"24703": {"line": 16613, "body": "To better understand this you should think in terms of capacity. Capacity is a theoretical notion that shows how much information your network can model.\nThe capacity of a network (given sufficient training) ties in directly with the bias/variance tradeoff:\n\ntoo little capacity and your network isn't able to learn the complex relationships in the data.\ntoo much capacity and your network has the capability of learning the noise in the dataset, besides the useful relationships.\n\nAt some point a network reaches a point where it has a high enough capacity to memorize the whole training set!\nNow, by increasing the number of hidden neurons you essentially increase the capacity of your network. If the network already has enough capacity to learn the problem, then by increasing the neurons you are giving the network the capability of overfitting more easily.\nNote: all the above assume that the network is trained sufficiently (i.e. no early stopping, etc).\n", "type": 2, "id": "24703", "date": "2020-11-18T21:36:31.613", "score": 0, "comment_count": 0, "parent_id": "24702"}, "24842": {"line": 16734, "body": "The accepted answer does not answer the question\n\nWhy, if the above is correct, do we not see many neural network architectures projecting the data into higher dimensions first then reducing the size of each layer thereafter?\n\nYes, it's true that if you increase the number of hidden neurons, you generally increase the capacity (in fact, the VC dimension of neural networks is typically expressed as a function of the number of parameters), but you're also suggesting to decrease the size afterwards.\nFor example, in this tutorial, they use 10 hidden neurons for the first hidden layer of an MLP, while the dataset contains only 4 features, which means that there are $4*10 = 40$ weights (aka parameters). In this other tutorial, there are less than 20 features (and only a few of them are used) and the first hidden layer has 128 neurons, which means that each feature is connected to $128$ hidden neurons, so there should be $4*128 = 512$ weights. So, MLPs can easily have more hidden neurons in the first layer and weights that connect the input(s) to the hidden neurons than the number of features.\nIn the case of CNNs, you may have fewer scalar parameters because of the properties of CNNs, i.e. parameter sharing. For instance, if you have $64$ filters of shape $3 \\times 3$ and you want to process grayscale images of shape $32 \\times 32$, then you have $3 * 3 * 64 = 576  < 1024 = 32*32$. Note that the number of parameters of the first layer does not change as a function of the input in the case of CNNs, so if that same CNN processes an image of size $5 \\times 5$, then the first layer contains more scalar parameters than pixels.\nSo, in general, NNs can project the data to higher-dimensional space. The typical NNs that project to a lower-dimensional space are auto-encoders or data compressors, in general, that's why they are called in this way.\n", "type": 2, "id": "24842", "date": "2020-11-24T14:53:15.797", "score": 0, "comment_count": 0, "parent_id": "24702"}}}
{"line": 14773, "body": "Given network architecture, what are the possible ways to define fully connected layer fc1 to have a generalized structure such as nn.Linear($size_of_previous_layer$, 50)?\nThe main issue arising is due to x = F.relu(self.fc1(x)) in the forward function. After using the flatten, I need to incorporate numerous dense layers. But to my understanding, self.fc1 must be initialized and hence, needs a size (to be calculated from previous layers). How can I declare the self.fc1 layer in a generalized manner?\nMy Thought:\nTo get the size, I can calculate the size of the outputs from each of Convolution layer, and since I have just 3, it is feasible. But, in case of n layers, how can you get the output size from the final convolutional layer?\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=3, padding = 1)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=3, padding = 1)\n        self.conv2_drop = nn.Dropout2d(0.4)\n        self.conv3 = nn.Conv2d(20, 40, kernel_size=3, padding = 1)\n        self.conv3_drop = nn.Dropout2d(0.4)\n\n        self.fc1 = nn.Linear(360, 50)  # self.fc1 = nn.Linear($size_of_previous_layer$, 50)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n\n        x = x.flatten(1)\n\n        x = F.relu(self.fc1(x))\n        return F.log_softmax(x)\n\nInput to the following architecture can assumed to be [3, 32, 32] (num_of_channels, height, width).\nPS:\n\nAnswers are expected in PyTorch.\nFor single convolutional layer, it is quite easy. The question refers to solve for n consecutive convolutional layers.\n\n", "type": 1, "id": "22231", "date": "2020-06-28T21:06:10.233", "score": 0, "comment_count": 1, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "pytorch"], "title": "How is it possible to get the output size of `n` Consecutive Convolutional layers?", "answer_count": 1, "views": 37, "accepted_answer": "22277", "answers": {"22277": {"line": 14808, "body": "The question was also asked in Pytorch Forums, where it has been sort of resolved and would love to have further discussions over there.\nLink to Pytorch thread: https://discuss.pytorch.org/t/how-is-it-possible-to-get-the-output-size-of-n-consecutive-convolutional-layers/87300\n", "type": 2, "id": "22277", "date": "2020-06-30T21:58:19.827", "score": 0, "comment_count": 0, "parent_id": "22231"}}}
{"line": 13816, "body": "I wanted to train a model that recognizes sign language. I have found a dataset for this and was able to create a model that would get 94% accuracy on the test set. I have trained models before and my main goal is not to have the best model (I know 94% could easiy be tuned up). However these models where always for class exercises and thus were never used on 'real' new data.\nSo I took a new picture of my hand that I know I wanted to be a certain letter (let's assume A).\nSince my model was trained on 28x28 images, I needed to re-size my own image because it was larger. After that I fed this image to my model only to get a wrong classification.\nhttps://imgur.com/a/QE6snTa\nThese are my pictures (upper-left = my own image (expected class A), upper-right = an image of class A (that my model correctly classifies as A), bottom = picture of class Z (the class my image was classified as)).\nYou can clearly see that my own image looks for more like the image of class A (that I wanted my model to predict), than the model it did predict.\nWhat could be reasons that my model does not work on real-life images? (If code is wanted I can provide it ofcourse but since I don't know where I go wrong, it seemed out of line to copy all the code).\n", "type": 1, "id": "20997", "date": "2020-05-07T21:14:24.493", "score": 0, "comment_count": 0, "tags": ["convolutional-neural-networks", "image-recognition"], "title": "My CNN model performs bad on new (self-created) pictures, what are possible reasons?", "answer_count": 3, "views": 258, "accepted_answer": null, "answers": {"20998": {"line": 13817, "body": "I'm assuming that you used LeNet (our some other model with small number of parameters) since your training image size is 28x28. Note that LeNet doesn't generalize well to new images. I think it performs fine (>90%) on MNIST but not good on CIFAR10 (>60%) albeit both datasets contain similar size image. (Just trying to remember the performance from PyTorch implementations). It's more about if the model has capacity to learn the complexity of dataset. CIFAR10 is more complex and harder to model than MNIST. \nLeNet is a small image classification model (in terms of capacity) so it cannot nicely learn the correlation between pixels of input images well and therefore doesn't perform well on unseen images. \nIn your case it seems like your model has overfit to training examples. It might perform well on test images because both training and test subsets are sampled from same data generating distribution but real-world images it might experience in future might be different (like your own hand). If it doesn't perform well on unseen images we say it has not generalized well, which looks the case in your situation. In this case you need a validation set to validate that your model generalizes to unseen images. If you have it then you should use it in early stopping regularization technique. You can also add other regularizers to your model (the simplest one is weight decay). \nBut instead of inventing your network architecture why don't you use models like ResNet. Just fine-tune the pre-trained ResNet on your own dataset. I'd prefer to fine-tune personally in this situation because data distribution it was trained on (ImageNet) is pretty different from your hand sign dataset. In other case if your dataset contained nature and surrounding images I'd rather freeze the parameters of fixed-feature extractor layers and trained only the last few layers of ResNet (or similar model). \nI hope this helps! \n", "type": 2, "id": "20998", "date": "2020-05-08T01:03:23.660", "score": 1, "comment_count": 0, "parent_id": "20997"}, "21734": {"line": 14403, "body": "Are you sure the image quality in your test set and phone camera image similar?\nI once trained a CNN model on poor quality images and with very good validation accuracy but when I tested on image from my camera it didn't work at all. I degraded the image quality by resizing the image from my camera to a small size then again back to required size and it worked perfectly.\n", "type": 2, "id": "21734", "date": "2020-06-08T15:56:17.367", "score": 0, "comment_count": 0, "parent_id": "20997"}, "21006": {"line": 13823, "body": "This is not an uncommon situation. The data set your model is trained on represents a certain probability distribution. Your test set is most likely a good representation from that distribution so your test results will be good. \nHowever when you use real world images they may or may not have a similar distribution. Typically if the training set is large and diverse it is a good representation of the distribution and when the model is used to classify a real world image it will do so correctly.\nI think I know of the data set you are working with and if I recall it is fairly large.\nSo the problem may be that your model is not complex enough to fully capture the complexity of the data. You can test that fairly simply by using transfer learning with a model that is known to be effective for image classification. I recommend using the MobileNet model. It contains only about 4 million parameters but is about as accurate as larger models containing 10 times as many parameters. So MobileNet is not computationally expensive to train. Documentation can be found here.\n", "type": 2, "id": "21006", "date": "2020-05-08T14:06:59.953", "score": 1, "comment_count": 0, "parent_id": "20997"}}}
{"line": 13657, "body": "\n\"Single-object tracking commonly uses Siamese networks, which can be seen as an RNN unrolled over two time-steps.\"\n\n(from the SQAIR paper)\nI'm wondering how Siamese networks can be viewed as RNNs, as mentioned above. A diagrammatic explanation, or anything that helps understand the same, would help! Thank you!\n", "type": 1, "id": "20782", "date": "2020-04-30T03:53:39.437", "score": 2, "comment_count": 0, "tags": ["neural-networks", "recurrent-neural-networks", "papers"], "title": "How can Siamese Networks be viewed as RNNs?", "answer_count": 2, "views": 108, "accepted_answer": "20874", "answers": {"20886": {"line": 13733, "body": "Single object tracking using a Siamese Network is a Detect and compare approach, where an object of interest is detected and the one to be tracked is passed through a siam network with next consecutive frame to get a correlation between them, if you look at the related reference, it is a correlation-based tracking, ie. correlation between objects detected in one frame and the ones detected in next frame, which you can imagine as samples considered across 2 timesteps or an RNN unrolled to 2 timesteps\n", "type": 2, "id": "20886", "date": "2020-05-04T07:55:22.150", "score": 0, "comment_count": 0, "parent_id": "20782"}, "20874": {"line": 13724, "body": "\nWell, here in the picture we have the unrolled or unfold RNN on the right side. Siamese network is formed when it is said to be \"unrolled over two time-steps\". So, take part where there is two first iterations of RNN and yes, you have kind of Siamese network.\nOne take from the source of the image:\n\nUnlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (U, V, W above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.\n\nSounds familiar to siamese network used on single-object tracking: there we take two signals (image and the tracked object), drive it through identical paths and make some maths to get results. Just something the RNN makes to time separated values!\nFor proof of similarity, a take from a site where siamese networks are nicely explained:\n\nSide note: I don't know then, how closely those relate in real world (could a Siamese network in anyway be a RNN or vice versa), but supposedly so, because the comparison is made by researcher to say so. At diagrammatic level at least there would be no problem on that.\n", "type": 2, "id": "20874", "date": "2020-05-03T16:18:50.443", "score": 2, "comment_count": 0, "parent_id": "20782"}}}
{"line": 15558, "body": "I understand that the batch size is the number of examples you pass into the neural network (NN). If the batch size is 10, it means you feed the NN 10 examples at once.\nAssuming I have an NN with a single Dense layer. This Dense layer of 20 units has an input shape (10, 3). This means that I am feeding the NN 10 examples at once, with every example being represented by 3 values. This Dense layer will have an output shape of (10, 20).\nI understand that the 20 in the 2nd dimension comes from the number of units in the Dense layer. However, what does the 10 (Batch Size) in the first dimension mean? Does this mean that the NN learns 10 separate sets of weights (with each set of weights corresponding to one example, and one set of weights being a matrix of 60 values:3 features x 20 units)?\n", "type": 1, "id": "23265", "date": "2020-08-26T15:18:39.967", "score": 1, "comment_count": 1, "tags": ["neural-networks", "keras", "hidden-layers", "fully-connected-layer", "batch-learning"], "title": "Why does the output shape of a Dense layer contain a batch size?", "answer_count": 1, "views": 159, "accepted_answer": "23273", "answers": {"23273": {"line": 15566, "body": "The Dense layers outputs 20 values per example. And since you have 10 examples in the batch the output is (10, 20) (one set of 20 values per example in the batch).\nThe nn doesn't learn 10 separate sets of weights. Each set of 20 values is computed with the same weight (and bias if you have any). So if say example 2 and 5 had the same input values, they'll always have the same output values.\n", "type": 2, "id": "23273", "date": "2020-08-26T19:41:28.677", "score": 0, "comment_count": 2, "parent_id": "23265"}}}
{"line": 13842, "body": "I am reading a paper implementing a deep deterministic policy gradient algorithm for portfolio management. My question is about a specific neural network implementation they depict in this picture (paper, picture is on page 14).\n\nThe first three steps are convolutions. Once they have reduced the initial tensor into a vector, they add that little yellow square entry to the vector, called the cash bias, and then they do a softmax operation.\nThe paper does not go into any detail about what this bias term could be, they just say that they add this bias before the softmax. This makes me think that perhaps this is a standard step? But I don't know if this is a learnable parameter, or just a scalar constant they concatenate to the vector prior to the softmax.\nI have two questions:\n1) When they write softmax, is it safe to assume that this is just a softmax function, with no learnable parameters? Or is this meant to depict a fully connected linear layer, with a softmax activation?\n2) If it's the latter, then I can interpret the cash bias as being a constant term they concatenate to the vector before the fully connected layer, just to add one more feature for the cash assets. However, if softmax means just a function, then what is this cash bias? It must be a constant that they implement, but I don't see what the use of that would be, how can you pick a constant scalar that you are confident will have the intended impact on the softmax output to bias the network to put some weight on that feature (cash)? \nAny comments/interpretations are appreciated!\n", "type": 1, "id": "21033", "date": "2020-05-09T19:49:04.690", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "papers", "algorithmic-bias", "softmax"], "title": "What do the authors of this paper mean by the bias term in this picture of a neural network implementation?", "answer_count": 1, "views": 43, "accepted_answer": null, "answers": {"32327": {"line": 21222, "body": "to the cash bias: I think this is simply the money that is still available at time t=50 and has not yet been invested.\n", "type": 2, "id": "32327", "date": "2021-11-08T09:57:36.437", "score": 0, "comment_count": 0, "parent_id": "21033"}}}
{"line": 17110, "body": "In Chapter 8, section 8.5.2, Raul Rojas describes how the weights for a layer of a neural network can be calculated using a pseudoinverse of the sigmoid function in the nodes, he explains this is an example of symmetric relaxation.\nBut the chapter doesn't explain what asymmetric relaxation would be or how it is done.\nSo, what is asymmetric relaxation and how would it be done in a simple neural network using a sigmoid function in its nodes?\n", "type": 1, "id": "25296", "date": "2020-12-19T22:53:42.280", "score": 3, "comment_count": 0, "tags": ["neural-networks", "terminology", "backpropagation"], "title": "What is asymmetric relaxation backpropagation?", "answer_count": 1, "views": 117, "accepted_answer": null, "answers": {"25432": {"line": 17219, "body": "I'll give you my initial $0.02 for symmetric relaxation or relaxation in general in working with neural networks. The book covers 'Weight perturbation' and this is a basic outline of that. Say you want to host a wedding and every person gives you a 'must-have' list of requirements for them to attend. You can abide by all the requirements of each wedding guest or start 'uninviting' guests whose restrictions cause too many complications.\nThere are several kinds of relaxation. I've only used Lagrangian relaxation, so my experience is biased to that application. Think of it like this: you are traveling from New York to LA and you want to optimize for time, if you 'relax' the constraints, you can just fly instead of driving. This, however, creates an increased cost of the air ticket. By relaxing the constraints you remove the isolating requirement that you must travel by car.\nSymmetric relaxation can be a challenging subject, so I'll include a few links academic research\nAcademic research arxiv.org is another site I use for research. Hope this helps.\nI also found a link on Medium which is another good source for application, theory, and implementation of algorithms. Medium Lagrangian Relaxation\n", "type": 2, "id": "25432", "date": "2020-12-28T21:09:50.283", "score": 0, "comment_count": 1, "parent_id": "25296"}}}
{"line": 17187, "body": "I am trying to make a neural network framework from scratch in C++ just for fun, and to test my backpropagation, I thought it would be an easy way to test the functionality if I give it one input - a randomized size 10 vector, and one output: a size 5 vector containing all 1s, and train it a bunch of times to see if the loss will decrease. Essentially trying to make it overfit\nThe problem is that for each run that I do, the loss either shoots up and goes to nan, or reduces a lot, going to 0.000452084 or other similar small values. However even in the low end of things, my output (which should be close to all 1s, as the \"ground truth\") is something like:\n0.000263654\n1e-07\n8.55893e-05\n1e-07\n0.999651\n\nThe only close value close to 1 being the last element.\nMy network consists of the input layer 10 neurons, one 10 neuron dense layer with RELU activation, and another 5 neuron dense layer for output, with SoftMax activation. I am using categorical cross entropy as my loss function, and I am normalizing my gradient by dividing it by the norm of my gradient if it is over 1.0. I initialize my weights to be random values between -0.1 and 0.1\nTo calculate the gradient of the loss function, I use -groundTruth/predictedOutput. To calculate\nthe other derivatives, I dot the derivative of that layer with the gradient of the previous layer with respects to its activation function.\nBefore this problem I was having exploding gradients, which the gradient scaling fixed, however it was very weird that that would even happen on a very small network like this, which could be related to the problem I am currently having. Is the implementation not correct or am I missing something very obvious?\nAny ideas about this weird behavior, and where I should look first? I am not sure how to show a minimal reproduceable example as that would require me to paste the whole codebase, but I am happy to show pieces of code with explanation. Any advice welcomed!!\n", "type": 1, "id": "25390", "date": "2020-12-26T15:42:06.640", "score": 0, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "deep-learning", "objective-functions", "gradient-descent"], "title": "Loss randomly changing, incorrect output (even for low loss) when trying to overfit on a single set of input and output", "answer_count": 1, "views": 43, "accepted_answer": "25394", "answers": {"25394": {"line": 17189, "body": "Softmax activation always adds up to 1, because it's designed to deal with probabilities (in problems of classification, those probabilities represent how likely the network thinks an object belongs to a specific class). You can verify that by summing up the numbers of your output layer. So currently your network is trying to do the impossible, to produce output that sums up to 5, instead of 1. Therefore, loss will never become stable. Since you want your output layer to produce all ones, you need to use some other activation, for example, linear. Linear activation does not have the same constraint that Softmax does.\n", "type": 2, "id": "25394", "date": "2020-12-26T20:54:23.760", "score": 0, "comment_count": 1, "parent_id": "25390"}}}
{"line": 17215, "body": "Suppose that a simple feedforward neural network (FFNN) contains $n$ hidden layers, $m$ training examples, $x$ features, and $n_l$ nodes in each layer. What is the space complexity to train this FFNN using back-propagation?\nI know how to find the space complexity of algorithms. I found an answer here, but here it is said that the space complexity depends on the number of units, but I think it must also depend on the input size.\nCan someone help me in finding its worst-case space complexity?\n", "type": 1, "id": "25428", "date": "2020-12-28T16:26:41.633", "score": 1, "comment_count": 4, "tags": ["neural-networks", "feedforward-neural-networks", "computational-complexity", "space-complexity"], "title": "What is the space complexity for training a neural network using back-propagation?", "answer_count": 1, "views": 602, "accepted_answer": null, "answers": {"25430": {"line": 17217, "body": "I will not tell you what the exact space complexity of training an FFNN with GD and BP is (because that actually depends on the specific implementation of GD and BP and I don't want to dive into the details of some specific implementation now, maybe later!), but I will guide you towards the specific answer, which you should be able to figure out alone (although it may take some time because you need to understand all the details of the BP algorithm), if you understand this answer.\nThe space complexity of an algorithm is just the amount of memory that you need to use during the execution of the algorithm. The space complexity, like the time complexity, is typically expressed as a function of the size of the input (or the number of inputs that you have) and, usually, in big-O notation, i.e. in the limiting case. So, $n$ is not the same thing as $\\mathcal{O}(n)$, $\\Omega(n)$ or $\\Theta(n)$. Moreover, you can also express the space/time complexity both in the worst, best, or average case, and this is orthogonal to upper (expressed with $\\mathcal{O}$), lower ($\\Omega$), or tight ($\\Theta$) bounds (check this).\nIf you use gradient descent (GD) and back-propagation (BP) to train an FFNN, at each training iteration (i.e. a GD update), you need to store all the matrices that represent the parameters (or weights) of the FFNN, as well as the gradients and the learning rate (or other hyper-parameters). Let's denote the vector that contains all parameters of the FFNN as $\\theta \\in \\mathbb{R}^m$, so it has $m$ components. The gradient vector has the same dimensionality as $\\theta$, so we need at least to store $2m + 1$ parameters.\nDepending on how you implement BP, you may need more memory. For example, if you need to store all the intermediate terms of the partial derivatives, that will require more memory. To compute exactly the amount of required memory, you will have to expand the gradient vector into all their components (which may not be a pleasant experience). As I just said, this only contributes to the space complexity if you need to store these intermediate components, so, ultimately, the space complexity of an algorithm depends on the specific implementation of the algorithm.\nMoreover, to be precise, we cannot just say that the space complexity is $2m + 1$ or whatever the amount of memory that you require is (although many careless or ignorant people will just say that), because we are not expressing this complexity as a function of the size of the input in the limiting case (which is usually done when expressing the space complexity of an algorithm), the number of layers or the number of units per layer (and you probably want to express the space complexity as a function of these 3 possible variable hyper-parameters).\nIf you take a look at this answer, where I describe how to compute the time complexity of the forward pass of an MLP (or FFNN) as a function of the number of inputs and outputs, the number of layers, and units per layers, then you can express the space complexity for training an FFNN in the same way. Given that you are already familiar with how space and time complexities of an algorithm are calculated (and given that this answer is already quite long), I will not repeat the description here.\nIn any case, to answer one of your questions more directly, yes, the space complexity will depend on the number of inputs that you have, because the number of inputs will determine the number of weights in the first layer, which you need to store in memory. This is true in the case of FFNNs (or MLPs) but note that this would not be true in the case of CNNs (i.e. the number of parameters in the convolutional layers does not depend on the size of the input), and that's why CNNs are often said to be more memory efficient.\n", "type": 2, "id": "25430", "date": "2020-12-28T17:41:55.957", "score": 0, "comment_count": 9, "parent_id": "25428"}}}
{"line": 14053, "body": "In this document, the terms \"Redes Neuronales estaticas monovariables\" and \"Redes Neuronales estaticas multivariables\" are mentioned.\nWhat are mono-variable and multi-variable neural networks? Is it the same as a multi-layer or uni-layer NN?\nI have searched about multivariable/mono-variable static/dynamic neural networks in some books, but at least in those books there's no information about these topics.\nI have the idea it refers to the inputs/outputs, but I'm not sure.\n", "type": 1, "id": "21285", "date": "2020-05-19T05:43:52.500", "score": 0, "comment_count": 0, "tags": ["neural-networks", "terminology"], "title": "What are mono-variable and multi-variable neural networks?", "answer_count": 1, "views": 112, "accepted_answer": null, "answers": {"21309": {"line": 14072, "body": "\nWhat are mono-variable and multi-variable neural networks?\n\nI am not sure about this, because most (if not all useful) neural networks are multivariable neural networks (i.e. they contain multiple parameters). Even the perceptron usually contains more than one parameter, so that terminology isn't clear even to me. Maybe they are referring to the number of inputs (sometimes called variables), but I don't see why this distinction in this context would make sense.\n\nWhat are static and dynamic neural networks?\n\nTo answer this question, I will first quote an excerpt from this document (written in Spanish) to provide some context to Spanish speakers (I am not a Spanish speaker, but I understand 99% of it).\n\nUn primer intento de clasificacion puede separ estaticos y dinamicos o recurrentes (fig.2.1)\nLos modelos estaticos realizan un mapeo entre entrada y salida. Despreciando el tiempo de procesamiento interno, la salida se obtiene en forma inmediata en funcion de la entrada, no existe memoria ni dinamica de estados en el sistema neuronal.\nPor el contrario los sistemas recurrentes si la poseen, son sistemas realimentados que ante un estimulo de entrada evolucionan hasta converger a una salida estable.\nCasos tipicos de ambos sistemas son el Perceptron (Rosemblatt, 1960a) (de una o multiples capas) y la memoria asociativa de Hopfield, respectivamente\n(Tank, 1987).\n\nSo, in this document, the word \"dynamic\" and \"recurrent\" are being used interchangeably. An example of a static (i.e. non-recurrent) neural network is the perceptron. An example of recurrent (or dynamic) neural network is the Hopfield network.\nAnyway, I recommend you contact the author of that article to ask for clarification (especially, about the mono-variable NNs)!\n", "type": 2, "id": "21309", "date": "2020-05-19T16:46:50.580", "score": 0, "comment_count": 2, "parent_id": "21285"}}}
{"line": 17360, "body": "I'm using MATLAB 2019, Linux, and UNet (a CNN specifically designed for semantic segmentation). I'm training the network to classify all pixels in an image as either cell or background to get segmentations of cells in microscopic images. My problem is the network is classifying every single pixel as background, and seems to just be outputting all zeroes. The validation accuracy improves a little at the very start of the training but than plateaus at around 60% for the majority of the training time. The network doesn't seem to be training very well and I have no idea why.\nCan anyone give me some hints about what I should look into more closely? I just don't even know where to start with debugging this.\nHere's my code:\n    % Set datapath\n    datapath = '/scratch/qbi/uqhrile1/ethans_lab_data';\n    \n    % Get training and testing datasets\n    images_dataset = imageDatastore(strcat(datapath,'/bounding_box_cropped_resized_rgb'));\n    load(strcat(datapath,'/gTruth.mat'));\n    labels = pixelLabelDatastore(gTruth);\n    [imdsTrain, imdsVal, imdsTest, pxdsTrain, pxdsVal, pxdsTest] = partitionCamVidData(images_dataset,labels);\n    \n    % Weight segmentation class importance by the number of pixels in each class\n    pixel_count = countEachLabel(labels); % count number of each type of pixel\n    frequency = pixel_count.PixelCount ./ pixel_count.ImagePixelCount; % calculate pixel type frequencies\n    class_weights = mean(frequency) ./ frequency; % create class weights that balance the loss function so that more common pixel types won't be preferred\n    \n    % Specify the input image size.\n    imageSize = [512 512 3];\n    \n    % Specify the number of classes.\n    numClasses = 2;\n    \n    % Create network\n    lgraph = unetLayers(imageSize,numClasses);\n    \n    % Replace the network's classification layer with a pixel classification\n    % layer that uses class weights to balance the loss function\n    pxLayer = pixelClassificationLayer('Name','labels','Classes',pixel_count.Name,'ClassWeights',class_weights);\n    lgraph = replaceLayer(lgraph,\"Segmentation-Layer\",pxLayer);\n    \n    %% TRAIN THE NEURAL NETWORK\n    \n    % Define validation dataset-with-labels\n    validation_dataset_with_labels = pixelLabelImageDatastore(imdsVal,pxdsVal);\n    \n    % Training hyper-parameters: edit these settings to fine-tune the network\n    options = trainingOptions('adam', 'LearnRateSchedule','piecewise', 'LearnRateDropPeriod',10, 'LearnRateDropFactor',0.3, 'InitialLearnRate',1e-3, 'L2Regularization',0.005, 'ValidationData',validation_dataset_with_labels, 'ValidationFrequency',10, 'MaxEpochs',3, 'MiniBatchSize',1, 'Shuffle','every-epoch');\n    \n    % Set up data augmentation to enhance training dataset\n    aug_imgs = {};\n    numberOfImages = length(imdsTrain.Files);\n    for k = 1 : numberOfImages\n        % Apply cutout augmentation\n        img = readimage(imdsTrain,k);\n        cutout_img = random_cutout(img);imwrite(cutout_img,strcat('/scratch/qbi/uqhrile1/ethans_lab_data/augmented_dataset/img_',int2str(k),'.tiff'));\n    end\n    aug_imdsTrain = imageDatastore('/scratch/qbi/uqhrile1/ethans_lab_data/augmented_dataset');\n    % Add other augmentations\n    augmenter = imageDataAugmenter('RandXReflection',true, 'RandXTranslation',[-10 10],'RandYTranslation',[-10 10]);\n    % Combine augmented data with training data\n    augmented_training_dataset = pixelLabelImageDatastore(aug_imdsTrain, pxdsTrain, 'DataAugmentation',augmenter);\n    \n    % Train the network\n    [cell_segmentation_nn, info] = trainNetwork(augmented_training_dataset,lgraph,options);\n    \n    save cell_segmentation_nn\n\n", "type": 1, "id": "25596", "date": "2021-01-06T05:46:33.883", "score": 0, "comment_count": 0, "tags": ["convolutional-neural-networks", "training", "image-segmentation", "matlab", "u-net"], "title": "Semantic segmentation CNN outputs all zeroes", "answer_count": 2, "views": 40, "accepted_answer": null, "answers": {"25656": {"line": 17412, "body": "I have never used MATLAB for ml before, so it is difficult for me to understand all your code. My first association to your problem is class imabalance. Since you seem to have got a handle on that, the problem could be dying ReLU or bloated activations. To check if the ReLU is dying, you could look at the activations of the early layers of your network. If many values are zero, it should be dying ReLU.\n", "type": 2, "id": "25656", "date": "2021-01-09T15:35:23.307", "score": 0, "comment_count": 0, "parent_id": "25596"}, "25669": {"line": 17421, "body": "Similar to other answers, I don't know Matlab that well but you could try the following steps to debug your problem.\nMake sure you can overfit to a single instance\nfrom your dataset, pull out a single image with a good amount of true positives in it. Duplicate that images B times (where B = Batch Size) and then try to train your network with only that small dataset. If you can't overfit to a single instance, then something is really wrong and you should validate all functional aspects of your network. If you can overfit, then it's probably more of an algorithmic or data imbalance issue.\nValidate Functional Aspects of the Network\nMake sure your input data and labels are correct.\nValidate that your images are being correclty inputted into the network. You can do this by printing out any images right before they go into the training function. For the labels, make sure to manually inspect a few labels to be sure that the labels correctly match up the images.\nMake sure that your loss function is correct.\nAdd a unit test or two to your loss function to validate that it is doing what it should be doing. Create a simiple example that you can easily validate.\nValidate the rest of the non-algorithmic functionality\nAnything that isn't a design choice should be validated. Make sure that the weights are the correct sizes, each layer has the correct number of weights, the intermediate features have the correct shape, etc.\nData Imbalance\nIf you were able to overfit to a single image, and validated the functional aspects of your network then you may be facing a data imbalance issue. Look at your dataset and see what % of your instances are true positives vs. true negatives. If you have an exteme imbalance (like 10% 90%) or something like that, then build a dataset that is more balanced and see if you can fit your data. If you fit the data with that more balanced dataset, then there's plenty of ways to fix your data imbalance issue. Google around for data imbalance and you should get a few good ideas. Some include focal loss, upsampling, etc...\nCheck your receptive field\nThe receptive field on a network is basically the area that can be looked at by the network for a specific region. This is controlled by the size of the convolution filters, stride, etc. If the data that you are segmenting doesn't fit within the receptive field of the model, then you might be saturating the loss function. tldr try playing around with kernel sizes.\n", "type": 2, "id": "25669", "date": "2021-01-10T00:13:58.870", "score": 1, "comment_count": 0, "parent_id": "25596"}}}
{"line": 15353, "body": "I am implementing OpenAI gym's cartpole problem using Deep Q-Learning (DQN). I followed tutorials (video and otherwise) and learned all about it. I implemented a code for myself and I thought it should work, but the agent is not learning. I will really really really appreciate if someone can pinpoint where I am doing wrong.\nNote that I have a target neuaral network and a policy network already there. The code is as below.\nimport numpy as np\nimport gym\nimport random\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom collections import deque\n\nenv = gym.make('CartPole-v0')\n\nEPISODES = 2000\nBATCH_SIZE = 32\nDISCOUNT = 0.95\nUPDATE_TARGET_EVERY = 5\nSTATE_SIZE = env.observation_space.shape[0]\nACTION_SIZE = env.action_space.n\nSHOW_EVERY = 50\n\nclass DQNAgents:\n    \n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.replay_memory = deque(maxlen = 2000)\n        self.gamma = 0.95\n        self.epsilon = 1\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n        self.model = self._build_model()\n        self.target_model = self.model\n        \n        self.target_update_counter = 0\n        print('Initialize the agent')\n        \n    def _build_model(self):\n        model = Sequential()\n        model.add(Dense(20, input_dim = self.state_size, activation = 'relu'))\n        model.add(Dense(10, activation = 'relu'))\n        model.add(Dense(self.action_size, activation = 'linear'))\n        model.compile(loss = 'mse', optimizer = Adam(lr = 0.001))\n        \n        return model\n\n    def update_replay_memory(self, current_state, action, reward, next_state, done):\n        self.replay_memory.append((current_state, action, reward, next_state, done))\n        \n    def train(self, terminal_state):\n        \n        # Sample from replay memory\n        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n        \n        #Picks the current states from the randomly selected minibatch\n        current_states = np.array([t[0] for t in minibatch])\n        current_qs_list= self.model.predict(current_states) #gives the Q value for the policy network\n        new_state = np.array([t[3] for t in minibatch])\n        future_qs_list = self.target_model.predict(new_state)\n        \n        X = []\n        Y = []\n        \n        # This loop will run 32 times (actually minibatch times)\n        for index, (current_state, action, reward, next_state, done) in enumerate(minibatch):\n            \n            if not done:\n                new_q = reward + DISCOUNT * np.max(future_qs_list)\n            else:\n                new_q = reward\n                \n            # Update Q value for given state\n            current_qs = current_qs_list[index]\n            current_qs[action] = new_q\n            \n            X.append(current_state)\n            Y.append(current_qs)\n        \n        # Fitting the weights, i.e. reducing the loss using gradient descent\n        self.model.fit(np.array(X), np.array(Y), batch_size = BATCH_SIZE, verbose = 0, shuffle = False)\n        \n       # Update target network counter every episode\n        if terminal_state:\n            self.target_update_counter += 1\n            \n        # If counter reaches set value, update target network with weights of main network\n        if self.target_update_counter > UPDATE_TARGET_EVERY:\n            self.target_model.set_weights(self.model.get_weights())\n            self.target_update_counter = 0\n    \n    def get_qs(self, state):\n        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]\n            \n\n''' We start here'''\n\nagent = DQNAgents(STATE_SIZE, ACTION_SIZE)\n\nfor e in range(EPISODES):\n    \n    done = False\n    current_state = env.reset()\n    time = 0 \n    total_reward = 0\n    while not done:\n        if np.random.random() > agent.epsilon:\n            action = np.argmax(agent.get_qs(current_state))\n        else:\n            action = env.action_space.sample()\n        \n        next_state, reward, done, _ = env.step(action)\n\n        agent.update_replay_memory(current_state, action, reward, next_state, done)\n        \n        if len(agent.replay_memory) < BATCH_SIZE:\n            pass\n        else:\n            agent.train(done)\n            \n        time+=1    \n        current_state = next_state\n        total_reward += reward\n        \n    print(f'episode : {e}, steps {time}, epsilon : {agent.epsilon}')\n    \n    if agent.epsilon > agent.epsilon_min:\n        agent.epsilon *= agent.epsilon_decay\n\nResults for first 40ish iterations are below (look for the number of steps, they should be increasing and should reach a maximum of 199)\nepisode : 0, steps 14, epsilon : 1\nepisode : 1, steps 13, epsilon : 0.995\nepisode : 2, steps 17, epsilon : 0.990025\nepisode : 3, steps 12, epsilon : 0.985074875\nepisode : 4, steps 29, epsilon : 0.9801495006250001\nepisode : 5, steps 14, epsilon : 0.9752487531218751\nepisode : 6, steps 11, epsilon : 0.9703725093562657\nepisode : 7, steps 13, epsilon : 0.9655206468094844\nepisode : 8, steps 11, epsilon : 0.960693043575437\nepisode : 9, steps 14, epsilon : 0.9558895783575597\nepisode : 10, steps 39, epsilon : 0.9511101304657719\nepisode : 11, steps 14, epsilon : 0.946354579813443\nepisode : 12, steps 19, epsilon : 0.9416228069143757\nepisode : 13, steps 16, epsilon : 0.9369146928798039\nepisode : 14, steps 14, epsilon : 0.9322301194154049\nepisode : 15, steps 18, epsilon : 0.9275689688183278\nepisode : 16, steps 31, epsilon : 0.9229311239742362\nepisode : 17, steps 14, epsilon : 0.918316468354365\nepisode : 18, steps 21, epsilon : 0.9137248860125932\nepisode : 19, steps 9, epsilon : 0.9091562615825302\nepisode : 20, steps 26, epsilon : 0.9046104802746175\nepisode : 21, steps 20, epsilon : 0.9000874278732445\nepisode : 22, steps 53, epsilon : 0.8955869907338783\nepisode : 23, steps 24, epsilon : 0.8911090557802088\nepisode : 24, steps 14, epsilon : 0.8866535105013078\nepisode : 25, steps 40, epsilon : 0.8822202429488013\nepisode : 26, steps 10, epsilon : 0.8778091417340573\nepisode : 27, steps 60, epsilon : 0.8734200960253871\nepisode : 28, steps 17, epsilon : 0.8690529955452602\nepisode : 29, steps 11, epsilon : 0.8647077305675338\nepisode : 30, steps 42, epsilon : 0.8603841919146962\nepisode : 31, steps 16, epsilon : 0.8560822709551227\nepisode : 32, steps 12, epsilon : 0.851801859600347\nepisode : 33, steps 12, epsilon : 0.8475428503023453\nepisode : 34, steps 10, epsilon : 0.8433051360508336\nepisode : 35, steps 30, epsilon : 0.8390886103705794\nepisode : 36, steps 21, epsilon : 0.8348931673187264\nepisode : 37, steps 24, epsilon : 0.8307187014821328\nepisode : 38, steps 33, epsilon : 0.8265651079747222\nepisode : 39, steps 32, epsilon : 0.8224322824348486\nepisode : 40, steps 15, epsilon : 0.8183201210226743\nepisode : 41, steps 20, epsilon : 0.8142285204175609\nepisode : 42, steps 37, epsilon : 0.810157377815473\nepisode : 43, steps 11, epsilon : 0.8061065909263957\nepisode : 44, steps 30, epsilon : 0.8020760579717637\nepisode : 45, steps 11, epsilon : 0.798065677681905\nepisode : 46, steps 34, epsilon : 0.7940753492934954\nepisode : 47, steps 12, epsilon : 0.7901049725470279\nepisode : 48, steps 26, epsilon : 0.7861544476842928\nepisode : 49, steps 19, epsilon : 0.7822236754458713\nepisode : 50, steps 20, epsilon : 0.778312557068642\n\n", "type": 1, "id": "22986", "date": "2020-08-11T15:42:37.790", "score": 5, "comment_count": 2, "tags": ["reinforcement-learning", "dqn", "deep-neural-networks", "open-ai", "gym"], "title": "My Deep Q-Learning Network does not learn for OpenAI gym's cartpole problem", "answer_count": 2, "views": 334, "accepted_answer": "23062", "answers": {"23060": {"line": 15410, "body": "I think the problem is with openAI gym CartPole-v0 environment reward structure.\nThe reward is always +1 for each time step. So if pole falls reward is +1 itself.\nSo we need to check and redefine the reward for this case.\nSo in the train function try this:\nif not done:\n    new_q = reward + DISCOUNT * np.max(future_qs_list)\nelse:\n    # if done assign some negative reward\n    new_q = -20\n\n(Or change the reward during replay buffer update)\nCheck the lines 81 and 82 in Qlearning.py code in this repo for further clarification.\n", "type": 2, "id": "23060", "date": "2020-08-15T15:57:09.080", "score": 0, "comment_count": 4, "parent_id": "22986"}, "23062": {"line": 15411, "body": "There is a really small mistake in here that causes the problem: \n\nfor index, (current_state, action, reward, next_state, done) in enumerate(minibatch):\n            if not done:\n                new_q = reward + DISCOUNT * np.max(future_qs_list) #HERE \n            else:\n                new_q = reward\n                \n            # Update Q value for given state\n            current_qs = current_qs_list[index]\n            current_qs[action] = new_q\n            \n            X.append(current_state)\n            Y.append(current_qs)\n\nSince np.max(future_qs_list) should be np.max(future_qs_list[index]) since you're now getting the highest Q of the entire batch.\nInstead of the getting the highest Q from the current next state.\nIt's like this after changing that (remember an epsilon of 1 means that you get 100% of your actions taken by the a dice roll so I let it go for a few more epochs, also tried it with the old code but indeed didn't get more then 50 steps (even after 400 epochs/episodes))\nepisode : 52, steps 16, epsilon : 0.7705488893118823\nepisode : 53, steps 25, epsilon : 0.7666961448653229\nepisode : 54, steps 25, epsilon : 0.7628626641409962\nepisode : 55, steps 36, epsilon : 0.7590483508202912\nepisode : 56, steps 32, epsilon : 0.7552531090661897\nepisode : 57, steps 22, epsilon : 0.7514768435208588\nepisode : 58, steps 55, epsilon : 0.7477194593032545\nepisode : 59, steps 24, epsilon : 0.7439808620067382\nepisode : 60, steps 46, epsilon : 0.7402609576967045\nepisode : 61, steps 11, epsilon : 0.736559652908221\nepisode : 62, steps 14, epsilon : 0.7328768546436799\nepisode : 63, steps 13, epsilon : 0.7292124703704616\nepisode : 64, steps 113, epsilon : 0.7255664080186093\nepisode : 65, steps 33, epsilon : 0.7219385759785162\nepisode : 66, steps 33, epsilon : 0.7183288830986236\nepisode : 67, steps 39, epsilon : 0.7147372386831305\nepisode : 68, steps 27, epsilon : 0.7111635524897149\nepisode : 69, steps 22, epsilon : 0.7076077347272662\nepisode : 70, steps 60, epsilon : 0.7040696960536299\nepisode : 71, steps 40, epsilon : 0.7005493475733617\nepisode : 72, steps 67, epsilon : 0.697046600835495\nepisode : 73, steps 115, epsilon : 0.6935613678313175\nepisode : 74, steps 61, epsilon : 0.6900935609921609\nepisode : 75, steps 43, epsilon : 0.6866430931872001\nepisode : 76, steps 21, epsilon : 0.6832098777212641\nepisode : 77, steps 65, epsilon : 0.6797938283326578\nepisode : 78, steps 45, epsilon : 0.6763948591909945\nepisode : 79, steps 93, epsilon : 0.6730128848950395\nepisode : 80, steps 200, epsilon : 0.6696478204705644\nepisode : 81, steps 200, epsilon : 0.6662995813682115\n\n", "type": 2, "id": "23062", "date": "2020-08-15T22:54:23.440", "score": 3, "comment_count": 0, "parent_id": "22986"}}}
{"line": 15535, "body": "I'm not sure how to describe this in the most accurate way but I'll give it a shot.\nI've developed a Inception-Resnet V2 model for detecting audio signals via spectrogram.  It does a pretty good job but is not exactly the way I'd like it to be.\nSome details:  I use 5 sets of data to evaluate my model during training.  They are all similar but slightly different.  Once I get to a certain threshold of F1 Scores for each training set I stop training.  My overall threshold is pretty hard to get to.  Every time training develops a model that produces a \"best yet\" of one of these data sets I save the model.\nWhat I've noticed is that, during training, some round will produce a high F1 Score for one particular set while the other sets languish as mediocre.  Then, several dozen rounds later, another data set will peak while the others are mediocre.  Overall the entire model gets better but there are always some models that work better for some data sets.\nWhat I would like to know is, given I might have 5 different models that each work better for a particular subset of data, is there a way that I can combine these models (either as a whole or better yet their particular layers) to produce a single model that works the best for all my data validation subsets?\nThank you.  Mecho\n", "type": 1, "id": "23234", "date": "2020-08-25T01:32:30.693", "score": 0, "comment_count": 0, "tags": ["convolutional-neural-networks", "models", "audio-processing"], "title": "How to combine specific CNN models that work better at slightly different tasks?", "answer_count": 1, "views": 22, "accepted_answer": null, "answers": {"28019": {"line": 19286, "body": "What you're describing are \"Ensemble Models\" -- where multiple models are trained in parallel, and then combined at inference time to squeeze out better performance.\nThis article gives a decent overview: https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c\n\nA single algorithm may not make the perfect prediction for a given dataset. Machine learning algorithms have their limitations and producing a model with high accuracy is challenging. If we build and combine multiple models, the overall accuracy could get boosted.\n\nAnd they're also covered in Standford's 2017 deep learning computer vision course, lecture 7: https://youtu.be/_JB0AO7QxSA?t=3098.\n\nRather than having just one model, we'll train 10 different modesl independently from different initial random restarts. At test time, we'll run our data trhough all of the 10 models and average the predictions.\n\nI recommend you check out this course, since it also delves quite a bit into convolutional networks too.\n", "type": 2, "id": "28019", "date": "2021-05-30T08:32:28.587", "score": 0, "comment_count": 0, "parent_id": "23234"}}}
{"line": 16012, "body": "Just starting learning things about tensorflow and NN.\nAs an exercise I decided to create a dataset of images, watermarked and not, in order to binary classify these. First of all, the dataset ( you can see it here ) was created artificially by me applying some random watermarks.\nFirst doubt, in the dataset I don't have both images one watermarked and one not, would be better to have?\nSecond, frustrating: model stand on 0.5 accuracy, so it just produce random output :(\nModel I tried is this:\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(16,(1,1), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(32,(3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(64,(3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='elu'),\n    tf.keras.layers.Dense(64, activation='elu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n\nand then compiled as this:\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics = ['accuracy'])\n\nHere below the fit:\nhistory = model.fit(train_data,\n                              validation_data=valid_data,\n                              steps_per_epoch=100,\n                              epochs=15,\n                              validation_steps=50,\n                              verbose=2)\n\nAs for any other details, code is here.\nI already checked for technical issues, I'm pretty sure image enter properly, train and validation dataset are 80/20, about 12K images for training. However accuracy bounches up and down around .5 while fitting. How can I improve?\n", "type": 1, "id": "23864", "date": "2020-10-02T06:54:50.333", "score": 0, "comment_count": 0, "tags": ["convolutional-neural-networks", "tensorflow", "keras", "image-recognition", "binary-classification"], "title": "Image Classification for watermarks with poor results", "answer_count": 1, "views": 34, "accepted_answer": "23975", "answers": {"23975": {"line": 16093, "body": "Well probably the response is that previous approach was a little naive.\nI managed to fave some interesting result with this kernel that allow me to have an accuracy of 0.969  and a validation accuracy of 0.931.\nModel I used is based on ResNet50 with the following additional layers ( and the last one just for binary classification ):\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(32, activation='relu'),\ntf.keras.layers.Dense(1,activation=\"sigmoid\")\n\nEven if the net is already trained, I trained each layer again otherwise I did not have any sensible accuracy.\nTraining history is like this:\n\nStill far to be really good, but progress.\n", "type": 2, "id": "23975", "date": "2020-10-08T15:05:56.290", "score": 0, "comment_count": 0, "parent_id": "23864"}}}
{"line": 16428, "body": "In a Convolutional Neural Network, unlike the fully connected layers, the same filter is used multiple times on the input while convolving - so during backpropagation, we get multiple derivatives for the filter parameters w.r.t the loss function. My question is, why do we sum all the derivatives to get the final gradient? Because, we don't sum the output of the convolution during forward pass. So, isn't it more sensible to average them? What is the intuition behind this?\nPS: although I said CNN, what I'm actually doing is correlation for simplicity of learning.\n", "type": 1, "id": "24471", "date": "2020-11-07T07:09:22.200", "score": 0, "comment_count": 2, "tags": ["convolutional-neural-networks", "backpropagation", "convolution", "filters"], "title": "In CNNs, why do we sum the filter derivatives w.r.t the loss function to get the final gradient?", "answer_count": 1, "views": 75, "accepted_answer": "24993", "answers": {"24993": {"line": 16848, "body": "I really liked the question. Yes, we sum over derivatives. First of all think what backpropagation is trying to do: finding the affect of each parameter on the loss.\nSo as you said:\n\nthe same filter is used multiple times on the input while convolving\n\nmeaning that each kernel affects the final loss in several ways, so those affects should be summed together, not averaged.\n", "type": 2, "id": "24993", "date": "2020-12-04T11:28:15.473", "score": 0, "comment_count": 4, "parent_id": "24471"}}}
{"line": 14668, "body": "I'm new to relatively RNNs, and I'm trying to train generative and guessing neural networks to produce sequences of real numbers that look random. My architecture looks like this (each \"circle\" in the output is the adverserial network's guess for the generated circle vertically below it -- having seen only the terms before it):\n\nNote that the adverserial network is rewarded for predicting outputs close to the true values, i.e. the loss function looks like tf.math.reduce_max((sequence - predictions) ** 2) (I have also tried reduce_mean).\nI don't know if there's something obviously wrong with my architecture, but when I try to train this network (and I've added a reasonable number of layers), it doesn't really work very well.\n\nColab Notebook\n\nIf you look at the result of the last code block, you'll see that my generative neural network produces things like\n\n[0.9907787, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827, 0.9907827]\n\nBut it could easily improve itself by simply training to jump around more, since you'll observe that the adverserial network also predicts numbers very close to the given number (even when the sequence it is given to predict is one that jumps around a lot!).\nWhat am I doing wrong?\n", "type": 1, "id": "22087", "date": "2020-06-23T05:40:23.750", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "recurrent-neural-networks", "generative-adversarial-networks"], "title": "Why does my \"entropy generation\" RNN do so badly?", "answer_count": 1, "views": 109, "accepted_answer": null, "answers": {"22122": {"line": 14697, "body": "I think there are two problems with your network.\nThe first one, always having very similar outputs, is the rather simple one. As it seems, your network suffers from the so-called, very common Mode Collapse problem. The attached link provides both an explanation and some potential remedy to that problem.\nThe second problem is more fundamental. You say that you want your network to produce random numbers. Or numbers that at least appear as such. However, once training is finished, your model is going to be a static function which will not change any further. Given the same input x, it will always produce the same output y. Consequently, unless the inputs to your network contain some randomness already or are, at least, always slightly dissimilar, you will not end up having a random generator. So, whether that is going to be useful will depend on your usecase. But if you make sure that the true random variable (like date&time) serves as input to the RNN and the RNN just translates this into some different format, that might work again. Just keep in mind that randomness can never arise out of a trained model.\n", "type": 2, "id": "22122", "date": "2020-06-23T17:38:07.900", "score": 0, "comment_count": 3, "parent_id": "22087"}}}
{"line": 17075, "body": "As part of a learning more about deep learning, I have been experimenting with writing ResNets with Dense layers to do different types of regression.\nI was interested in trying a harder problem and have been working on a network that, given a private key, could perform point multiplication along ECC curve to obtain a public key.\nI have tried training on a dataset of generated keypairs, but am seeing the test loss values bounce around like crazy with train loss values eventually decreasing after many epochs due to what I assume is overfitting.\nIs this public key generation problem even solvable with a deep learning architecture? If so, am I doing something wrong with my current approach?\n", "type": 1, "id": "25257", "date": "2020-12-18T03:55:59.130", "score": 1, "comment_count": 12, "tags": ["deep-learning", "deep-neural-networks", "residual-networks"], "title": "Regression For Elliptical Curve Public Key Generation Possible?", "answer_count": 1, "views": 71, "accepted_answer": "25384", "answers": {"25384": {"line": 17183, "body": "I don't know of any neural network that can do cryptography well, so you would have to do a little experimenting yourself. The main thing that sticks out to me is that doing operations in the elliptical curve requires the modulus operator since it works in finite field, and I think neural networks have a hard time learning the modulus operator in general. So I would focus on that first. Some things to help the network learn the modulus operator:\n\nI would try to increase the hidden layers to a number 4x-10x bigger than the input dimension, which maps it to a higher dimension to hopefully learn more complex behavior.\nI would use less layers, maybe only 2-4 hidden layers, to speed up the development time.\nMost importantly, I would train with a LOT of data points (25% of the total possible finite field possibilities). I don't think the neural network can learn unless the number of data is this high.\nFor reference, someone got a network to learn the modulus operator using these points.\n\nFor rapid iterating, I would test with a much smaller finite field. E.g. use a 8 bit security and see if the neural network can do well with that first before moving on to the full 256-bit security key (or whatever your end goal is).\nTaking this a step further, I would first test to see if the neural network can even perform a point addition in the elliptical curve well, because if it doesn't then it definitely can't do point multiplication which is needed to compute the public key.\n", "type": 2, "id": "25384", "date": "2020-12-25T22:37:11.967", "score": 0, "comment_count": 0, "parent_id": "25257"}}}
{"line": 15793, "body": "I am constructing a convolutional variational autoencoder for images, starting out with mnist digits. Typically I would specify convolutional layers in the following way:\ninput_img = layers.Input(shape=(28,28,1))\nconv1 = keras.layers.Conv2D(32, (3,3), strides=2, padding='same', activation='relu')(input_img)\nconv2 = keras.layers.Conv2D(64, (3,3), strides=2, padding='same', activation='relu')(conv1) \n...\n\nHowever, I would also like to construct a convolutional filter/kernel that is fixed BUT dependent on some content related to the input, which we can call an auxiliary label. This could be a class label or some other piece of relevant information corresponding to the input. For example, for MNIST I can use the class label as auxiliary information and map the digit to a (3,3) kernel and essentially generate a distinct kernel for each digit. This specific filter/kernel is not learned through the network so it is fixed, but it is class dependent. This filter will then be concatenated with the traditional convolutional filters shown above.\ninput_img = layers.Input(shape=(28,28,1))\nconv1 = keras.layers.Conv2D(32, (3,3), strides=2, padding='same', activation='relu')(input_img)\n\n# TODO: add a filter/kernel that is fixed (not learned by model) but is class label specific\n# Not sure how to implement this?\n# auxiliary_conv = keras.layers.Conv2D(1, (3,3), strides=2, padding='same', activation='relu')(input_img)\n\nI know there are kernel initializers to specify initial weights https://keras.io/api/layers/initializers/, but I'm not sure if this is relevant and if so, how to make this work with a class specific initialization.\nIn summary, I want a portion of the model's weights to be input content dependent so that some of the trained model's weights vary based on the auxiliary information such as class label, instead of being completely fixed regardless of the input. Is this even possible to achieve in Keras/Tensorflow? I would appreciate any suggestions or examples to get started with implementation.\n", "type": 1, "id": "23571", "date": "2020-09-14T03:29:47.943", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "tensorflow", "keras", "variational-autoencoder", "filters"], "title": "How to construct input dependent convolutional filter?", "answer_count": 1, "views": 63, "accepted_answer": null, "answers": {"23595": {"line": 15812, "body": "Not a tensorflow expert but I may be able to offer some conceptual advice. Since you do not care to learn the filter, but instead want to fix a discrete set of possible values for a discrete set of cases, you can use tensor operations (i.e convolutions) rather than neural network layer operations. Essentially, in framework-agnostic pseudocode this would look like:\n# layers with learned parameters\noutput1 = layers1(input)\n\n# apply unlearned but changeable layer convolutions\nkernel_val = kernel_val_selection_function(output1)\noutput2 = convolve_2D(output1,kernel_val)\n\n# more layers with learned parameters\noutput3 = layers3(output2)\n\n...\n\nThe function graph will treat kernel_val as a constant for purposes of backpropagation, so as long as your convolution operations are done within the framework used to create the function graph (i.e. tensorflow) you shouldn't have any problems with backprop.\n", "type": 2, "id": "23595", "date": "2020-09-15T04:18:02.783", "score": 0, "comment_count": 0, "parent_id": "23571"}}}
{"line": 19172, "body": "I am currently reading a paper called Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs (2017, CPPR), and I cannot understand the following sentence:\n\nWe identify that the current formulations of graph convolution do not\nexploit edge labels, which results in an overly homogeneous view of\nlocal graph neighborhoods, with an effect similar to enforcing\nrotational invariance of filters in regular convolutions on images.\n\nWhat does this sentence mean?\n", "type": 1, "id": "27881", "date": "2021-05-20T10:09:39.480", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "papers", "convolution", "geometric-deep-learning", "graph-neural-networks"], "title": "Why the non-exploitation of edge labels in current graph convolutions \"results in an overly homogeneous view of local graph neighborhoods\"?", "answer_count": 1, "views": 27, "accepted_answer": "27896", "answers": {"27896": {"line": 19182, "body": "Consider a two-dimensional convolution layer with 3x3 kernels. The 2d inputs of this layer can be seen as a particular graph with each pixel being a graph node, that is connected to 8 of his neighbors:\n\nThe 3x3 kernels of the convolutional layer not only process the information about neighborhood relation between pixels, but also about their relative orientation. For example the [0,0] element of the kernel might represent the weight of the node to NW. And the [1,2] element of the kernel represent the weight the node across the S edge:\n\nNow, if we make a convolution that \"doesn't exploit edge labels\" then we'll have to forget the labels on the picture above, making us loose directional information:\n\nAll we can say now is that the \"red\" node has those neighbors, but we don't really know how they are oriented relative to it. Since now the sub-graph does not provide any directional information, the learned convolution kernels will be direction-agnostic - in other words, they will be rotation-invariant.\n", "type": 2, "id": "27896", "date": "2021-05-20T23:10:33.183", "score": 0, "comment_count": 3, "parent_id": "27881"}}}
{"line": 14901, "body": "I have seen two different representations of neural networks when it comes to bias. Consider a \"simple\" neural network, with just an input layer, a hidden layer and an output layer. To compute the value of a neuron in the hidden layer, the weights and neurons from the input layer are multiplied, shifted by a bias and then activated by the activation function. To compute the values in the output layer, you may choose not to have a bias and have an identity activation function on this layer, so that this last calculation is just \"scaling\".\nIs it standard to have a \"scaling\" layer? You could say that there is a bias associated with each neuron, except those in the input layer correct (and those in the output layer when it is a scaling layer)? Although I suppose you could immediately shift any value you're given. Does the input layer have a bias?\nI have seen bias represented as an extra unchanging neuron in each layer (except the last) having value 1, so that the weights associated with the connections from this neuron correspond to the biases of the neurons in the next layer. Is this the standard way of viewing bias? Or is there some other way to interpret what bias is that is more closely described by \"a number that is added to the weighted sum before activation\"?\n", "type": 1, "id": "22390", "date": "2020-07-08T00:45:12.953", "score": 2, "comment_count": 2, "tags": ["neural-networks"], "title": "Does the input layer have bias and are there bias neurons?", "answer_count": 1, "views": 525, "accepted_answer": null, "answers": {"22399": {"line": 14906, "body": "The purpose of the input layer is just to conceptually represent the input and, in case it is necessary, define the dimensions of the input that the neural network expects. In fact, some neural networks, such as multi-layer perceptrons, expect a fixed-size input, but not all of them: fully convolutional networks can deal with inputs of different dimensions.\nThe input layer doesn't contain neurons (although in the diagrams that you will come across they are usually represented as circles, like the neurons, and that's probably why you are confused!), so it also does not contain biases, linear transformations, and non-linearities. In fact, in the context of neural networks, you could define a neuron as some unit/entity that performs a linear or non-linear transformation (to which you can add a bias). Note that the hidden and output layers can contain biases because they contain neurons that perform a linear or non-linear transformation.\nHowever, although I have never seen it (or I don't recall having seen it), I would not exclude the existence of an input layer that transforms or augments the inputs before passing them to the next layer. For example, one could implement a neural network that first scales the input to a certain range, and the input layer could do this, although, in practice, this is typically done by some object/class that does not belong to the neural network (e.g. tf.data.Dataset).\n", "type": 2, "id": "22399", "date": "2020-07-08T12:37:32.347", "score": 0, "comment_count": 0, "parent_id": "22390"}}}
{"line": 16908, "body": "I'm trying to create a simple blogpost on RNNs, that should give a better insight into how they work in Keras. Let's say:\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.SimpleRNN(5, return_sequences=True, input_shape=[None, 1]))\nmodel.add(keras.layers.SimpleRNN(5, return_sequences=True))\nmodel.add(keras.layers.Dense(1))\n\nI came up with the following visualization (this is only a sketch), which I'm quite unsure about:\n\nThe RNN architecture is comprised of 3 layers represented in the picture.\nQuestion: is this correct? Is the input \"flowing\" thought each layer neuron to neuron or only though the layers, like in the picture below. Is there anything else that is not correct - any other visualizations to look into?\n\nUpdate: my assumptions are based on my understanding from what I saw in Geron's book. The recurrent neurons are connected, see: https://pasteboard.co/JDXTFVw.png ... he then proceeds to talk about connections between different layers, see: https://pasteboard.co/JDXTXcz.png - did I misunderstand him or is it just a peculiarity in keras framework?\n", "type": 1, "id": "25062", "date": "2020-12-08T09:44:03.710", "score": 4, "comment_count": 2, "tags": ["keras", "recurrent-neural-networks", "data-visualization"], "title": "How to graphically represent a RNN architecture implemented in Keras?", "answer_count": 1, "views": 51, "accepted_answer": null, "answers": {"25063": {"line": 16909, "body": "The first image is correct. The information will flow from left to right in each layer and from top to bottom in between layers.\n", "type": 2, "id": "25063", "date": "2020-12-08T10:41:08.433", "score": 0, "comment_count": 3, "parent_id": "25062"}}}
{"line": 14971, "body": "I am new to working with neural networks. However, I have built some linear regression models in the past. My question is, is it worth looking for features with a correlation to my target variable as I would normally do in a linear regression or is it better to feed the neural network with all the data I have?\nAssuming that the data I have is all related to my target variable of course. I am working with this dataset and building a neural network regressor for it.\nhttps://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv\nHere is a snippet of the data. The target variable is the concrete strength rate given a certain combination of materials for that concrete sample.\n\nI greatly appreciate any tips and explanations. I excuse me if this is too noob of a question but unfortunately I did not find any info about it on google. Thanks again!\n", "type": 1, "id": "22478", "date": "2020-07-13T03:07:55.687", "score": 1, "comment_count": 1, "tags": ["neural-networks", "python", "keras", "linear-regression"], "title": "Do correlations matter when building neural networks?", "answer_count": 1, "views": 82, "accepted_answer": "22479", "answers": {"22479": {"line": 14972, "body": "If there is some correlation between features, that is what the network will ideally find out on its own and learn to utilize. So, in general, don't take correlated samples or features out of the training loop only because they look correlated. After all, they could convey a lot of valuable information.\nWhen it comes to correlation between data samples during training, this correlation is commonly broken up by training a network on randomly selected mini-batches of training data samples. So, you randomly sample e.g. 16 or 32 (or so) training examples based on which you apply a single update of the weights using some Stochastic Gradient Descent variant. Since the members of a mini-batch are sampled at random, chances for finding highly correlated training samples in some mini-batch shall be sufficiently minimized in order not to negatively affect the training outcome.\nHaving said that, if you are concerned about overfitting of your model or weights that would overly weight just a small subset of all available input features, you could try applying regularization techniques like L1 (encouraging sparse representations) or L2 (encouraging low weights in general) regularization or dropout.\nIn your particular case, since the main concern is an excessive contribution of only a small set of input features, L2 shall yield better results (avoiding excessively large weights that would be required to excessively much weight just a small number of features).\nBesides that: Commonly, you split your training dataset into 3 parts:\n\nData used for fitting the model (actual training data)\nData used for assessing the training progress & possibly for determining when to apply early stopping (validation data)\nTest data used to assess the performance of the system after all training & intermediate testing is done\n\nThe final evaluation on the test dataset shall reveal then the generalization ability of your trained model to novel data.\nSo, with regularization in place during training and relatively low error rates on the validation and test datasets, you are pretty much save even without checking for correlated data beforehand. Only when you really struggle decreasing the validation loss, it might be worth to further inspect what exactly is going wrong in terms of correlations and such.\n", "type": 2, "id": "22479", "date": "2020-07-13T09:25:34.530", "score": 0, "comment_count": 1, "parent_id": "22478"}}}
{"line": 16774, "body": "Convolution Neural Network (CNNs) operate over strict grid-like structures ($M \\times N \\times C$ images), whereas Graph Neural Networks (GNNs) can operate over all-flexible graphs, with an undefined number of neighbors and edges.\nOn the face of it, GNNs appear to be neural architectures that can subsume CNNs. Are GNNs really generalized architectures that can operate arbitrary functions over arbitrary graph structures?\nAn obvious follow-up - How can we derive a CNN out of a GNN?\nSince non-spectral GNNs are based on message-passing that employ permutation-invariant functions, is it possible to derive a CNN from a base-architecture of GNN?\n", "type": 1, "id": "24891", "date": "2020-11-28T17:07:49.003", "score": 3, "comment_count": 7, "tags": ["deep-learning", "convolutional-neural-networks", "comparison", "geometric-deep-learning", "graph-neural-networks"], "title": "How can we derive a Convolution Neural Network from a more generic Graph Neural Network?", "answer_count": 1, "views": 131, "accepted_answer": null, "answers": {"25431": {"line": 17218, "body": "Spectral Graph Convolution\nWe use the Convolution Theorem to define convolution for graphs. The Convolution Theorem states that the Fourier transform of the convolution of two functions is the pointwise product of their Fourier transforms:\n$$\\mathcal{F}(w*h) = \\mathcal{F}(w) \\odot \\mathcal{F}(h) \\tag{1}\\label{1} $$\n$$ w * h = \\mathcal{F}^{-1}(\\mathcal{F}(w)\\odot\\mathcal{F}(h))  \\tag{2}\\label{2}$$\nHere $w$ is the filter in spatial domain(time domain) and $h$ is the signal in spatial domain(time domain). For images this signal $h$ is a $2D$ matrix and for other cases this $h$ can be a $1D$ signal.\nAssume we have $n$ number of node in a graph. In graph fourier transform the eigenvalues carry the notion of frequency. $\\Lambda$ is the $ n \\times n$ egenvalue matrix and it is a diagonal matrix. We can write equation 2 as:\n$$w * h = \\phi(\\phi^{T}w \\odot \\phi^{T}h) = \\phi\\hat{w}(\\Lambda)\\phi^{T}h \\tag{3}$$\nHere $\\phi$ is the eigenvector matrix of graph Laplacian $\\in R^{n \\times n}$,\n$\\hat{w}(\\Lambda)$ is the filter in spectral domain(frequency domain) $\\in R^{n \\times n}$ a diagonal matrix, $h$ is the $1D$ graph signal $\\in R^{n}$ in spatial domain and w is the filter in spatial domain $\\in R^{n}$.\nVanilla Spectral GCN\nWe define the spatial convolutional layer such that given layer $h^{l}$ , the activation of the next layer is:\n$$h^{l+1}=\\sigma(w^l*h^l)  \\tag{4}\\label{4},$$\nwhere $\\sigma$ represents a nonlinear activation and $w^l$ is a spatial filter and $h$ is the graph signal.\nWe can perform the above equation in terms of spectral graph convolution operation as:\n$$h^{l+1}=\\sigma(\\hat{w}^l*\\hat{h}^l)  \\tag{5}\\label{5},$$\nwhere $\\hat{w}$ is the same filter but in the spectral domain(frequency domain). In case of vanilla GCN this equation yeild to:\n$$ h^{l+1} = \\sigma(\\phi\\hat{w}^{l}(\\Lambda)\\phi^{T}h)  \\tag{6}\\label{6}$$\nNow, we will learn the $\\hat{w}$ using backpropagation.\nThis vanilla GCN has several limitations, like larger time complexity and this does not guarantee localization in the spatial domain that we get from CNN's filter.\nIn next works, such as SplineGCNs, ChebNet, Klipf and Welling's GCN, and many other works address those issues, and try to solve them.\nNote that we can think of ChebNet and Klipf and Welling's GCN as a message-passing system, but, in the background, they are computing spectral convolution and also they use some standard assumption that's why we do not need any eigenvector and we implement them in the spatial domain, but still they are spectral convolution.\nThere is also another branch in graph convolution called spatial graph convolution. I only talked about the spectral graph convolution.\n", "type": 2, "id": "25431", "date": "2020-12-28T20:58:19.020", "score": -1, "comment_count": 4, "parent_id": "24891"}}}
{"line": 19358, "body": "I am a little confused on how, you can find online papers that describe complex Machine Learning formulas in a mathematical/probabilistic way, and, in the other hands, easy tutorials that teach you how to use frameworks to create neural networks, without mentioning the maths behind.\nWhat is not clear is, what is the correlation between these two worlds? What are the \"parameters\" that make you understand i.e. how many layer to code, what kind of perceptrons to use, etc?\nto make an example:\nLet's take this formula, which in Wikipedia Italy is described as \"the standard learning algorytm\":\n\nAnd suppose that size of w and x is 4. , and g(x) and f(x) are, for examples, linear functions.\nWhat next? Where do I start coding a neural network that solves this problem?\nIt would seem more logical to me to code this \"directly\" without defining perceptrons, convolution, layers etc.\n", "type": 1, "id": "28104", "date": "2021-06-04T22:48:26.997", "score": 0, "comment_count": 1, "tags": ["neural-networks", "machine-learning", "math", "definitions"], "title": "how to go from mathematical problem to neural network (and back)?", "answer_count": 1, "views": 46, "accepted_answer": null, "answers": {"28111": {"line": 19364, "body": "Basic feed forward neural nets (MLPs) are essentially just computing sequences of matrix multiplications (with nonlinear activations in between), so this is in fact easy to code \"directly\" like you mentioned. The more difficult part is computing the gradients with respect to the parameter matrices (usually with backpropagation). However there really isn't anything that fancy behind the basic neural network model, it really is just sequences of simple blocks.\nYou certainly want at least one hidden layer usually, because without it you'll just have a generalized linear model. Neural networks are useful for creating arbitrarily nonlinear models, which can be achieved by adding more layers or more neurons per layer.\nIf you want some clarification about how to code a neural net, I recommend taking the classic Andrew Ng ML course on Coursera.\nWith regard to the amount of layers, number of neurons, etc, these are hyperparameters -- there is no known way to determine the correct values for them without experimentation.\n", "type": 2, "id": "28111", "date": "2021-06-05T14:45:34.747", "score": 0, "comment_count": 0, "parent_id": "28104"}}}
{"line": 17246, "body": "I found this question very interesting, and this is a follow up on it.\nPresumably, we'd want all the filters to converge towards some complementary set, where each filter fills as large a niche as possible (in terms of extracting useful information from the previous layer), without overlapping with another filter.\nA quick thought experiment tells me (please correct me if I'm wrong) that if two filters are identical down to maximum precision, then without adding in any other form of stochastic differentiation between them, their weights will be updated in the same way at each step of gradient descent during training. Thus, it would be a very bad idea to initialise all filters in the same way prior to training, as they would all be updated in exactly the same way (see footnote 1).\nOn the other hand, a quick thought experiment isn't enough to tell me what would happen to two filters that are almost identical, as we continue to train the network. Is there some mechanism causing them to then diverge away from one another, thereby filling their own \"complementary niches\" in the layer? My intuition tells me that there must be, otherwise using many filters just wouldn't work. But during back-propagation, each filter is downstream, and so they don't have any way of communicating with one another. At the risk of anthropomorphising the network, I might ask \"How do the two filters collude with one another to benefit the network as a whole?\"\n\nFootnotes:\n\nWhy do I think this? Because the expession for the partial derivative of the $k$th filter weights with respect to the cost $\\partial W^k/\\partial C$ will be identical for all $k$. From the perspective of back-propagation, all paths through the filters look exactly the same.\n\n", "type": 1, "id": "25464", "date": "2020-12-30T16:13:11.343", "score": 2, "comment_count": 0, "tags": ["convolutional-neural-networks", "gradient-descent"], "title": "Is there anything that ensures that convolutional filters end up different from one another?", "answer_count": 2, "views": 86, "accepted_answer": null, "answers": {"25487": {"line": 17267, "body": "Here I am just trying to simplify what @user3667125 already said uses math arguments\nSay we have a cost function $J(x, y; F(\\cdot; \\Theta))$ which regards training a NN $F(\\cdot, \\Theta)$ with $x$ input and $y$ expected output\nGradient descent tells us how to upgrade each $\\theta_{i} \\in \\Theta$ and it is\n$$ \\theta_{i}(t+1) = \\theta_{i}(t) - \\alpha \\frac{\\partial J(x,y; \\Theta)}{\\partial \\theta_{i}} $$\nwith $t$ training time\nSo let's focus on a specific component of the NN $f(x; \\theta)$ and from its perspective, we can say the computation is\n$$ h(f(g(x); \\theta), y) $$\nwith\n\n$h(x,y)$ representing all the subsequent components + loss function\n$g(x)$ representing all the previous computation\n$x$ input\n$y$ expected output\n\nso its update is\n$$ \\Delta \\theta(t) = \\frac{\\partial h(f(g(x(t), \\theta), y(t)))}{\\partial \\theta} $$\nwith\n\n$x(t)$ the concrete input at $t$ time\n$y(t)$ expected output at $t$ time\n$\\theta(t)$ the concrete value of the parameter at $t$ time\n\nApplying the chain rule we have\n$$ \\Delta \\theta(t) = h'(f(g(x(t), \\theta(t))), y(t)) f'(g(x(t)), \\theta(t)) $$\nso the gradient observed by a certain parameter $\\theta$ depends on\n\n$\\theta(t)$ the current value of the parameter\n$x(t)$ the current input and more specifically by $g(x(t))$ the processing of this input by the previous part of the NN\n$y(t)$ the expected output\n$h'(\\cdot)$ the gradient of the subsequent part of the network\n\nSo even if we have 2 weights with the same value $\\theta_{i}(t) = \\theta_{j}(t) \\quad i \\neq j$ at a certain point in training time, they can still see different gradients since\n\nthe upstream processing $g(x(t))$ can be different\nthe gradient backpropagating from the downstream processing $h'(\\cdot, y(t))$ can be different\n\n", "type": 2, "id": "25487", "date": "2020-12-31T17:45:21.243", "score": 0, "comment_count": 1, "parent_id": "25464"}, "25476": {"line": 17256, "body": "Yes, your thought experiment is correct, and the concept is known as breaking the symmetry. This is why biases can be initialized to $0$ (bias initialization doesn't matter), but weights should be randomly initialized to different numbers -- to break the symmetry. Otherwise, if not, the network will function as if it has $n-1$ filters (or however many filters that are unique) instead of the full $n$ filters.\nAs for your main question, if two filters are initialized to very similar values, they may branch out as long as that is what minimizes the training loss. There is no collusion or coordination going on; each filter updates completely independently. You can even freeze all the other filters and only perform gradient descent on one filter at a time. Each filter just follows the direction of their gradient to minimize the training loss.\nConsider the backprop equations as defined by this online book:\n\nThe gradient of the current layer's weights depends on\n\nThe future layers' weights, errors, and activation function's derivatives\nThe current layer's activation function's derivative, and\nThe previous layer's outputs.\n\nEach weight in the layer (i.e. each filter in the layer) looks at different parts of these three components (indexed by $j$ and $k$ in equation $BP4$). It is this different perspective that allows them to update their gradients in different directions, even if their initial weights are very similar to each other. Note that it is possible that they end up with the same gradient, but it is very unlikely.\n", "type": 2, "id": "25476", "date": "2020-12-31T07:56:30.857", "score": 1, "comment_count": 2, "parent_id": "25464"}}}
{"line": 15870, "body": "I know that if you use an ReLU activation function at a node in the neural network, the output of that node will be non-negative. I am wondering if it is possible to have a negative output in the final layer, provided that you do not use any activation functions in the final layer, and all the activation functions in the previous hidden layers are ReLU?\n", "type": 1, "id": "23667", "date": "2020-09-20T14:18:54.747", "score": 1, "comment_count": 0, "tags": ["neural-networks", "relu"], "title": "Is it possible to have a negative output using only ReLU activation functions, but not in the final layer?", "answer_count": 2, "views": 456, "accepted_answer": null, "answers": {"23688": {"line": 15885, "body": "I guess you are using NN for Regresions.\nIn the most common aplication a scale of the outputs is implemented.\nThis is recommended. Specialy if you have more than one output with diferent scales.\nOtherwise, you will remunerate the neural network for correcting the error of one variable over the other.\nIf you still want to avoid a scale of the outputs. Yes. You can use the identity function in the output layer or a linear function (tha same with different slope).\nThe weights and bias of some conections will become negative and the hidden neurons are going to work as always.\n", "type": 2, "id": "23688", "date": "2020-09-21T18:19:51.063", "score": 0, "comment_count": 0, "parent_id": "23667"}, "23668": {"line": 15871, "body": "Yes, if there's no activation function in the last layer, the weights could simply be negative there, so the network would multiply a positive value with a negative weight, therefore outputting a negative value.\nThere is still an activation function, but it is the identity.\n", "type": 2, "id": "23668", "date": "2020-09-20T14:45:18.817", "score": 2, "comment_count": 0, "parent_id": "23667"}}}
{"line": 16351, "body": "(I have a very primitive understanding of neural networks, so please forgive the lack of technicality here.)\nI am used to seeing a neuron in a neural network as something that-\n\nTakes the inputs and multiplies them by their weights,\nthen sums them up,\nand after that it applies the activation function to the sum.\n\nNow, what if it was \"smarter\"? Say, a single neuron could do the function of an entire layer in a network, could that make the network more effective? This comes from an article I was reading at Quanta, where the author says:\n\nLater, Mel and several colleagues looked more closely at how the cell might be managing multiple inputs within its individual dendrites. What they found surprised them: The dendrites generated local spikes, had their own nonlinear input-output curves and had their own activation thresholds, distinct from those of the neuron as a whole. The dendrites themselves could act as AND gates, or as a host of other computing devices.\n\n\n...realised that this meant that they could conceive of a single neuron as a two-layer network. The dendrites would serve as nonlinear computing subunits, collecting inputs and spitting out intermediate outputs. Those signals would then get combined in the cell body, which would determine how the neuron as a whole would respond.\n\nMy thoughts: I know that Backpropagation is used to \"teach\" the network in the normal case, and the fact that neurons are simply activation buttons is somehow related to that. So, if neurons were to be more complicated, it would reduce efficiency. However, I am not sure of this: why would complex individual components make the network less effective?\n", "type": 1, "id": "24358", "date": "2020-11-01T04:52:06.063", "score": 1, "comment_count": 0, "tags": ["neural-networks", "artificial-neuron", "hidden-layers"], "title": "If neurons performed the operation of an entire layer, would that make the neural network more effective?", "answer_count": 1, "views": 48, "accepted_answer": "24359", "answers": {"24359": {"line": 16352, "body": "\nSay, a single neuron could do the function of an entire layer in a network, could that make the network more effective?\n\nThat depends what you mean by \"more effective\". In terms of number of neurons to achieve the same result, then you should need fewer units. In terms of being able to calculate an end result for any specific problem, then no, because you can generally solve a problem using simpler units by adding more of them.\nIf this could somehow be done using less resources, it might reduce overall costs. In a biological system, there are possibly overheads per cell in order to maintain it on top of the costs for calculation, so it may be better to do more than the simplest calculation in each cell. Further to that, there may be an optimal amount of processing that each cell could do (this is all conjecture on my part).\nIn an artifical neural network, the calculations are the only thing being considered, there is no separate overhead per neuron.\nThere are neural network architectures with complex \"sub-units\". Probably the most well known are the recurrent neural network designs for LSTM and gated recurrent units (GRU), plus \"skip connections\" in residual neural networks. For efficiency these are normally processed in groups per layer with matrix processing functions, but you can also view them as per-neuron complexities.\n\nI am not sure of this- why would complex individual components make the network less effective?\n\nIf the complexity was not used, or not really needed, in some of the units, then it would be wasted capacity. In a biological system, this might correspond to maintaining cells larger than they needed to be. In an artificial system, it would mean using memory and CPU to calculate interim values that were not needed for the task at hand.\n", "type": 2, "id": "24359", "date": "2020-11-01T10:16:06.030", "score": 0, "comment_count": 0, "parent_id": "24358"}}}
{"line": 16098, "body": "I know that my question probably seems like being asked many times, but Ill try\nto be more speciffic:\nLimitations to my question:\n\nI am NOT asking about convolutional neural networks, so please, try not to mention this as an example or as an answer as long as it is possible. (maybe only in question number 3)\n\nMy question is NOT about classification using neural networks\n\nI am asking about a \"simple\" neural network designed to solve the regression type of problem. Let's say it has 2 inputs and 1 output.\n\n\nPreambula:\nAs far as I understood, from the universal approximation theorem, in such a case, even if the model is nonlinear, only one hidden layer can perfectly fit a nonlinear model, as shown here\nhttp://neuralnetworksanddeeplearning.com/chap4.html.\nQuestion 1\nIn this specific case, is there any added value in using extra layers?\n(maybe the model will be more precise, or faster training?)\nQuestion 2\nSuppose in 1st question the answer was there is no added value. In such a case will the added value appear if I enlarge inputs from two inputs as described above, to some larger number?\nQuestion 3\nSuppose in 2nd question the answer was there is no added value. I am still trying to pinpoint the situation where it STARTS making sense in adding more layers AND where it makes NO sense at all using one layer.\n", "type": 1, "id": "23980", "date": "2020-10-08T20:12:37.457", "score": 2, "comment_count": 1, "tags": ["neural-networks", "regression", "hidden-layers", "non-linear-regression", "universal-approximation-theorems"], "title": "When are multiple hidden layers necessary?", "answer_count": 1, "views": 104, "accepted_answer": "23981", "answers": {"23981": {"line": 16099, "body": "A very wide but shallow neural network is going to be harder to train.\nYou can check that with the playground of tensorflow or with the MPG example in Google Colab.\nThe relationship between architecture and learning capabilities is not fully understood, but, empirically, thats what you see.\nBut making the network too deep creates more problems:\n\nVanishing gradients.\nLess parameters for the same amount of neurons.\nExploding gradients.\n\nIt is for that reason that humans and neural networks are the one deciding a good architecture.\n", "type": 2, "id": "23981", "date": "2020-10-08T21:42:09.780", "score": 0, "comment_count": 1, "parent_id": "23980"}}}
{"line": 18561, "body": "I stumbled upon this passage when reading this guide.\n\nUniversality theorems are a commonplace in computer science, so much\nso that we sometimes forget how astonishing they are. But it's worth\nreminding ourselves: the ability to compute an arbitrary function is\ntruly remarkable. Almost any process you can imagine can be thought of\nas function computation.* Consider the problem of naming a piece of\nmusic based on a short sample of the piece. That can be thought of as\ncomputing a function. Or consider the problem of translating a Chinese\ntext into English. Again, that can be thought of as computing a\nfunction.  Or consider the problem of taking an mp4 movie file and\ngenerating a description of the plot of the movie, and a discussion of\nthe quality of the acting. Again, that can be thought of as a kind of\nfunction computation.* Universality means that, in principle, neural\nnetworks can do all these things and many more.\n\nHow is this true? How can any process be thought of as function computation? How would one compute function in order to translate Chinese text to English?\n", "type": 1, "id": "27105", "date": "2021-03-31T19:51:19.707", "score": 2, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "deep-learning", "function-approximation", "universal-approximation-theorems"], "title": "How can \"any process you can imagine\" be thought of as function computation?", "answer_count": 2, "views": 36, "accepted_answer": null, "answers": {"27108": {"line": 18564, "body": "To speak to your question about how Chinese to English translation can be a computation, it first requires a way to turn the base units of translation (tokens) into something computable. One basic way is to define the set of your vocabulary terms and create a gigantic matrix (typically called an embedding) with each column representing a token as well as one-hot encoded matrices to perform a selection from the matrix.\nSay, if my vocabulary is (\"apple\", \"kiwi\") and I want 2-dimensional vectors for the tokens (trivially small but manageable example), you'd need a 2x2 (two tokens in two dimensions) random matrix and two one-hot vectors:\n\n\"apple\" = [1 0]\n\"kiwi\" = [0 1]\n\nMultiplying [1 0] for instance, by your 2x2 matrix, will \"select\" the vector in the first column, which represents the token \"apple\".\nOnce you have a randomly initialized embedding matrix, you have to train it to be useful. A relatively common method to do so is to make it the hidden layer in a neural network, mask tokens in the source data, and train the model with gradient descent to \"guess\" what token was removed. You can also just fully train the embedding as part of training a larger network, but that increases training time significantly.\nIf you have a lot of sentence pairs of English-Chinese translations, you could train an English/Chinese joint embedding and then further train a neural network that uses it to translate between sentence pairs (but this is not a state of the art method).\nEvery step of the process after text preparation here is a mathematical operation, so a forward pass of the trained model can be expressed as an equation (though one you could never fully write out by hand), and if we're careful to only choose differentiable operations, then the training of the model as well comes down to solving (many, massive) equations.\n", "type": 2, "id": "27108", "date": "2021-03-31T20:36:18.093", "score": 0, "comment_count": 0, "parent_id": "27105"}, "27106": {"line": 18562, "body": "A function is simply a procedure that maps a particular input to a particular output. You put in $X$, and the function computes $Y$. Those $X$ and $Y$ can take many different forms. It could be mapping one number to another number (convert miles to kilometres), mapping sound to text (name that tune), mapping text to text (translate languages), mapping a video to text (review this movie), or mapping text to an image (draw a picture of $X$). Anytime you have a procedure that produces a fixed output based on a fixed input, it's a function.\nUniversality theorems guarantee that a neural network can produce an arbitrarily good approximation of any possible function. That doesn't mean it's easy, though - finding the right function that maps $X$ to $Y$ is the hard part.\n", "type": 2, "id": "27106", "date": "2021-03-31T20:06:28.233", "score": 2, "comment_count": 0, "parent_id": "27105"}}}
{"line": 20728, "body": "Suppose that I have a 1D dataset with 6 features.\nCan I apply a 2D convolutional neural net to this dataset?\n", "type": 1, "id": "31723", "date": "2021-09-17T19:34:20.060", "score": 0, "comment_count": 0, "tags": ["convolutional-neural-networks", "convolution"], "title": "Is it possible to apply 2D convolution to 1D data?", "answer_count": 2, "views": 39, "accepted_answer": "31725", "answers": {"31725": {"line": 20730, "body": "This depends on the engine you use, but in general, yes, of course.\nFor example, in the TensorFlow height and width are separate variables, so nothing in your way to set one of them to 1 to have 1D data in it.\n", "type": 2, "id": "31725", "date": "2021-09-17T22:26:50.417", "score": 1, "comment_count": 0, "parent_id": "31723"}, "31727": {"line": 20732, "body": "You can certainly reshape the data to make it fit a 2D network. You could set the width or height to 1 as suggested by DamirTenishev, but you could also set the features/channels to 1 and treat the features as a height, if you wanted to convolve that way (would be a bit strange).\n", "type": 2, "id": "31727", "date": "2021-09-18T07:16:07.453", "score": 0, "comment_count": 0, "parent_id": "31723"}}}
{"line": 17158, "body": "I have a dataset consisting of a set of samples. Each sample consists of two distinct desctized signals S1(t), S2(t). Both signals are synchronous; however, they show different aspects of a phenomena.\nI want to train a Convolutional Neural Network, but I don't know which architecture is appropriate for this kind of data. \nI can consider two channels for input, each corresponding to one of the signals. But, I don't think convolving two signals can produce appropriate features.\nI believe the best way is to process each signal separately in the first layers, then join them in the classification layers in the final step. How can I achieve this? What architecture should I use?\n", "type": 1, "id": "25349", "date": "2020-12-23T13:11:29.020", "score": 2, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks"], "title": "Appropriate convolutional neural network architecture when the input consists of two distinct signals", "answer_count": 2, "views": 35, "accepted_answer": null, "answers": {"25350": {"line": 17159, "body": "I don't know what you mean by desctized signals but if I understand your question correctly, separating two signal and passing them through same architecture of CNN (even with different parameters) is not a good idea. Because when they are together (as different channels) they will be treated differently by the CNN (each channel has its own parameters) and even this way the network is able to combine these two signals and get better results by information extracted from this combination.\n", "type": 2, "id": "25350", "date": "2020-12-23T14:07:30.560", "score": 1, "comment_count": 1, "parent_id": "25349"}, "25353": {"line": 17162, "body": "You can safely give both signals as input in different channels. Actually, it's the best way. This way, the network is able to find low-degree patterns that involve both signals early in training. This will therefore enable the early discovery of more complex patterns, too.\nDifferently from what one might understand from your question, the two signals will not be convolved against eachother, as it's typically done in signal processing. The convolution taking place is that of the first layer kernels with a two-component signal (the one you give as input). It can happen that there's a first order pattern that can only be recognised by looking at both signals at the same time. If that's not the case, the kernels will ignore one signal or the other (having the corresponding weights a value of zero).\n", "type": 2, "id": "25353", "date": "2020-12-23T16:09:49.110", "score": 0, "comment_count": 2, "parent_id": "25349"}}}
{"line": 18359, "body": "So I am new to NN and I'm trying to go deep and apply it to my subject. I would like to ask: the input of the NN can be 2 or more values for example-> the measurement of a value, distance, and time? An example of input data would be [ [1,2,3, ....],[11,22,33, .....],[5] ] whose output is a value 1 for example or cat or an generated model.\n", "type": 1, "id": "26843", "date": "2021-03-16T11:22:37.430", "score": 1, "comment_count": 1, "tags": ["neural-networks", "tensorflow"], "title": "Can we use Multiple data as Input in a NN for a single Output?", "answer_count": 1, "views": 61, "accepted_answer": "26863", "answers": {"26863": {"line": 18373, "body": "Yes, it can be those multiple inputs, you would have to design the network like that (to have 3 input layers and then pool them afterwards).\nBut, an easier solution would be to flatten this array and put it through 1D CNN and finally put it through a softmaxx.\n", "type": 2, "id": "26863", "date": "2021-03-17T04:25:34.710", "score": 0, "comment_count": 11, "parent_id": "26843"}}}
{"line": 17673, "body": "There are five parameters from an LSTM layer for regularization if I am correct.\nTo deal with overfitting, I would start with\n\nreducing the layers\nreducing the hidden units\nApplying dropout or regularizers.\n\nThere are kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, dropout and recurrent_dropout.\nThey have their definitions on the Keras's website, but can anyone share more experiences on how to reduce overfitting?\nAnd how are these five parameters used? For example, which parameters are most frequently used and what kind of value should be input? ?\n", "type": 1, "id": "25963", "date": "2021-01-25T05:50:25.033", "score": 5, "comment_count": 3, "tags": ["recurrent-neural-networks", "long-short-term-memory", "overfitting", "regularization", "dropout"], "title": "How should we regularize an LSTM model?", "answer_count": 3, "views": 1575, "accepted_answer": null, "answers": {"26208": {"line": 17872, "body": "One LSTM layer should be enough unless you have lots of data. The same thing goes for the number of nodes in the layer. Start small first so 5 to 10 nodes and increment it until the performance is reasonable.\nOnce you have a model working you can apply regularization if you think it will improve performance by reducing overfitting of the training data. You can check this by looking at the learning curves or compring the error on the validation and test sets.\nIn my experiments I've used the L1 and L2 regularizers along with dropout. These can all be mixed together in fact using both L1 and L2 at the same time is called the ElasticNet.\nI tend to apply the regularizers on the kernel_regularizer because this affects the weights for the inputs. Basically feature selection.\nThe value for the L1 and L2 can start with the default (for tensorflow) of 0.01 and change it as you see fit or read what other research papers have done.\nDropout can start at 0.1 then increment it until there is no performance gain. This is basically a percentage so 0.1 would remove about 10% of your nodes.\nFinding the best regularizer is the same as any other hyperparameter optimization which is mostly trial and error.\n", "type": 2, "id": "26208", "date": "2021-02-04T20:39:24.043", "score": 1, "comment_count": 0, "parent_id": "25963"}, "26017": {"line": 17722, "body": "Regularization is trying to discourage complex information from being learned so we want to eliminate the model from actually learning to memorize the training data. We don't want to learn like very specific pinpoints of the training data that don't generalize well to test data.\nDropout, the idea of drop out is that during training we randomly set some of the activations of the hidden neurons to zero with some probability say 0.5. This idea is extremely powerful because it allows the network to lower its capacity, it also makes it such that the network can't build these memorization channels through the network where it tries to just remember the data because on every iteration 50% of that data is going to be wiped out so it's going to be forced to not only generalize better but it's going to be forced to have multiple channels through the network and build a more robust representation of its prediction. We repeat this on every iteration so on the first iteration we dropped out one 50% of the nodes on the next iteration we can drop out a different randomly sampled 50% which may include some of the previously sampled nodes as well and this will allow the network to generalize better to new test data.\nEarly stopping, when the network is improving its performance during training there comes a point where the training data starts to diverge from the testing data, at some point the network is going to start to do better on its training data than its testing data, what this means is basically that the network is starting to memorize some of the training data and that's what you don't want so what we can do is we can identify this inflection point where the test data starts to increase and diverge from the training data so we can stop the network early and make sure that our test accuracy is as minimum as possible.\n", "type": 2, "id": "26017", "date": "2021-01-27T14:22:00.073", "score": -1, "comment_count": 1, "parent_id": "25963"}, "31661": {"line": 20673, "body": "If what is mentioned above, that is probably in the context of lstm networks. I would suggest using the keras tuner bayesian optimizer and making the l1 or l2 number a parameter of the kernel space. This way you find the optimal values, and its a great way to hypertune. Just keep in mind, the greater the range of parameters, or kernel if i am not wrong, the higher computer power you need.\nfrom tensorflow import keras\nimport keras_tuner as kt\n\ndef model1(hp):\n  model=Sequential()\n  model.add(keras.layers.LSTM(units=hp.Int('units',min_value=40, max_value=800, step=20),\n                              dropout=hp.Float('droput',min_value=0.15, max_value=0.99, step=0.05),\n                              recurrent_dropout=hp.Float('redroput',min_value=0.05, max_value=0.99, step=0.05),\n                              activation='relu',\n                              return_sequences=True,\n                              input_shape=(30,1)))\n  Attention()\n  model.add(keras.layers.LSTM(units=hp.Int('units',min_value=40, max_value=800, step=20),\n                              dropout=hp.Float('droput',min_value=0.15, max_value=0.99, step=0.05),\n                              activation='relu',return_sequences=True))\n  Attention()\n  model.add(keras.layers.LSTM(units=hp.Int('units',min_value=40, max_value=800, step=20), activation='relu'))\n  model.add(keras.layers.Dense(1))\n  \n  model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-7, 1e-10])))\n  return model\n\nbayesian_opt_tuner = kt.BayesianOptimization(\n    model1,\n    objective='val_loss',\n    max_trials=200,\n    executions_per_trial=1,\n    project_name='timeseries_bayes_opt_POC',\n    overwrite=True,)\n\nxval=X_test\nbayesian_opt_tuner.search(x=X_train ,y=X_train, \n             epochs=300,\n             #validation_data=(xval ,xval),\n             validation_split=0.95,\n             validation_steps=30,  \n             steps_per_epoch=30,\n             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                              patience=4,\n                              verbose=1,\n                              restore_best_weights=True),\n                        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                   factor=0.1, \n                                   patience= 2, \n                                   verbose=1, \n                                   min_delta=1e-5, \n                                   mode='min')]\n             )\nThis is where the magic happens. Something I composed myself. If interested holla \n\n", "type": 2, "id": "31661", "date": "2021-09-11T23:53:55.297", "score": 1, "comment_count": 0, "parent_id": "25963"}}}
{"line": 19127, "body": "I'm interested in artificial neural networks (ANN) and I wonder how big ANNs in practical use are, for example, Tesla Autopilot, Google Translate, and others.\nThe only thing I found about Tesla is this one:\n\n\"A full build of Autopilot neural networks involves 48 networks that\ntake 70,000 GPU hours to train. Together, they output 1,000 distinct\ntensors (predictions) at each timestep.\"\n\nIt seems like most companies don't publish clear information about their ANN sizes. I really can't find anything detailed on this subject.\nIs there any information about the size of big practical/commercial ANNs that include something like the amount of neurons/connections/layers etc.?\nI'm looking for a few examples in this scale with more precise information on the size of the neural networks.\n", "type": 1, "id": "27827", "date": "2021-05-16T15:01:55.057", "score": 5, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "reference-request", "autonomous-vehicles"], "title": "What are the typical sizes of practical/commercial artificial neural networks?", "answer_count": 4, "views": 222, "accepted_answer": "27832", "answers": {"27852": {"line": 19147, "body": "The size of the model depends on the domain. I am currently working with a model that is used for real time inference on an embedded device. Speed of computation is critical.\nThe model size is a 5 layer CNN, about 700k parameters and it's about 12MB in size on disk.\n", "type": 2, "id": "27852", "date": "2021-05-18T14:22:25.857", "score": 0, "comment_count": 0, "parent_id": "27827"}, "28641": {"line": 19823, "body": "Use the benchmarked algorithms or research papers will be a good start. Addition to that use the open sourced Bert GPT 2 like architectures is a good start.\n", "type": 2, "id": "28641", "date": "2021-07-13T03:03:09.083", "score": 0, "comment_count": 0, "parent_id": "27827"}, "27842": {"line": 19138, "body": "I hope this helps. Disclaimer: the info is extracted from Computer Vision at Tesla, though aditional references may be needed....\n\nTesla is using 8 cameras per vehicle fused with radars.\nTo detect road lanes, vehicles, pedestrians etc., Tesla have to run\nat least 50 neural networks simultaneously.\nThe neural networks are trained using PyTorch. Each image (for each\ncamera I guess) is of dimension (1280, 960, 3). RGB image it seems.\nRegarding the neural network architecture, \"the backbone is a modified ResNet 50\", that is, a convolutional network.\nThe network is 50 layers deep. The network depth is defined as the largest number of sequential convolutional or fully connected layers on a path from the input layer to the output layer. In total, ResNet-50 has 177 layers. The ResNet-50 has over 23 million trainable parameters\nEvery camera is processed through a single neural network. Then everything is combined into a middle neural network.\n\n", "type": 2, "id": "27842", "date": "2021-05-17T09:22:26.647", "score": 1, "comment_count": 1, "parent_id": "27827"}, "27832": {"line": 19131, "body": "NLP Domain\nYou can easily find such open-source neural networks in NLP applications that have been published by Companies like Google. For example, in BERT models, you can see the BERT-Base has the following specifications:\n\nBERT-Base, Multilingual Cased: 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n\nYou can find more data about other versions of BERT in the same link.\nAnother example is GPT models, like GPT-3:\n\nAll GPT-3 models use the same attention-based architecture as their GPT-2 predecessor. The smallest GPT-3 model (125M) has 12 attention layers, each with 12x 64-dimension heads. The largest GPT-3 model (175B) uses 96 attention layers, each with 96x 128-dimension heads.\n\nImage Procssing Domain\nAnother useful domain for your expectation is image processing tasks such as image classification. Pretrained models such as VGG, ResNet, and Inception. These are mostly used for image classification tasks in different companies, and you can find their specification many where. For example for VGG-16, we can see the followings:\n\nSpeech Processing\nAnother practical domain is Auto-Speech Recognition or ASR in short. One of the renowned models in this context is DeepSpeech(2) by Baido Research center. For example, you can find some info like the number of its parameters and its structure in this github link.\nSumming up\nNote that one regular metric to measure the size of neural networks is \"the number of parameters\" of the network that is required to be learned in the training phase. Hence, you can compare the size of models even between cross domains by knowing their number of parameters (instead of going to more details about the number of hidden layers and their types). Although, sometimes the length (number of layers) and height (number of neurons in each layer) of the network are very important in the matter of performance and capability of the network.\n", "type": 2, "id": "27832", "date": "2021-05-16T16:18:03.703", "score": 2, "comment_count": 0, "parent_id": "27827"}}}
{"line": 18683, "body": "Adding BatchNorm layers improves training time and makes the whole deep model more stable.  That's an experimental fact that is widely used in machine learning practice.\nMy question is - why does it work?\nThe original (2015) paper motivated the introduction of the layers by stating that these layers help fixing \"internal covariate shift\". The rough idea is that large shifts in the distributions of inputs of inner layers makes training less stable, leading to a decrease in the learning rate and slowing down of the training.  Batch normalization mitigates this problem by standardizing the inputs of inner layers.\nThis explanation was harshly criticized by the next (2018) paper -- quoting the abstract:\n\n... distributional stability of layer inputs has little to do with the success of BatchNorm\n\nThey demonstrate that BatchNorm only slightly affects the inner layer inputs distributions.  More than that -- they tried to inject some non-zero mean/variance noise into the distributions. And they still got almost the same performance.\nTheir conclusion was that the real reason BatchNorm works was that...\n\nInstead BatchNorm makes the optimization landscape significantly smoother.\n\nWhich, to my taste, is slightly tautological to saying that it improves stability.\nI've found two more papers trying to tackle the question: In this paper the \"key benefit\" is claimed to be the fact that Batch Normalization biases residual blocks towards the identity function. And in this paper that it \"avoids rank collapse\".\nSo, is there any bottom line? Why does BatchNorm work?\n", "type": 1, "id": "27260", "date": "2021-04-10T23:05:51.463", "score": 14, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "papers", "hidden-layers", "batch-normalization"], "title": "Why does Batch Normalization work?", "answer_count": 2, "views": 300, "accepted_answer": null, "answers": {"27374": {"line": 18780, "body": "To some extend, it get rid of low intensity numerical noise. Condition properties of the optimization problem is always an issue, i suspect BatchNorm alleviate this instability.\n", "type": 2, "id": "27374", "date": "2021-04-16T18:16:40.913", "score": -2, "comment_count": 2, "parent_id": "27260"}, "28444": {"line": 19649, "body": "It is a question with no simple answer.\nOn one hand the BatchNormalization is unloved by some arguing it doesn't change the accuracy of neural networks or biased them.\nOn the other hand, it is highly recommended by the other because it leads to better trained models with a larger scope of predictions and less chances of overflow.\nAll I know for sure is that BN is really efficient on image classification. In fact, like the image categorization and classification soar this last years and that BN is a good practice in this field, it has spread to almost all DNNs.\nNot only is the BN not always used in the right purpose, but it is often used without taking into account several elements such as :\n\nThe layers between which apply BN\nThe initializer algorithms\nThe activation algorithms\netc\n\nFor more computer sciences litterature \"against\" BN, I will let you look at the H. Zhang et al paper who has trained a DNN without BN and get good results.\nSome people use Gradient Clipping technique (R. Pascanu) instead of the BN in particular for RNNs\nI hope it will give you some answers !\n", "type": 2, "id": "28444", "date": "2021-06-28T14:00:46.430", "score": -2, "comment_count": 3, "parent_id": "27260"}}}
{"line": 19354, "body": "I am relatively new to Python but I taught myself enough to code a two-player board game that is similar to chess. It has a simple Tkinter UI. Now I am dipping into machine learning, and I want to write another program to play itself in this game repeatedly and \"naturally\" learn strategies for playing the game.\nCan anyone give advice on what I might be able to use for this? Is Tensorflow a good option? Is there a Python library well suited for this that I could adapt and train? I am partially through the buildingai.elementsofai.com course, but I am still very new at ML / AI.\n", "type": 1, "id": "28100", "date": "2021-06-04T20:19:37.417", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "reinforcement-learning", "python"], "title": "Best way to use/learn ML for board-game reinforcement learning", "answer_count": 2, "views": 48, "accepted_answer": "28233", "answers": {"28233": {"line": 19469, "body": "If it's a 2-player game it goes a little deeper into RL if you want both sides to be RL algorithms. I recommend reading about game theory and what is a Nash Equilibrium to start. For algorithms hiivemdptoolbox has openai-gym compatibility as well as Q-Learning. You will need to add code to make 2 learners play each other. I would also recommend adding Dyna-Q to the Q-Learner as it will probably speed up the learning.\n", "type": 2, "id": "28233", "date": "2021-06-13T18:36:40.053", "score": 0, "comment_count": 1, "parent_id": "28100"}, "28102": {"line": 19356, "body": "There are many approaches - the initial one would be a rule based one with some amount of randomness. The ML-AI approach is some variation of reinforcement learning, defining your game as an environment, see for instance openai-gym. \"Some variation\" might be Deep Q Learning or A3C.\n", "type": 2, "id": "28102", "date": "2021-06-04T20:53:21.153", "score": 0, "comment_count": 0, "parent_id": "28100"}}}
{"line": 18821, "body": "I'm trying to implement a neural network that can capture the drift in a measured angle as a way of dynamic calibration. i.e, I have a reference system that may change throughout the course of the data gathering and would like to train a network layer which actually converts the drifting reference to the desired reference by updating the angle parameter.\nFor example: Consider the 2d case. We would have a set of 2d points $X\\in \\mathbb{R}^2$ and a trainable parameter called $\\theta$ in the layer. The output of the layer would then be:\n$$X_o = XR$$ where\n$$R = \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta) \\\\ \n\\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix}$$\nUsing Adam optimizer I then try to find the $\\theta$ which transforms a given angle to the desired reference.\nHowever, the $\\theta$ value seems to fluctuate around the initial value probably because of a diverging gradient(?). How can I overcome this issue?\nThe code is below.\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass Rotation2D(tf.keras.layers.Layer):\n  def __init__(self):\n    super(Rotation2D, self).__init__()\n\n  def build(self, input_shape):\n    self.kernel = self.add_weight(\"kernel\", initializer=tf.keras.initializers.Constant(90),\n                                  shape=[1, 1])\n\n  def call(self, input):\n    matrix = ([[tf.cos(self.kernel[0, 0]), -tf.sin(self.kernel[0, 0])],\n              [tf.sin(self.kernel[0, 0]), tf.cos(self.kernel[0, 0])]])\n    return tf.matmul(input, tf.transpose(matrix))\n\nlayer = Rotation2D()\n\nt = np.arange(0, 1000)/200.\n\ny_in = np.array([np.sin(t), np.cos(t)]).T\ny_ta = np.array([np.cos(t), np.sin(t)]).T\n\nmodel = tf.keras.Sequential()\nmodel.add(layer)\n\nmodel.compile(tf.keras.optimizers.SGD(lr=1.), loss='MSE')\nmodel.fit(y_in, y_ta, epochs=1)\nfor i in range(100):\n  print(layer.get_weights())\n  model.fit(y_in, y_ta,verbose=0, batch_size=5)\ny_out = (model.predict(y_in))\n\nfig, axes = plt.subplots(2, 1)\n\nfor i in range(2):\n  ax = axes[i]\n\n  ax.plot(y_in.T[i], label = 'input')\n  ax.plot(y_ta.T[i], label = 'target')\n  ax.plot(y_out.T[i], label = 'prediction')\n\nplt.legend()\n\nplt.show()```\n\n", "type": 1, "id": "27428", "date": "2021-04-20T07:51:41.620", "score": 1, "comment_count": 0, "tags": ["deep-neural-networks"], "title": "Unable to 'learn' a rotational angle by parametrising the angle as a neural network layer", "answer_count": 1, "views": 63, "accepted_answer": "27435", "answers": {"27435": {"line": 18827, "body": "There are two basic problems with your code:\n\nThe functions sin(t) and cos(t) (both in numpy and tensorflow) take radians as inputs. Seeing Constant(90) in your code, and the learning rate of 1. I'm guessing that you assume that it is in degrees - that's incorrect.\n\nIn your training data y_ta is not a rotation of y_in:\ny_in = np.array([np.sin(t), np.cos(t)]).T\ny_ta = np.array([np.cos(t), np.sin(t)]).T\n\n\n\nIt is a reflection about y=x diagonal. No wonder it fails to find an appropriate rotation.\nI just had to change y_ta to:\n  y_ta = np.array([-np.cos(t), np.sin(t)]).T\n\nAnd train with more sensible learning rate:\n  model.compile(tf.keras.optimizers.SGD(lr=0.1), loss='MSE')\n  model.fit(y_in, y_ta, epochs=10)\n\nTo get the angle (which I then convert to degrees):\n  (180 * model.layers[0].kernel[0,0].numpy() / np.pi) % 360\n  > 90.00187079311581\n\n", "type": 2, "id": "27435", "date": "2021-04-20T14:46:17.760", "score": 0, "comment_count": 1, "parent_id": "27428"}}}
{"line": 20736, "body": "In the image below taken from a Youtube video, the author explains that the neural network can be used to fit a relational graph for a set of data points shown by the green line. And that this is accomplished by using weights, biases and activation functions.\nMy slight confusion is that, initially, the weights and biases and randomized, and they are re-adjusted by backpropagation. This means that, at the end of the output layer, we must have the actual values of the target function anyway.\nSo what problem does the neural network really solve?\nSo, for example, we want to find the target function for dosage and efficacy, we are given the data points shown in blue. If we initially choose randomized values for the weights, biases and activation function, then, at the output layer, we determine an output value for efficacy, but there is no way to know whether this value is in fact correct or not. So, we need the actual values to determine the difference.\nWhat about when we choose a value of dosage which has not been observed, for example, 0.25? Doesn't this rely upon a best-fit relation graph that has already been fitted to the data prior to adjusting the neural network?\n\n", "type": 1, "id": "31731", "date": "2021-09-18T12:46:45.283", "score": 3, "comment_count": 2, "tags": ["neural-networks", "training", "supervised-learning", "labeled-datasets"], "title": "What problem does the neural network really solve?", "answer_count": 1, "views": 66, "accepted_answer": "31732", "answers": {"31732": {"line": 20737, "body": "\nThis means that, at the end of the output layer, we must have the actual values of the target function anyway.\n\nYes, this is necessary for supervised learning. You will often see this called a labelled dataset, where the \"label\" is an output value that you know is associated with each input. A set of labels associated with some inputs, that you have collected for training may also the called the \"ground truth\".\nWe do not need all possible values though, but enough examples that the neural network can interpolate between them. How many examples that is depends on the complexity of the function we want to learn.\n\nSo what problem does the neural network really solve?\n\nThere are three main things it solves, and these are shared with most other machine learning approaches:\n\nThe neural network learns a function from examples of input and output.\n\nThe neural network will learn an expected value (or for classifiers, a probability distribution) when trained using noisy or stochastic data.\n\nThe neural network makes few assumptions about the relationship between input and output, and can learn successfully even for quite complex relationships.\n\n\nThese traits are all useful when you do not have a strong sense of what the correct function should be, in terms of writing an equation, but do have many examples of it.\n\nWhat about when we choose a value of dosage which has not been observed, for example, 0.25?\n\nThe neural network will still produce an output. In a very simple scenario, where you had trained with example inputs at e.g. 0.2 and 0.3, then the output will likely be somewhere between the outputs for those two values. For a neural network, this in-between value can be much more sophisticated than a simple mean of the nearest examples. ML that uses the nearest values exists, that is called k-nearest neighbours.\nIf this process has worked well, and the trained neural network produces useful, accurate predictions from unseen inputs, it is said to generalise well. Very often, that is the goal for training a neural network or other machine learning system.\nIt is worth mentioning some additional facts and features of training neural networks (and ML in general):\n\nWhen generalisation is the goal (and it often is), then you need to test for it. This is done by keeping some example data back, not using it to train, but instead using it to check results on unseen inputs. In fact, this is so important, the data is often split into three sets - a training set, a cross-validation (aka development) set, and a test set.\n\nThe lack of assumptions in the basic model can lead to needing many training examples. If you know something specific or useful about the function being learned, you can pre-process the inputs to help - this is called feature engineering.\n\nMachine learning can be good at interpolation, i.e. calculating outputs for inputs that are not in the original training data, when the unseen inputs are in-between or close to the training examples. Even when good at interpolation, it will still be bad at extrapolating to new unseen inputs that are outside of the ranges of the training examples. That is because it has used a very general/flexible system to fit some line or curve to examples, it has not learned an analytical function.\n\n\nExceptions to all of these points exist. They are the norm, but it will depend on the details of what you are trying to do.\n", "type": 2, "id": "31732", "date": "2021-09-18T14:52:00.837", "score": 0, "comment_count": 3, "parent_id": "31731"}}}
{"line": 17552, "body": "The validation accuracy of my 1D CNN is stuck on 0.5 and that's because I'm always getting the same prediction out of a balanced data set. At the same time my training accuracy keeps increasing and the loss decreasing as intended.\nStrangely, if I do model.evaluate() on my training set (that has close to 1 accuracy in the last epoch), the accuracy will also be 0.5. How can the accuracy here differ so much from the training accuracy of the last epoch? I've also tried with a batch size of 1 for both training and evaluating and the problem persists.\nWell, I've been searching for different solutions for quite some time but still no luck. Possible problems I've already looked into:\n\nMy data set is properly balanced and shuffled;\nMy labels are correct;\nTried adding fully connected layers;\nTried adding/removing dropout from the fully connected layers;\nTried the same architecture, but with the last layer with 1 neuron and sigmoid activation;\nTried changing the learning rates (went down to 0.0001 but still the same problem).\n\n\nHere's my code:\nimport pathlib\nimport numpy as np\nimport ipynb.fs.defs.preprocessDataset as preprocessDataset\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, Dropout, Dense\nfrom tensorflow.keras.optimizers import SGD\n\nmain_folder = pathlib.Path.cwd().parent\ndatasetsFolder=f'{main_folder}\\\\datasets'\ntrainDataset = preprocessDataset.loadDataset('DatasetTime_Sg12p5_Ov75_Train',datasetsFolder)\ntestDataset = preprocessDataset.loadDataset('DatasetTime_Sg12p5_Ov75_Test',datasetsFolder)\n\nX_train,Y_train,Names_train=trainDataset[0],trainDataset[1],trainDataset[2]\nX_test,Y_test,Names_test=testDataset[0],testDataset[1],testDataset[2]\n\nmodel = Sequential()\n\nmodel.add(Input(shape=X_train.shape[1:]))\n\nmodel.add(Conv1D(16, 61, strides=1, padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(2, strides=2, padding=\"valid\"))\n\nmodel.add(Conv1D(32, 3, strides=1, padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(2, strides=2, padding=\"valid\"))\n\nmodel.add(Conv1D(64, 3, strides=1, padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(2, strides=2, padding=\"valid\"))\n\nmodel.add(Conv1D(64, 3, strides=1, padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(2, strides=2, padding=\"valid\"))\n\nmodel.add(Conv1D(64, 3, strides=1, padding=\"same\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(200))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\nopt = SGD(learning_rate=0.01)\n\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n\nmodel.summary()\n\nmodel.fit(X_train,Y_train,epochs=10,shuffle=False,validation_data=(X_test, Y_test))\n\nmodel.evaluate(X_train,Y_train)\n\n\nHere's model.fit():\nmodel.fit(X_train,Y_train,epochs=10,shuffle=False,validation_data=(X_test, Y_test))\n\nEpoch 1/10\n914/914 [==============================] - 277s 300ms/step - loss: 0.6405 - accuracy: 0.6543 - val_loss: 7.9835 - val_accuracy: 0.5000\nEpoch 2/10\n914/914 [==============================] - 270s 295ms/step - loss: 0.3997 - accuracy: 0.8204 - val_loss: 19.8981 - val_accuracy: 0.5000\nEpoch 3/10\n914/914 [==============================] - 273s 298ms/step - loss: 0.2976 - accuracy: 0.8730 - val_loss: 1.9558 - val_accuracy: 0.5002\nEpoch 4/10\n914/914 [==============================] - 278s 304ms/step - loss: 0.2897 - accuracy: 0.8776 - val_loss: 20.2678 - val_accuracy: 0.5000\nEpoch 5/10\n914/914 [==============================] - 277s 303ms/step - loss: 0.2459 - accuracy: 0.8991 - val_loss: 5.4945 - val_accuracy: 0.5000\nEpoch 6/10\n914/914 [==============================] - 268s 294ms/step - loss: 0.2008 - accuracy: 0.9181 - val_loss: 32.4579 - val_accuracy: 0.5000\nEpoch 7/10\n914/914 [==============================] - 271s 297ms/step - loss: 0.1695 - accuracy: 0.9317 - val_loss: 14.9538 - val_accuracy: 0.5000\nEpoch 8/10\n914/914 [==============================] - 276s 302ms/step - loss: 0.1423 - accuracy: 0.9452 - val_loss: 1.4420 - val_accuracy: 0.4988\nEpoch 9/10\n914/914 [==============================] - 266s 291ms/step - loss: 0.1261 - accuracy: 0.9497 - val_loss: 4.3830 - val_accuracy: 0.5005\nEpoch 10/10\n914/914 [==============================] - 272s 297ms/step - loss: 0.1142 - accuracy: 0.9548 - val_loss: 1.6054 - val_accuracy: 0.5009\n\nHere's model.evaluate():\nmodel.evaluate(X_train,Y_train)\n\n914/914 [==============================] - 35s 37ms/step - loss: 1.7588 - accuracy: 0.5009\n\nHere's model.summary():\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d (Conv1D)              (None, 4096, 16)          992       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 4096, 16)          64        \n_________________________________________________________________\nactivation (Activation)      (None, 4096, 16)          0         \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 2048, 16)          0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 2048, 32)          1568      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 2048, 32)          128       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 2048, 32)          0         \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 1024, 32)          0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 1024, 64)          6208      \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 1024, 64)          256       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 1024, 64)          0         \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 512, 64)           0         \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, 512, 64)           12352     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 512, 64)           256       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 512, 64)           0         \n_________________________________________________________________\nmax_pooling1d_3 (MaxPooling1 (None, 256, 64)           0         \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, 256, 64)           12352     \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 256, 64)           256       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 256, 64)           0         \n_________________________________________________________________\nflatten (Flatten)            (None, 16384)             0         \n_________________________________________________________________\ndropout (Dropout)            (None, 16384)             0         \n_________________________________________________________________\ndense (Dense)                (None, 200)               3277000   \n_________________________________________________________________\nactivation_5 (Activation)    (None, 200)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 402       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 3,311,834\nTrainable params: 3,311,354\nNon-trainable params: 480\n_________________________________________________________________\n\n", "type": 1, "id": "25819", "date": "2021-01-17T17:31:29.747", "score": 1, "comment_count": 0, "tags": ["deep-learning", "convolutional-neural-networks", "python", "keras", "1d-convolution"], "title": "Keras 1D CNN always predicts the same result even if accuracy is high on training set", "answer_count": 1, "views": 36, "accepted_answer": "26371", "answers": {"26371": {"line": 18007, "body": "The solution for my problem was implementing Batch Renormalization: BatchNormalization(renorm=True). In addition normalizing the inputs helped a lot improving the overall performance of the neural network.\n", "type": 2, "id": "26371", "date": "2021-02-13T17:11:55.607", "score": 0, "comment_count": 0, "parent_id": "25819"}}}
{"line": 19401, "body": "We want to try and distinguish real voices from (deep)fake voices using the graphs generated by a discrete fourier transform (generated from .wav audio files). We know from each image if it is a real or a fake voice, so it's a supervised classification problem. An image would look like this:\n\nWe think that real voices generate a graph with clear spikes, whereas fake voices have more noise resulting in less clear spikes. For this reason, we thought of using a CNN to take such an image as input (with x and y-axes ommited), and classify it as real or fake. Our concern is that it's actually a graph and not an image of an object, so we're not sure if this would be a good approach. We could also use the arrays generated from the fourier transform, but we're not sure how we could use that as input as we want to classify if it's real or fake, and not predict y for each x.\n", "type": 1, "id": "28149", "date": "2021-06-08T14:36:52.683", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning"], "title": "Can you use a graph as input for a neural network?", "answer_count": 1, "views": 73, "accepted_answer": null, "answers": {"28152": {"line": 19404, "body": "There no problem with the use of the data in form of an array to classify, whether the audio belongs to a real or fake voice. Just use 1d convolutional neural network with downsamplings or some global pooling operations, such that in the final layer the temporal extent of the signal has length 1. This would be the logit for binary classification.\nHowever, as far as I understand, you get rid of phase after the Fourier transform, but it can be useful for the prediction. Probably, a better approach would be to use mel_spectrogram https://en.wikipedia.org/wiki/Mel-frequency_cepstrum for this problem.\n", "type": 2, "id": "28152", "date": "2021-06-08T15:50:46.940", "score": 0, "comment_count": 1, "parent_id": "28149"}}}
{"line": 18943, "body": "Maybe it's silly to ask but for random exploration in an RL for choosing discrete action, that in the neural network last layer softmax will be used, what random samples should we provide? binary like (0,0,1,0,0,0) or continuous softmax like (0.1, 0.15, 0.45, 0.25, 0,5, 0.1)??\nif the answer is continuous, what algorithm do you suggest? like generating random numbers between 0 and 1 and then using softmax? (this algorithm mostly provides close numbers and I think it's not the correct way)\n", "type": 1, "id": "27597", "date": "2021-05-01T16:14:44.513", "score": 0, "comment_count": 3, "tags": ["neural-networks", "reinforcement-learning", "tensorflow", "python", "softmax"], "title": "Exploration for softmax should be binary or continuous softmax?", "answer_count": 1, "views": 50, "accepted_answer": null, "answers": {"27607": {"line": 18950, "body": "Firstly, I would suggest you do not use softmax for exploration, because it does not imply the model's uncertainty. Training with softmax and cross-entropy, your model may be very confident, but wrongly, because of overfitting.\nAnother reason why you should not use softmax to measure uncertainty is that your estimate of the variance (optimal estimate of the variance is the average error) may be quite low, explaining the low entropy of your model. However, there maybe exist regions (holes), where your model is highly uncertain (large error).\nThus, you should try other options for measuring uncertainty.\n\nOne way to measure model (or epistemic) uncertainty is to use a Bayesian Network estimating the whole distribution $p(\\theta | D)$, where \\theta are your parameters and D is the collected dataset of experience. The entropy of $p(\\theta | D)$ tells us the model uncertainty.\nWhy Bayesian Network? Intuitively, if our estimator says all ths are equally likely given your data, then it means that you have no idea of what the model really is. On the other hand, if it says there is one only th, that can possibly explain D, then you have high confidence in the model.\n\nAnother way to measure epistemic uncertainty is to use bootstrap ensembling. Instead of estimating the uncertainty of every single parameter in the Bayesian Net, learn N different networks! Use bootstrapping in order to generate N datasets (of same size as D) by resampling with replacement. Then train each network on a corresponding generated dataset. Then in testing, you can either sample uniformly a network ($p(\\theta | D) = \\frac{1}{N}\\sum_i^N{\\delta({\\theta}_i)}$)  or just average all networks predictions.\n\n\nFor exploration (in Deep RL), there are a lot of different options. I recommend you check information gain, Thompson sampling and pseudo-counts. However, if you were using the standard deep RL agents, like DQN etc, you could begin with the standard e-greedy behavior policy, ignoring that this is not an optimal exploration policy for the most cases.\n", "type": 2, "id": "27607", "date": "2021-05-02T09:53:23.290", "score": 0, "comment_count": 0, "parent_id": "27597"}}}
{"line": 20846, "body": "Assume we have a neural network and we want to train it on a classification problem. The hidden layers of the neural network are kind of feature representations of the input data.\nIf the neural network is big and the amount of data isn't enough for the complexity of the model, does it usually learn worse representations than a smaller neural network which is good for the amount of data we have?\n", "type": 1, "id": "31862", "date": "2021-09-28T09:36:11.883", "score": 2, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "overfitting", "generalization", "representation-learning"], "title": "Does a bigger neural network learn \"worse\" representations than a small neural network when the amount of data isn't enough?", "answer_count": 1, "views": 42, "accepted_answer": "31864", "answers": {"31864": {"line": 20848, "body": "Yes, it is a well known problem called Curse of Dimensionality. It happens when a finite number of data samples is used to train a network with a high-dimensional feature space (very deep network).\nWith regard to your question: yes, smaller networks (representational spaces with lower dimensions) describe better smaller datasets.\nWhy?\nBecause of data sparsity. As the features to represent a single sample increases the space between samples in the network representation also increases. When the samples are represented in too sparse space (all samples are very far away from each other in this high-dimensional space) then you can not draw any conclusions or relationships about them.\nThere is a balance between network dimensionality and dataset size, both of them must be in accordance to each other. You can think of this in this way: if data is represented in a very low dimensional space you can not tell them appart (1D space), if the data is represented in a very high dimensional space, then you can not find relationships within it. The dimensionality must be just the right one.\n\nImage from this post\n", "type": 2, "id": "31864", "date": "2021-09-28T10:29:12.053", "score": 0, "comment_count": 2, "parent_id": "31862"}}}
{"line": 20492, "body": "Is there a way to make a certain output dimension of a neural network independent of a particular feature dimension? For example, I have a function $f_{\\theta} : \\mathcal{R}^{10} \\rightarrow \\mathcal{R}^2$, I want to make $f_{\\theta}(\\mathbf{x})_2$  independent of $\\mathbf{x}_6$. How can this condition be imposed on a neural network?\nI am thinking of penalizing the gradient of $f_{\\theta}(\\mathbf{x})$ w.r.t $\\mathbf{x}_6$ for a considerable range of $\\mathbf{x}_6 \\in [-1, 1]$. Will this give me the similar effect? If so, how can this be coded in Pytorch or any other deep learning framework?\n", "type": 1, "id": "30450", "date": "2021-08-30T15:36:31.347", "score": 2, "comment_count": 0, "tags": ["neural-networks", "deep-learning"], "title": "How to make an output independent of input feature in neural networks?", "answer_count": 2, "views": 54, "accepted_answer": null, "answers": {"31880": {"line": 20860, "body": "You could use Mutual Information between the model's prediction, and that particular feature as a regularization term. This will minimize the dependence of the output to that particular feature. Note that simply removing the feature from the dataset might not work if other features are associated with the feature which you don't want your model to depend on.\n", "type": 2, "id": "31880", "date": "2021-09-29T19:45:56.893", "score": 0, "comment_count": 2, "parent_id": "30450"}, "30453": {"line": 20495, "body": "    def call(self, x):\n        x_no_x6 = tf.concat([x[:,:5], x[:,5+1:]], axis=1)\n        f2 = ... model architecture goes here ... (function of x_no_x6)\n        f1 = ... model architecture... (function of x)\n        return tf.concat([f1, f2], axis=1)\n\nYou could have two models, one of which uses $x$ (including $x_6$) and the other which removes $x_6$ from the input. The overall model is just the concatenation of the two outputs. (Pseudo-code is assuming model is implemented by subclassing tensorflow.keras.Model link)\nNot sure about the gradient penalization idea. Seems nontrivial to implement and it's not clear to me if it would work.\n", "type": 2, "id": "30453", "date": "2021-08-30T19:02:27.190", "score": 0, "comment_count": 3, "parent_id": "30450"}}}
{"line": 20925, "body": "I need to build a hand detector that recognizes the chord played by a hand on a guitar.\nI read this article Static Hand Gesture Recognition using Convolutional Neural Network with Data Augmentation that looks like what I need (hand gesture recognition).\nI think my task is (from my point of view) a little more difficult than that in the paper, because I think it is more difficult to distinguish between two chords than between a punch and a palm.\nWhat I don't understand clearly is how to choose the best parameters for this more complex task: is it better to have more/less convolutional layers? A higher or lower number of poolings? Max or avg pooling?\nThe input will be more or less like this one:\n\nThere will be a first net (MobileNetV2 trained on EgoHands) that will find the bounding box, crops the image and then passes the saturated blending between the original one and Frei&Chen edges to the second net (unfortunately I don't have a processed picture yet, I will post an example as soon as I get it)\n", "type": 1, "id": "31968", "date": "2021-10-08T07:16:21.577", "score": 3, "comment_count": 1, "tags": ["convolutional-neural-networks", "object-detection", "hyperparameter-optimization", "convolution", "pooling"], "title": "How do I choose the hyper-parameters for a model to detect different guitar chords?", "answer_count": 1, "views": 95, "accepted_answer": "32014", "answers": {"32014": {"line": 20964, "body": "The ideal hyperparameters is usually dependent on your dataset and will differ on a case by case basis. Go for trial and error to determine the hyperparameters that works best for you.\nFew research papers similar to your use case is listed below.\n\nCNN transfer learning for visual guitar chord classification\nA Study of Left Fingering Detection Using CNN for Guitar Learning\nA 3D Guitar Fingering Assessing System Based on CNN-Hand\nPose Estimation and SVR-Assessment\nApplying Deep Learning Techniques to Estimate Patterns of Musical Gesture : forearm gestures on violin playing\n\n", "type": 2, "id": "32014", "date": "2021-10-11T23:16:36.530", "score": 0, "comment_count": 0, "parent_id": "31968"}}}
{"line": 17692, "body": "I get the part from the paper where the image is split into P say 16x16 (smaller images) patches and then you have to Flatten the 3-D (16,16,3) patch to pass it into a Linear layer to get what they call \"Liner Projection\". After passing from the Linear layer, the patches will be vectors but with some \"meaning\" to them.\nCan someone please explain how the two types of embeddings are working?\nI visited this implementation on github, looked at the code too and looked like a maze to me.\nIf someone could just explain how these embeddings are working in laymen's terms, I'll look at the code again and understand.\n", "type": 1, "id": "25986", "date": "2021-01-26T04:28:19.783", "score": 2, "comment_count": 0, "tags": ["neural-networks", "computer-vision", "transformer", "embeddings", "vision-transformer"], "title": "How does the embeddings work in vision transformer from paper?", "answer_count": 1, "views": 53, "accepted_answer": null, "answers": {"32587": {"line": 21446, "body": "In Machine Learning \"embedding\" means taking some set of raw inputs (like natural language tokens in NLP or image patches in your example) and converting them to vectors somehow. The embeddings usually have some interesting dot-product structure between vectors (like in word2vec for example). The Transformer machinery then uses this embedding in the dot-product attention pipeline. The dimension of the embedding $D$ should stay constant throughout the transformer blocks due to ResNet skip connections.\nThe simplest idea in the case of the image patches would be to just take all the channels of all the pixels and treat them as a single vector. For example, if you've got (16, 16, 3) patches then you'll have 768-dimensional \"embeddings\". The problem with such a naive \"embedding\" is that dot-products between them won't make much sense. So we also multiply these vectors by a trainable matrix $W$ and add a bias vector.\nFor example, if you've got (16, 16, 3) patches and the transformer downstream uses $D=128$ dimensional embeddings, then you first flatten the patches into 16 * 16 * 3 = 768-dimensional vectors and then multiply by a 768$\\times$128  matrix and add a 128-dimensional bias vector.\nLooking at the code, it seems that the authors kept improving on that idea by adding one or several early convolutions with nonlinearities (the conv_stem branches in the code). The simplest execution branch before these embellishments seems to be here. And the matrix multiplication I've been talking about is here\n", "type": 2, "id": "32587", "date": "2021-12-01T16:00:13.697", "score": 0, "comment_count": 0, "parent_id": "25986"}}}
{"line": 18380, "body": "Hello I am currently doing research on the effect of altering a neural network's structure. Particularly I am investigating what affect would putting a random DAG (directed acyclic graph) in the hidden layer of a network instead of a usual fully connected bipartite graph.\nFor instance my neural network would look something like this:\n\nBasically I want the ability to create any structure in my hidden layer as long as it remains a DAG [add any edge between any node regardless of layers]. I have tried creating my own library to do so but it proved to be much more tedious than anticipated therefore I am looking for ways to do this on existing libraries such as Keras, pytorch, or tensorflow.\n", "type": 1, "id": "26870", "date": "2021-03-17T13:03:39.193", "score": 2, "comment_count": 0, "tags": ["neural-networks", "tensorflow", "keras", "pytorch", "implementation"], "title": "How can I model any structure for a neural network?", "answer_count": 3, "views": 74, "accepted_answer": null, "answers": {"26872": {"line": 18382, "body": "You seem to be seeking an implementation of a Residual Neural Network (https://en.m.wikipedia.org/wiki/Residual_neural_network), or ResNET for short. If you want some premade networks, the module tf.keras.applications.resnet from tensorflow (do check TF's documentation) might help you.\n", "type": 2, "id": "26872", "date": "2021-03-17T14:34:11.020", "score": 0, "comment_count": 0, "parent_id": "26870"}, "28091": {"line": 19345, "body": "With the existing frameworks PyTorch, Tensorflow you can easily implement this functionality, by keeping some of the intermediate computations inside forward or call method and passing them as an input to the given layer. For example:\nx[i] = layer[i-1](x[i - 1])\n...\nx[j] = layer[j-1](x[j - 1] + x[i]) # resnet-like skip connection\nor\nx[j] = layer[j-1](concat(x[j - 1], x[i])) # densenet-like skip connection\n\nHowever, if you are asking whether, there is some more educated way, or a constructor of this functionality, as far as I know, it is not implemented in these frameworks.\nIn case you want to have a computational graph of some arbitrary DAG structure, you will need to create some structures inside you NN class in order to know, where to keep activations in order to pass them further as skip connection and where to sum/concatenate them with the output of some other layer.\n", "type": 2, "id": "28091", "date": "2021-06-04T06:30:32.050", "score": 0, "comment_count": 0, "parent_id": "26870"}, "26873": {"line": 18383, "body": "Mentioned frameworks don't restrict you with linear layers sequence, you could do any acyclic sequence. I.e. very popular resnet architecture based on skip-connections that jumps over the layers.\nI.e. simple example on pytorch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass Custom(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = nn.Parameter(torch.tensor(1.))\n        self.b = nn.Parameter(torch.tensor(2.))\n        self.c = nn.Parameter(torch.tensor(3.))\n\n\n    def forward(self, x):\n        x1 = F.relu(self.a * x)\n        # take note, we skip\n        out = F.relu(self.b * x1 + self.c*x)\n        return out\n\nmodel = Custom()\nprint('before', model.a)\n# You could do pretty much the same training\nx = torch.tensor([2])\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\nprediction = model(x)\nloss = criterion(prediction, torch.tensor([20.]))\nloss.backward()\noptimizer.step()\nprint('after', model.a) \n\n", "type": 2, "id": "26873", "date": "2021-03-17T14:42:22.700", "score": 0, "comment_count": 0, "parent_id": "26870"}}}
{"line": 20040, "body": "I am asking this question for a better understanding of the concept of channels in images.\nI am aware that a convolutional layer generates feature maps from a given image. We can adjust the size of the output feature map by proper padding and regulating strides.\nBut I am not sure whether there exist kernels for a single convolution layer that are capable of changing an {RGBA, RGB, Grayscale, binary} image into (any) another {RGBA, RGB, Grayscale, binary} image?\nFor example, I have a binary image of a cat, is it capable to convert it into an RGBA image of a cat? If no, can it at least convert a binary cat image into an RGBA image?\nI am asking only from a theoretical perspective.\n", "type": 1, "id": "29922", "date": "2021-07-30T13:41:41.813", "score": 0, "comment_count": 8, "tags": ["convolutional-neural-networks", "convolution", "channel"], "title": "Is a convolutional layer capable of converting, for example, a binary image into an RGBA image?", "answer_count": 1, "views": 33, "accepted_answer": null, "answers": {"29929": {"line": 20047, "body": "No, because each output from a convolution layer only looks at a local region of the image. A convolution layer cannot do any global transformation, only local ones. Convolution layers must have translation invariance which means if it converts an eyeball to a tail at one position, it'll also convert the same eyeball to the same tail if it's found at a different position. If it's not overfitted, it will also convert similar eyeballs to similar tails. If you want only some eyeballs to become tails, you can't do that without introducing overfitting, or expanding the convolution size until the layer can see enough context to distinguish which eyeballs should become tails and which ones shouldn't.\nIf you want to change one image into a specific other image, and don't care what happens to all other images, it might be possible to create a convolution layer that does this transformation. The input image has to be different wherever the output image is different, or else the convolution layer won't be able to produce that difference in the output image. You would be teaching it to recognize the specific pixel patterns in the input image and generate the specific pixels in the output image. This would be an extreme case of overfitting and wouldn't work for any other input images.\nThe number of channels in the input and output image is irrelevant, except that more channels means the network has more data to learn from, obviously.\n", "type": 2, "id": "29929", "date": "2021-07-30T15:12:58.710", "score": 0, "comment_count": 10, "parent_id": "29922"}}}
{"line": 19336, "body": "I'm trying to build a neural network (NN) for classification using only N-bit integers for both the activations and weights, then I will train it with some heuristic algorithm, based only on the NN evaluation.\nCurrently, I'm using a non-linear activation function for hidden units. Because of its probability interpretation, I am forced to use the softmax (or the sigmoid for 2-class case) for the output layer. However, because of the use of integers, the linear combination of the activations and weights can easily be too large, and this causes a problem to the exponential in the softmax evaluation.\nAny solution?\n", "type": 1, "id": "28080", "date": "2021-06-03T15:46:19.750", "score": 1, "comment_count": 1, "tags": ["neural-networks", "convolutional-neural-networks", "activation-function", "weights", "softmax"], "title": "Which solutions are there to the problem of having too large activations before the softmax (or sigmoid) layer?", "answer_count": 1, "views": 51, "accepted_answer": null, "answers": {"28082": {"line": 19337, "body": "First of all, check out this question. Generally, you don't need to apply softmax and using raw logits leads to better numerical stability.\nThe numerical issue that you are talking about is well known and dealt with the so-called logsumexp trick. This usually is already incorporated in standard NN libraries. For example keras\nCategoricalCrossentropy loss can be configured to compute it from_logits.\n", "type": 2, "id": "28082", "date": "2021-06-03T17:29:15.027", "score": 0, "comment_count": 0, "parent_id": "28080"}}}
{"line": 20633, "body": "I was reading the following article on Towards Data Science (here) and it says the following, regarding the calculation of convolutional layers:\n\nSo the overall steps are:\n\nTransform the graph into the spectral domain using eigendecomposition\nApply eigendecomposition to the specified kernel\nMultiply the spectral graph and spectral kernel (like vanilla convolutions)\nReturn results in the original spatial domain (analogous to inverse GFT)\n\n\nQuestion: How can we visualize the convolutional layer working for a graph neural network?\nFor example, for a CNN we can imagine the following (source: Stanford CS231n YouTube lectures, Lecture 5: Convolutional Neural Networks (here)). What is the analogous image for a graph convolutional filter?\n\n", "type": 1, "id": "31614", "date": "2021-09-08T23:22:31.960", "score": 1, "comment_count": 4, "tags": ["geometric-deep-learning", "graph-neural-networks", "convolutional-layers"], "title": "How do convolutional layers of basic Graph Convolutional Networks work?", "answer_count": 2, "views": 74, "accepted_answer": null, "answers": {"32543": {"line": 21409, "body": "Actually, the given pipeline was used in the old days of Graph Neural Networks.\nCanonical paper on the subject is Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\nYou start from arbitrary graph with adjacency matrix $A_{ij}$ (let us assume that graph is undirected), such that:\n$$\nA_{ij} = \n\\begin{cases}\n1 & \\text{if there and edge between vertices $i$ and $j$} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThen one constructs graph Laplacian:\n$$\nL = D-A\n$$\n$D$ is the degree matrix (number of edges entering the given vertex). There are also different normalizations in the literature, like $I - D^{-1/2} A D^{-1/2}$ is called normalized Laplacian.\nThis matrix has several properties:\n\nIt is symmetric\nNon-negative definite\n\nFrom the first statement it follows, that the matrix can be diagonalized due to the Spectral theorem,\nTherefore, it makes sense to perform the eigendecomposition of this operator. And the eigenvectors form the graph Fourier basis.\nNote, that in the special case, when the graph is a regular square grid, graph Laplacian just the discrete Laplace operator:\n$$\n\\begin{pmatrix}\n0 & -1 & 0 \\\\\n-1 & 4 & -1 \\\\\n0 & -1 & 0 \\\\\n\\end{pmatrix}\n\\qquad \n(\\text{for 2d case})\n$$\nAnd the Fourier basis consists of plane waves:\n$$\n\\sim e^{i (k_i i + k_j j)} \n$$\nThe next important fact is the Convolution theorem that states that convolution of two signals can be done as inverse Fourier transform of the dot product of their Fourier transforms:\n$$\nf * g = \\mathcal{F}^{-1} [\\mathcal{F}[f] \\cdot \\mathcal{F}[g]]\n$$\nThese operations correspond to steps 3 and 4 in the pipeline in the OP.\nI am not aware of the simple geometrical intuition in this setting. But in the following research, full eigendecomposition was truncated to Chebyshev polynomials, and then up to the first term in the decomposition, which gave rise to Graph Convolutional Networks by T.Kipf.\nThey allow for more intuitive and visual interpretation. Given adjacency matrix $A$, input feature map on the graph $H^{(l)}$ one defines the output of the layer to be:\n$$\nf(H^{(l)}, A) = \\sigma(\\tilde{D}^{-1/2} A \\tilde{D}^{-1/2} H^{(l)} W^{(l)})\n$$\n$\\tilde{D}$ is the graph Laplacian for adjacency matrix with self-loops (arrows $i \\rightarrow i$).  $W^{(l)}$ is the matrix of learnable parameters.\nIn essence, one transforms the feature vectors $ H^{(l)}$ by a pointwise linear transformation, and aggregates the information from the neighborhood with some coefficients. Equivalently the function above can be rewritten as;\n$$H_{i}^{l+1}=\\eta\\left(\\frac{1}{\\hat{d}_{i}} \\sum_{j \\in N_{i}} \\hat{\\boldsymbol{A}}_{i j}{W}^{l} H_{j}^{l}\\right)$$\nFrom the formula above one can see, that graph convolution is the summation of the features in the neighborhood vertices which are transformed by the weight matrix ${W}^{l}$ and division by the normalizing constant.\n", "type": 2, "id": "32543", "date": "2021-11-28T19:47:40.010", "score": 0, "comment_count": 3, "parent_id": "31614"}, "32542": {"line": 21408, "body": "This post uses visual examples and calculations to better understand the convolution layer. Helped me a lot!\n", "type": 2, "id": "32542", "date": "2021-11-28T18:36:40.277", "score": 0, "comment_count": 2, "parent_id": "31614"}}}
{"line": 18718, "body": "I would like to show the RGB features learned in the first layer of a convolutional neural network similarly to this visualization of the same layer's features from AlexNet:\n\nMy learned weights are in the range [-1.1,1.1]. When I use imshow in python or imagesc in Matlab, the weight values are clipped to [0,1], leaving only positive weights intact, everything else black (obviously).\nNegative weight values could be informative, so I don't want to clip them. Rescaling the weights to [0,1] works fine for grayscale features, but not for RGB features as it is unclear how negative values of a channel should be visualized. In the above picture 0 furthermore seems to map to the middle of the range (gray).\nHow are such RGB features visualized so that they look similarly to above AlexNet visualization?\n(Sorry for the beginner's question.)\n", "type": 1, "id": "27304", "date": "2021-04-13T14:17:18.933", "score": 2, "comment_count": 0, "tags": ["neural-networks", "convolutional-neural-networks"], "title": "Showing first layer RGB weights similarly to AlexNet", "answer_count": 1, "views": 56, "accepted_answer": null, "answers": {"27483": {"line": 18858, "body": "It seems Alex has just used the Matlab function mat2gray, as described here: https://www.mathworks.com/help/vision/ug/image-category-classification-using-deep-learning.html\nThe visual outcome of the features is very similar. mat2gray will simply scale the weights between 0 and 1 (no clipping).\nLeaving the (slightly adapted) example code of Mathworks here for future reference:\nlayer1weights = mat2gray(layer1weights) ;\nlayer1weights = imresize(layer1weights,5) ; \n\nfigure\nmontage(layer1weights)\ntitle('First convolutional layer weights')\n\n", "type": 2, "id": "27483", "date": "2021-04-23T14:37:52.773", "score": 0, "comment_count": 0, "parent_id": "27304"}}}
{"line": 18489, "body": "I'm trying to create a neural network to simulate a XOR gate.\nHere's my dataset:\n+--------+--------+\n| x1, x2 | y1, y2 |\n+--------+--------+\n|  0, 0  |  0, 1  |\n|  0, 1  |  1, 0  |\n|  1, 0  |  1, 0  |\n|  1, 1  |  0, 1  |\n+--------+--------+\n\nAnd my neural network:\n\nI use logistic loss to get the error between target $y_{k}$ and output $\\hat{y}_{k}$:\n$$ E(y_{k}, \\hat{y}_{k}) = - y_{k} \\cdot log(\\hat{y}_{k}) + (1 - y_{k}) \\cdot log(1 - \\hat{y}_{k}) $$\nAnd then use the chain rule to update the weights. For example weight $w_{3}$'s contribution to the error is:\n$$ \\sum_{k=1}^{2} \\frac{\\partial E(y_{k}, \\hat{y}_{k})}{\\partial w_{3}} = \\sum_{k=1}^{2} \\left(\\frac{\\partial E(y_{k}, \\hat{y}_{k})}{\\partial \\hat{y}_{k}} \\cdot \\frac{\\partial s_{k}}{\\partial c_{1}}\\right) \\cdot \\frac{\\partial c_{1}}{\\partial w_{3}} $$\nWhich in developed form is:\n$$ \\sum_{k=1}^{2} \\frac{\\partial E(y_{k}, \\hat{y}_{k})}{\\partial w_{3}} = \\left( \\left( - \\frac{y_{1}}{\\hat{y}_{1}} + \\frac{1 - y_{1}}{1 - \\hat{y}_{1}} \\right) \\cdot \\hat{y}_{1} \\cdot (1 - \\hat{y}_{1}) + \\left( - \\frac{y_{2}}{\\hat{y}_{2}} + \\frac{1 - y_{2}}{1 - \\hat{y}_{2}} \\right) \\cdot (- \\hat{y}_{2}) \\cdot\\frac{c_{1}}{c_{1} + c_{2}} \\right) \\cdot s_{0} $$\nMy issue is that after a couple epochs of training on the entire dataset, the network always outputs:\n$$ \\hat{y}_{1} = \\hat{y}_{2} = 0.5 $$\nWhat am I doing wrong?\n", "type": 1, "id": "27010", "date": "2021-03-25T18:55:47.093", "score": 1, "comment_count": 0, "tags": ["neural-networks", "softmax", "xor-problem", "categorical-crossentropy"], "title": "XOR Neural Network gets stuck in training", "answer_count": 1, "views": 72, "accepted_answer": "27253", "answers": {"27253": {"line": 18676, "body": "One Neuron on its own can only solve linearly separable problems.\nYou need a combination of Neurons to solve non-linearly separable problems.\nFor the XOR case, you need at least 2 Neuron at the first layer, and 1 Neuron at the Output layer to properly classify it.\nKeep in mind sometimes the 3 Neuron network might get stuck in a local minima as well, you will need some luck in the random initialization of weights.\nUsing the right seed during the random initialization of weights can help converge, and some other seed will only result in a stuck network.\n", "type": 2, "id": "27253", "date": "2021-04-10T12:29:05.713", "score": 0, "comment_count": 0, "parent_id": "27010"}}}
{"line": 19744, "body": "In transfer learning, we use big data from similar tasks to learn the parameters of a neural network, and then fine-tune the neural network on our own task that has little data available for it. Here, we can think of the transfer learning step as learning a (proper) prior, and then fine-tuning as learning the posterior.\nSo, we can argue that Bayesian networks can also solve the problem of small data-set regimes. But, what are the directions that we can mix Bayesian neural networks with similar tasks to transfer learning, for example, few-shot learning?\nThey make sense when they both take a role as a solution to the low data regime problems, but I can't think of a mix of them to tackle this issue.\nIs it possible, for example, to learn a BNN for which we have picked a good prior to learn the posterior with little data and use the weight distribution for learning our new task? Is there any benefit in this?\n", "type": 1, "id": "28549", "date": "2021-07-06T20:38:18.800", "score": 2, "comment_count": 2, "tags": ["reference-request", "transfer-learning", "bayesian-deep-learning", "bayesian-neural-networks", "one-shot-learning"], "title": "How could Bayesian neural networks be used for transfer learning?", "answer_count": 1, "views": 63, "accepted_answer": null, "answers": {"28551": {"line": 19746, "body": "Well, I would say, that purpose of Bayesian inference is not transfer learning, but uncertainty estimation.\nIn case you have good feature extractor in the beginning, you can adjust small number of parameters, like few last layers to achieve good quality in few epochs.\nHowever, this is about adjusting the means of distributions over each weight.\nConcerning the variance, I think transfer learning is inapplicable since the source and target distributions can be very different. For example, ImageNet is a broad and diverse dataset with many classes, and the target problem can involve only a few classes. Most probably, uncertainty estimate and the standard deviations of model weights on the ImageNet would be larger, than for the model, trained solely on the target task.\n", "type": 2, "id": "28551", "date": "2021-07-06T21:05:20.663", "score": 0, "comment_count": 0, "parent_id": "28549"}}}
{"line": 17741, "body": "In the paper Hopfield networks is all you need, the authors mention that their modern Hopfield network layers are a good replacement for pooling, GRU, LSTM, and attention layers, and tend to outperform them in various tasks.\nI understand that they show that the layers can store an exponential amount of vectors, but that should still be worse than attention layers that can focus parts of an arbitrary length input sequence.\nAlso, in their paper, they briefly allude to Neural Turing Machine and related memory augmentation architectures, but do not comment on the comparison between them.\nHas someone studied how these layers help improve the performance over pooling and attention layers, and is there any comparison between replacing layers with Hopfield layers vs augmenting networks with external memory like Neural Turing Machines?\nEdit 29 Jan 2020\nI believe my intuition that attention mechanism should outperform hopfield layers was wrong, as I was comparing the hopfield layer that uses an input vector for query $R (\\approx Q)$ and stored patterns $Y$ for both Key $K$ and Values $V$. In this case my assumption was that hopfield layer would be limited by its storage capacity while attention mechanism does not have such constraints.\nHowever the authors do mention that the input $Y$ may be modified to ingest two extra input vectors for Key and Value. I believe in this case it would perform hopfield network mapping instead of attention and I do not know how the 2 compare.\n\n", "type": 1, "id": "26038", "date": "2021-01-28T07:06:17.593", "score": 1, "comment_count": 5, "tags": ["neural-networks", "attention", "neural-turing-machine", "hopfield-network"], "title": "Reasoning behind performance improvement with hopfield networks", "answer_count": 1, "views": 86, "accepted_answer": null, "answers": {"26157": {"line": 17833, "body": "Will try to formulate my understanding of the ideas in this paper, mention my own concerns that I see are relevant to your question, and see if we can identify any confusions along the way that might clarify the issue\nOn eq(6) of the relevant blog post, they identify the weight matrix of a discrete, binary Hopfield network as\n$$ \\boldsymbol{W} = \\sum_i^N x_i x_i^T $$\nwith N raw stored entries, that are retrieved by iterating the initial guess $\\xi$ with the following update rule\n$$ \\xi_{t+1} = \\text{sgn}( \\boldsymbol{W} \\xi_{t} - b ) $$\nNow to the paper in question, the update rule for the generalization they propose for continuous states that would be used is (eq 22 in OP):\n$$ \\xi_{t+1} = \\boldsymbol{X} \\text{softmax}( \\beta \\boldsymbol{X}^T \\xi_{t} ) $$\nWhere $\\boldsymbol{X} = (x_0, x_1, \\dots , x_N ) $\nThe first substantial difference I see is that, while on the case of binary entries, all the weights of the network are encoded in the matrix $\\boldsymbol{W}$, hence the storage is constant regardless of how many actual patterns are stored in it. In contrast, in this continuous case generalized rule, the $\\boldsymbol{X}$ matrix seems to grow linearly with the size of entries, in fact it keeps all the stored entries directly. By their update rule, it doesn't seem to be a way around storing and keeping around the entire entries, and the update rule seems to only find a \"best fit\" among the entries, using the scalar dot product of the attention mechanism. I still think I might be missing something important here\n", "type": 2, "id": "26157", "date": "2021-02-02T18:41:11.527", "score": 0, "comment_count": 0, "parent_id": "26038"}}}
{"line": 18392, "body": "Goal\nTo build an RNN which would receive a word as an input, and output the probability that the word is in English (or at least would be English sounding).\nExample\ninput:  hello \noutput: 100%\n\ninput:  nmnmn \noutput: 0%\n\nApproach\nHere is my approach.\nRNN\nI have built an RNN with the following specifications: (the subscript $i$ means a specific time step)\nThe vectors (neurons):\n$$\nx_i \\in \\mathbb{R}^n \\\\\ns_i \\in \\mathbb{R}^m \\\\\nh_i \\in \\mathbb{R}^m \\\\\nb_i \\in \\mathbb{R}^n \\\\\ny_i \\in \\mathbb{R}^n \\\\\n$$\nThe matrices (weights):\n$$\nU \\in \\mathbb{R}^{m \\times n} \\\\\nW \\in \\mathbb{R}^{m \\times m} \\\\\nV \\in \\mathbb{R}^{n \\times m} \\\\\n$$\nThis is how each time step is being fed forward:\n$$\ny_i = softmax(b_i) \\\\\nb_i = V h_i \\\\\nh_i = f(s_i) \\\\\ns_i = U x_i + W h_{i-1} \\\\\n$$\nNote that the $ + W h_{i-1}$ will not be used on the first layer.\nLosses\nThen, for the loss of each layer, I used cross entropy ($t_i$ is the target, or expected output at time $i$):\n$$\nL_i = -\\sum_{j=1}^{n} t_{i,j} \\ln(y_{i,j})\n$$\nThen, the total loss of the network:\n$$\nL = \\sum L_i\n$$\nRNN diagram\nHere is a picture of the network that I drew:\n\nData pre-processing\nHere is how data is fed into the network:\nEach word is split into characters, and every character is split into a one-hot vector. Two special tokens START and END are being appended to the word from the beginning and the end. Then the input at each time step will be every sequential character without END, and the output at each time step will be the following character to the input.\nExample\nHere is an example:\n\nStart with a word: \"cat\"\nSplit it into characters and append the special tags: START  c  a  t  END\nTransform into one-hot vectors: $v_1, v_2, v_3, v_4, v_5$\nThen the input is $v_1, v_2, v_3, v_4$ and the output $v_2, v_3, v_4, v_5$\n\nDataset\nFor the dataset, I used a list of English words.\nSince I am working with English characters, the size of the input and output is $n=26+2=28$ (the $+2$ is for the extra START and END tags).\nHyper-parameters\nHere are some more specifications:\n\nHidden size: $m=100$\nLearning rate:  $0.001$\nNumber of training cycles:  $15000$ (each cycle is a loss calculation and backpropagation of a random word)\nActivation function: $f(x) = \\tanh(x)$\n\nProblem/question\nHowever, when I run my model, I get that the probability of some word being valid is about 0.9 regardless of the input.\nFor the probability of a word begin valid, I used the value at the last layer of the RNN at the position of END tag after feeding forward the word.\nI wrote a gradient checking algorithm and the gradients seem to check up.\nIs there conceptually something wrong with my neural network?\nI played a bit with $m$, the learning rate, and the number of cycles, but nothing really improved the performance.\n", "type": 1, "id": "26882", "date": "2021-03-18T00:22:17.573", "score": 1, "comment_count": 2, "tags": ["neural-networks", "natural-language-processing", "recurrent-neural-networks", "data-preprocessing"], "title": "Is my approach to building an RNN to predict the probability that the word is in English appropriate?", "answer_count": 2, "views": 90, "accepted_answer": "26931", "answers": {"26949": {"line": 18443, "body": "I think the problem is that you're only training the network on words. Every example in your training data has a desired label of \"is a word,\" and so your network could achieve the lowest possible loss by simply giving a probability of 100% to \"is a word\" all of the time.\nThe most straightforward way to fix this would be to also include non-words in your training data. Of course, the words should have a target label of \"is a word\" and the non-words should have a target label of \"is not a word.\"\n", "type": 2, "id": "26949", "date": "2021-03-22T20:25:18.437", "score": 0, "comment_count": 0, "parent_id": "26882"}, "26931": {"line": 18427, "body": "while using a neural network for this type of problem is not the ideal use-case, it is a good exercise.\nIn terms of conceptual issues, the most concerning that I see is the loss: $\\sum_{i=1}^N L_i$.\nFirst issue, is that it validates loss at each time step equivalently. This is probably not ideal because in the example (cat), we dont expect it to know its English from just c or ca, but at cat. The quickest fix and probably the best, is to just use $L = L_N$. Though an argument would be that the model should become more and more aware as it gets more letters, and this loss function doesnt achieve that, so another solution would be to add another fixed parameter that you can play with: $L = \\sum_i r^{-i}L_i$ where $0 \\lt r \\lt 1$. Note that $r^{-i}$ can be replaced with any function that increases in size.\nAnother issue with the loss is it doesnt normalize, meaning on average, larger words will hold more weight to the model than smaller ones, while this may be intended it should be noted and considered (also note if you do end up going with $L=L_N$ this will no longer be a concern.\nHope this helps\n", "type": 2, "id": "26931", "date": "2021-03-20T21:17:22.123", "score": 0, "comment_count": 5, "parent_id": "26882"}}}
{"line": 21038, "body": "This question is assuming a sequential, deep neural network\nGiven some features [X1, X2, ... Xn], I'm trying to predict some value Y.\nThe raw data available to me contains feature X1 and feature X2. Say that I know there is an effect on Y based on the ratio of the two features, i.e. X1 / X2.\nShould I add a new feature, mathematically defined as the ratio of the two features? I haven't been able to locate any literature which begins to describe the necessity or warnings of this.\nInstinctly I'm worried about the following:\n\nOverfitting and the need for excessive regularization, due to duplicate information in the feature set\nExponentially growing number of features, since defining a ratio between each feature may be necessary\n\nHowever, I also recognize that certain relationships are impossible to be defined by a deep neural network (i.e. logic gates, exponential relationships, etc), so when would this sort of \"relationship defining\" be necessary? For example, if an exponential relationship is known to exist?\n", "type": 1, "id": "32097", "date": "2021-10-18T14:47:02.060", "score": 3, "comment_count": 4, "tags": ["neural-networks", "deep-learning"], "title": "Should I allow NN to infer relationships of inputs?", "answer_count": 3, "views": 108, "accepted_answer": "32409", "answers": {"32413": {"line": 21297, "body": "Ideal advise is to feed the raw data to the neural networks to let neural networks make its own inference\nConsidering you have expert knowledge that $X1/X2$ has effect on $Y$ , here the new feature ($X1/X2$) is referred to as a derived feature\nHowever, there are few advantages which can help you consider of using derived features like $X1/X2$ in your case\n\nBeing a domain expert or SME choosing X1/X2 as an important feature, you can ideally accelerate the training process\nHighly advantageous when you are short on CPU time\nMost neural networks calculate sums fed through the activation functions, estimating the ratio or the multiplication requires lot of neurons especially if $X1/X2$ is an important feature\nYou can use feature pruning techniques to eliminate redundant features\nOne major drawback of neural network is often considered to be a black box model that's the approximation of neural networks doesn't give any insight of the form of function f. Use make use of feature selection algorithms to pare down feature space.\nWith large number of parameters in model also increasing the risk of overfitting the network. You can however overcome this with good regularisation methods. You can also use feature pruning to avoid overfitting when you are having limited data.\n\nI found a literature in the field of medicine that deals with deriving features based on expertise accurately identified the required states.\nHeart rate variability-derived features based on deep neural network for distinguishing different anaesthesia states\n\nThe incorporation of four HRV-derived features in the time and frequency domain and a deep neural network could accurately distinguish between different anaesthesia states.\n\n", "type": 2, "id": "32413", "date": "2021-11-15T19:27:32.813", "score": 1, "comment_count": 0, "parent_id": "32097"}, "32409": {"line": 21293, "body": "You are refering to the first and very important step in a machine learning process called data preprocessing. Refering to this article, inside data preprocessing there are many smaller processes that deal with features: feature extraction, feature selection, feature aggregration and feature encoding to name a few.\nThe idea of creating new features out of raw features is not new, but rather a well known concept in machine learning called feature extraction. Suppose you decide to create the new $X_1 / X_2$ feature and add it to the raw set of features ${X_1, X_2}$, because you are completely sure that it has an effect on $Y$, then that is a perfectly reasonable example of feature extraction.\nRefering to the article above on feature extraction, notice that nowadays there is some growing consensus that when using deep neural networks, the first hidden layers of the network can serve the purpose of automatic feature extraction, without you manually adding the new features to the raw set of features.  This premise stems from the fact that deep neural networks are essentially non-linear function approximators, so the first hidden layers can approximate any feature extraction that you do manually.\nYou can understand that in order to do automatic feature extraction, you might need more hidden layers and more neurons in each layer, to increase the computing power of your model. There is also the downside that you can not know for sure what is happening in the first hidden layers, if it is doing feature extraction the way you intended or not.\nIn your case, my advice would be to do the manual addition of $X_1 / X_2$ in the set of features, if you are only adding one such fraction or a handful. Also, if you are completely sure there is a correlation between those fractions and $Y$. If on the other hand, you are adding more than a handful of such fractions, then it does not make sense to add them all. It is hard to be completely sure that all the fractions have an effect on $Y$. It would also exhaust the memory space, by having a lot of features on the input layer. In that case I would suggest that you skip manual feature extraction, increase the number of hidden layers and let the network do its auto-magic.\n", "type": 2, "id": "32409", "date": "2021-11-15T16:34:21.290", "score": 2, "comment_count": 2, "parent_id": "32097"}, "32360": {"line": 21251, "body": "Model and Objective function are playing together. If you can have an objective function that somehow exclude this relation that you have in your mind, then the model can focus on learning to predict based on other information. Then you have trained the model, but if your downstream task should consider that at the end, you could manually add this relation and apply the relation you have in your mind at the end. That's my philosophical answer, but the question is how do you want to implement it? It depends on details of the project, and this approach might not be feasible, but I guess your concern is real. The model is inclined to learn the shortest path to the answer, and if you provide it for it, then it will use it.\nIf I were you, I might try to first normalize the features so they'd have similar effects. I don't know how this would be implemented in your problem though.\n", "type": 2, "id": "32360", "date": "2021-11-11T06:22:21.957", "score": 0, "comment_count": 0, "parent_id": "32097"}}}
{"line": 18838, "body": "While reading the AlphaZero paper in preparation to code my own RL algorithm to play Chess decently well, I saw that the\n\n\"The board is oriented to the perspective of the current player.\"\n\nI was wondering why this is the case if there are two agents (black and white). Is it because there is only one central DCNN network used for board and move evaluation (i.e. there aren't two separate networks/policies used for the respective players - black and white) in the algorithm AlphaZero uses to generate moves?\nIf I were to implement a black move policy and a white move policy for the respective agents in my environment, would reflecting the board to match the perspective of the current player be necessary since theoretically the black agent should learn black's perspective of moves while the white agent should learn white's perspective of moves?\n", "type": 1, "id": "27457", "date": "2021-04-22T02:53:28.157", "score": 3, "comment_count": 0, "tags": ["neural-networks", "reinforcement-learning", "alphazero", "chess", "multi-agent-systems"], "title": "Why does Alpha Zero's Neural Network flip the board to be oriented towards the current player?", "answer_count": 2, "views": 183, "accepted_answer": "27469", "answers": {"27466": {"line": 18844, "body": "I am not an expert in RL. I have been playing Go for some years.\nLet's quote from AlphaZero's paper first:\n\nAside from\nkomi, the rules of Go are also invariant to colour transposition; this knowledge is\nexploited by representing the board from the perspective of the current player (see\nNeural network architecture).\n\nIn the Game of Go, the difference between Black and White except the board representation is the komi (the amount of points that Black has to compensate White in the final count for playing first). Except the presence of komi, there should be no difference in strategy under the same position if colours exchanged. In other words, given a state $s$ of black stones and white stones on the board, if the optimal policy of Black playing first is $\\pi$, then if colours of stones on the board exchanged and it is White's turn, the optimal policy for White should be the same as $\\pi$.\nWith this in consideration, there are at least 2 advantages of using a network that represents the board in the perspective of Self/Opponent rather than Black/White.\nThe first is that it prevents the network from the possibility of giving inconsistent strategies under two representations of the same state. Consider a network $f_\\theta$ that accepts the board representation in the order of $(B,W)$, and a state $s = (X_t,Y_t)$ in which $X_t$ is a feature map for black stones and $Y_t$ is a feature map for white stones and it is black's turn. Now consider a state $s' = (Y_t,X_t)$ (i.e. colours flipped) and it is white's turn. $s$ and $s'$ are essentially representation of the same state (except Komi which does not affect optimal policy). There could be a possibility that the network $f_\\theta$ gives different policies for these two representations. However, if $f_\\theta$ accepts the state as $(Self,Opponent)$, the input to the network would be the same (except the komi feature).\nTherefore, this representation would significantly reduce number of states represented by the features vector $(X_t,Y_t)$, which would be the second advantage to training the neural network. If we consider that in Go, the same local position could appear in exchanged colour in another position, the network could by this implementation, recognize them as the same position. A decrease in the number of states could mean a significant drop in parameters and power needed from the network.\nThe same principle of making use of different representations of the same state is followed in AlphaGo's other training implementations as well, such as augmenting its training data to include rotations and reflections of the same board position.\nHowever, in the game of Chess, this would be a different case. For a chess position, if the pieces' colours are exchanged and it becomes opponent's turn, it would be a different state because the positions of the KING and the QUEEN are not the same for the two colours.\n", "type": 2, "id": "27466", "date": "2021-04-22T14:58:52.097", "score": 2, "comment_count": 1, "parent_id": "27457"}, "27469": {"line": 18847, "body": "There is a single neural network that guides self-plays in the Monte Carlo Tree Search algorithm. The neural network gets the current state of the board $s$ as an input and outputs current policy $\\pi(a|s)$ and value $v(s)$.\nThe action probabilities are encoded in a (8,8,73) tensor. First two dimensions encode the coordinates of the figure to \"pick\" from the board. The third dimension encode where to move this figure: check out this question for a discussion on how all possible moves are encoded in a 73 dimensional vector.\nSimilarly, the inputs of the network are organized in the (8, 8, 14 * 8 + 7 = 119) tensor. The first two 8 x 8 dimensions, again, encode the positions on the board. Then the positions of the figures one plane per 6 figure types: first 6 planes for player's figures, next 6 planes for opponent's figures and two repetition planes. The 14 planes are repeated 8 times supplying predecessor positions to the network. Finally, there are 7 extra planes encoding as a single uniform value over the board - castling rights (4 planes), total move count (2 planes) and the current player color (1 plane).\nNote that the positions of player's figures and opponent's figures are encoded in fixed layers of the state tensor. If you don't flip the board to the perspective of the player then the network will have very different training inputs for black and white states. It also will have to figure out which direction the pawns can move depending on the current player color. None of that it is impossible, of course - but that unnecessarily complicates something that is already a very hard problem for the DNN to learn.\nYou can go further and completely split the training for white and black players, as you've described. But that'll essentially double the work you'll have to do train your nets (and, I suspect, there would be some stability troubles typical for adversarial training).\nTo summarize - you are generally right - there is no fundamental need to flip the board. All the above details in state encoding are done to simplify the learning task for the deep neural network.\n", "type": 2, "id": "27469", "date": "2021-04-22T16:58:17.007", "score": 0, "comment_count": 1, "parent_id": "27457"}}}
{"line": 20035, "body": "I'm trying to train a Neural Network in a particular situation -- similar to a genetic algorithm domain as far as I know.\nI have to run a simulation with a length of $K$ steps.\nI have a neural network $N$ that at each time step is used to produce an output, so that:\n$$\no_{t+1} = N(i_{t})\n$$\n$i_t$ is a feature vector built upon $o_{t-1}$, and $i_0$ is given.\nMy ground-truth value is $o_k$, namely the right value at the end of the simulation. So, I can evaluate the loss (e.g. MSE) only at the end of the simulation.\nSuppose to fix k to 3, the evaluation is: $N(N(N(i_0))$\nbecause:\n$$\no_1 = N(i_o) \\\\\no_2 = N(i_1) \\\\\no_3 = N(i_2)\n$$\nSo my questions are:\n\ndoes it have any sense to apply backpropagation in these settings?\nif yes, what happens to the gradients?\n\nPractically, in some simple situations, the backpropagation seems to work, but in others, the gradients explode or vanishes\n", "type": 1, "id": "29916", "date": "2021-07-30T07:08:39.347", "score": 0, "comment_count": 2, "tags": ["neural-networks", "machine-learning"], "title": "Backpropagation after N sequential input-output pass", "answer_count": 1, "views": 64, "accepted_answer": null, "answers": {"30118": {"line": 20203, "body": "I/ Does it has any sense to apply backpropagation in these settings?\nIf I understand correctly, this question should be \"Can backpropagation return the gradient for every node of these networks?\".\nIt depends on your network $N$ if it is differentiable with all inputs $i_t$ so the backpropagation can be guaranteed that there are the gradient values at every node of the network.\nII/ If yes, what happens to the gradients?\nWe deal with the vanishing or exploding gradient after we make sure that the gradient is exist. The reason for this phenomenon in a neural network can come various, some typical causes are:\n\nVery deep neural networks (a large number of layers):\nAssume a neural network with $n$-th layers, the backpropagation is followed the Markov Chain's rule can be show as:\n\n\n$$\\frac{\\partial loss}{\\partial w_t} = \\frac{\\partial loss}{\\partial l_{n}} \\times \\frac{\\partial l_{n}}{\\partial l_{n-1}}\\times...\\times \\frac{\\partial l_{t+1}}{\\partial l_{t}}\\times \\frac{\\partial l_t}{\\partial w_t}$$\nIt's easy to see that the gradient is scaled exponentially for each layer, so if each gradient value is larger than 1, the gradient will become $\\inf$ with $n\\rightarrow \\inf$ (exploding) and $0$ in vice versa.\n\nActivation function:\nThe image below shows the difference between different activation functions. It's easy to see that the logistic function such as Sigmoid or Tanh is limited in the range $[0,1]$ (in case sigmoid) or $[-1,1]$ in case Tanh. Therefore, if the output from a node of the neural network is larger than 2 or smaller than -2, the gradient will become nearly zero (vanishing) because the output is always the same.\n\n\n\nSolution:\n\nReplace it by the simple function such as ReLU: $o_{t+1} = max(0, N(i_t))$. However, the output from ReLU will be $0$ if it's lower than $0$, so there are many variants of ReLU to solve this problem, you can find them easily by the keyword \"ReLU family\" on google (example).\n\nThere are other methods or strategies to duel with vanishing/exploding gradient, it also depends on your model or your data. The answer can be more detailed if you give more information.\n", "type": 2, "id": "30118", "date": "2021-08-11T02:12:30.777", "score": 0, "comment_count": 0, "parent_id": "29916"}}}
{"line": 21057, "body": "I have recently been studying GNN, and the fundamental idea seems to be the aggregation and transfer of information from a node's neighborhood to update the node's internal state. However, there are few sources that mention the implementation of GNN in code, specifically, how do GNNs adapt to the differing number of nodes and connections in a dataset.\nFor example, say we have 2 graph data that looks like this:\n\nIt is clear that the number of weights required in the two data points would be different.\nSo, how would the model adapt to this varying number of weight parameters?\n", "type": 1, "id": "32122", "date": "2021-10-20T02:39:13.067", "score": 1, "comment_count": 0, "tags": ["implementation", "geometric-deep-learning", "graph-neural-networks"], "title": "How do graph neural networks adapt to different number of nodes and connections of different graphs?", "answer_count": 1, "views": 32, "accepted_answer": "32123", "answers": {"32123": {"line": 21058, "body": "The essence of the reason, why this approach works for graphs with a different number of nodes is the locality and node order permutation invariance.\nThe typical form of the layer-wise signal propagation rule is:\n$$\nH^{(l+1)} = f(H^{(l)}, A) = \\sigma (A H^{(l)} W^{(l)})\n$$\nHere $H^{(k)}$ are the activations of the $k$-th later, $W^{(k)}$ is the weight matrix, $A$ is the adjacency matrix and $\\sigma$ is the activation function.\nActivation function $\\sigma$ and $W^{(k)}$  is the same on any graph, and the difference is only in the choice of adjacency matrix $A$.\nAggregation of the information from the neighborhood is done in permutation-invariant way, and the only way to do this is to assign the same weight for every member in neightboorhood, and (probably) some other weight to the node itself.\nFor Graph Convolutional Neural Networks (GCNN's) this works as follows:\n$$\nh_{v_i}^{(l+1)} = \\sigma (\\sum_{j \\in N(i)} \\frac{1}{c_{ij}} h_{v_j}^{(l)} W^{(l)})\n$$\nRegardless of whether the node is isolated or has many neighbors procedure is unchanged.\nOlder approaches, the spectral for example, had to calculate the Graph Laplacian and perform its eigendecomposition. They did not generalize to other graphs.\n", "type": 2, "id": "32123", "date": "2021-10-20T06:18:02.950", "score": 0, "comment_count": 5, "parent_id": "32122"}}}
{"line": 19001, "body": "I'm creating a neural network with 3 layers and no bias.\nOn internet I saw that the expression for the derivative of the weights between the hidden layer and the output layer was:\n$$\\Delta W_{j,k} = (o_k - t_k) \\cdot f'\\left[\\sum_j (W_{j,k} \\ \\cdot o_j)\\right] \\cdot o_j,$$\nwhere $t$ is the target output, $o$ is the activated output layer and $f'$ the derivative of the activation function.\nBut the shape of these weights is $\\text{output nodes}\\times\\text{hidden nodes}$, and $\\text{hidden nodes}$ can be bigger than $\\text{output nodes}$, so the formula is wrong because of I'm taking $o_k$ and $o$ has length $\\text{output nodes}$.\n\nIn simple terms, what is the right formula for updating these weights?\n\nAlso, what is the right formula for updating the weights between the input layer and the hidden layer?\n\n\n", "type": 1, "id": "27673", "date": "2021-05-06T13:15:46.530", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "backpropagation", "calculus"], "title": "What is the correct formula for updating the weights in a 1-single hidden layer neural network?", "answer_count": 1, "views": 53, "accepted_answer": null, "answers": {"27685": {"line": 19012, "body": "\nThe correct formula for updating the weights between the hidden layer and the output layer is:\n$$\\Delta W_{j,k} = h_k \\ \\cdot \\ o'_{j} \\ \\cdot \\ (o_j - t_j),$$\nwhere $h$ is the activated hidden layer and $o'$ is the derivative of output layer.\nI found this formula in the book Artificial Intelligence: A Guide to Intelligent Systems by Michael Negnevitsky.\n\n", "type": 2, "id": "27685", "date": "2021-05-06T21:24:36.790", "score": 0, "comment_count": 0, "parent_id": "27673"}}}
{"line": 20099, "body": "I have two questions about the structure of attention modules:\nSince I work with imagery I will be talking about using convolutions on feature maps in order to obtain attention maps.\n\nIf we have a set of feature maps with dimensions [B, C, H, W] (batch, channel, height, width), why do we transform our feature maps before we calculate their affinity/correlation in attention mechanisms? What makes this better than simply taking the cosine distance between the feature vectors (e.g. resizing the maps to [B, C, HW] and [B, HW, C] and multiplying them together). Aren't the feature maps already in an appropriate feature/embedding space that we can just use them directly instead of transforming them first?\n\nMost of the time, attention mechanisms will take as input some stack of feature maps (F), and will apply 3 transformations on them to essentially produce a \"query\", \"key\" and \"value\". The query and key will be multiplied together to get the affinity/correlation between a given feature vector and all other feature vectors. In computer vision these transformation will typically be performed by the different 1x1 convolutions. My question is, how come we use 3 different 1x1 convolutions? Wouldn't it make more sense to apply the same 1x1 convolution to the input F? My intuition tells me that since we want to transform/project the feature maps F into some embedding/feature space that it would make the most sense if the \"query\", \"key\" and \"value\" were all obtained by using the same transformation. To illustrate what I mean lets pretend we had a 1x1 feature map and we wanted to see how well the pixel correlates with itself. Obviously it should correlate 100% because it is the same pixel. But wouldn't applying two sets of 1x1 convs to the pixel lead to the chance that the pixel would undergo a different transformation and in the end would have a lower correlation than it should?\n\n\n", "type": 1, "id": "29989", "date": "2021-08-03T08:15:49.330", "score": 1, "comment_count": 3, "tags": ["neural-networks", "machine-learning", "deep-learning", "convolutional-neural-networks", "attention"], "title": "Attention mechanism: Why apply multiple different transformations to obtain query, key, value", "answer_count": 1, "views": 45, "accepted_answer": null, "answers": {"30001": {"line": 20109, "body": "I assume you're talking about this design: (image source)\n\n\nBut wouldn't applying two sets of 1x1 convs to the pixel lead to the chance that the pixel would undergo a different transformation and in the end would have a lower correlation than it should?\n\nYes, that's the point. We are not trying to measure a pixel's correlation with itself. Rather we are trying to allow it to query different related data. We are giving it freedom to change both the data and the queries.\nIt is true that the space for the queries and the keys is the same - but we shouldn't use the same transformation for both, or else each instance of the attention layer is just trying to fetch its own value! Generally the purpose of an attention layer is to query different parts of the input.\nThe first half of your question was essentially \"why do we have a convolution at all?\" and I think this has the same answer: you'd just be able to detect similar pixels, you wouldn't be able to pay attention to noses whenever eyes are detected.\nIt is also true that you could probably skip the convolution on the h(x) input. It looks like this one is somewhat redundant because the convolutions on h(x) and v(x) apply in series - which makes it a two-layer convolution, not quite the same as a one-layer convolution, but perhaps only one layer is needed.\nIt is possible that if you removed the conv layer on either the keys or the queries (but not both) the model would learn to generate the keys directly as the features, but this would hinder it because it would be unable to output any data in the values and queries that wasn't part of the keys (or vice versa). Seems silly. Don't do that.\n", "type": 2, "id": "30001", "date": "2021-08-03T15:03:26.160", "score": 0, "comment_count": 0, "parent_id": "29989"}}}
{"line": 19548, "body": "What are some recent books that introduce AI and neural networks while also discussing the related philosophical issues, like epistemology and whether AI is really thinking, etc.?\n", "type": 1, "id": "28323", "date": "2021-06-18T23:55:54.737", "score": 0, "comment_count": 0, "tags": ["neural-networks", "reference-request", "terminology", "philosophy"], "title": "Is there a recent book that covers the theoretical and philosophical aspects of artificial intelligence?", "answer_count": 3, "views": 64, "accepted_answer": "28370", "answers": {"28540": {"line": 19735, "body": "I would like to add \"The Master Algorithm\" by Pedro Domingos. I would say it's more philosophical but still provides high level discussions about differences between algorithms. He also has a sense of humor which makes it a lighter read.\n", "type": 2, "id": "28540", "date": "2021-07-05T21:18:25.347", "score": 0, "comment_count": 0, "parent_id": "28323"}, "28538": {"line": 19733, "body": "The famous book Artificial Intelligence: A Modern Approach (by Stuart Russell and Peter Norvig) covers all or most of the theoretical aspects of artificial intelligence (such as deep learning) and it also dedicates one chapter to the common philosophical topics that you mention.\n", "type": 2, "id": "28538", "date": "2021-07-05T12:37:07.987", "score": 0, "comment_count": 0, "parent_id": "28323"}, "28370": {"line": 19583, "body": "Deep Learning (2016) by Ian Goodfellow, Yoshua Bengio, & Aaron Courville, introduction:\n\nInventors have long dreamed of creating machines that think. This desire dates\nback to at least the time of ancient Greece. The mythical figures Pygmalion,\nDaedalus, and Hephaestus may all be interpreted as legendary inventors, and\nGalatea, Talos, and Pandora may all be regarded as artificial life\n\n\nIronically, abstract and formal tasks that are among the most difficult mental undertakings for a human being are among the easiest for a computer. Computers have long been able to defeat even the best human chess player but only recently have begun matching some of the abilities of average human beings to recognize objects or speech. A person's everyday life requires an immense amount of knowledge about the world. Much of this knowledge is subjective and intuitive, and therefore difficult to articulate in a formal way. Computers need to capture this same knowledge in order to behave in an intelligent way. One of the key challenges in artificial intelligence is how to get this informal knowledge into a computer.\n\n\nDeep learning has had a long and rich history, but has gone by many names, reflecting different philosophical viewpoints, and has waxed and waned in popularity.\n\n", "type": 2, "id": "28370", "date": "2021-06-22T20:54:35.467", "score": 0, "comment_count": 0, "parent_id": "28323"}}}
{"line": 20843, "body": "Let's say I have two channels that I wish to feed into a CNN. One of the channel contains 4 traces and has a width of 512. Stacking them on top of each other therefore yields an image with dimensions (4, 512). The other channel is just 1 trace, so its dimensions would be (1, 512).\nI then have convolutional filters that are of dimension (1, 5) as an example. That means that the filters run over each trace separately. The first channel (containing the 4 traces) will then have a set of filter weights, shared among the 4 traces. The second channel (containing the 1 trace) will have a completely different set of weights (as per this SE question).\nTLDR: Can convolutional layers in a CNN have different dimensions? Putting this in the context of images: Could we have a CNN that takes an image that has dimensions (100, 100) for the red channel, (100, 100) for the green channel, and (50, 100) for the blue channel?\n", "type": 1, "id": "31859", "date": "2021-09-28T07:21:21.063", "score": 2, "comment_count": 2, "tags": ["convolutional-neural-networks", "filters", "channel"], "title": "Is it possible to have different channel dimensions in a CNN?", "answer_count": 1, "views": 31, "accepted_answer": null, "answers": {"31888": {"line": 20866, "body": "You can do whatever the heck you want.\nOf course you will have to design the data flow through the network so that it can make whatever inferences you intend it to make.\n\nThe first channel (containing the 4 traces) will then have a set of filter weights, shared among the 4 traces. The second channel (containing the 1 trace) will have a completely different set of weights (as per this SE question).\n\nSure, you can do that. No reason why you couldn't. Will it work well? Who knows. Have to try it and see.\nThe best way to combine them will depend on what the NN is supposed to actually do. With the architecture you've described, at least this layer is unable to relate traces to each other. This would be bad when processing, for example, colour images - you don't want to treat red, green and blue the same way as each other, and you want to detect certain combinations of red, green and blue. If you also want the network to have this ability, then maybe you should treat each trace as a channel so the network can see all of them at once.\nAt some point you will obviously have to combine the results together.\n\nCould we have a CNN that takes an image that has dimensions (100, 100) for the red channel, (100, 100) for the green channel, and (50, 100) for the blue channel?\n\nAs I said, it depends on what the network is supposed to do. Are these channels totally separate? Then you can process them with different sized CNNs - or even the same CNN until the dense layers - and combine the results in the dense layers at the end. But if these are the RGB components of one image, you'd be better off just stretching the blue channel so the CNN can recognize colours like yellow.\n", "type": 2, "id": "31888", "date": "2021-09-30T11:20:07.127", "score": 0, "comment_count": 0, "parent_id": "31859"}}}
{"line": 20966, "body": "I've come across two types of neural networks to predict, both from Matlab, the closed structure and the net that removes one delay to find new data.\nFrom Matlab's app generated scripts we see:\n\n% Closed Loop Network\n% Use this network to do multi-step prediction.\n% The function CLOSELOOP replaces the feedback input with a direct\n% connection from the output layer.\n\nnetc = closeloop(net);\nnetc.name = [net.name ' - Closed Loop'];\nview(netc)\n[xc,xic,aic,tc] = preparets(netc,{},{},T);\nyc = netc(xc,xic,aic);\nclosedLoopPerformance = perform(net,tc,yc)\n\n\n% Step-Ahead Prediction Network\n% For some applications it helps to get the prediction a timestep early.\n% The original network returns predicted y(t+1) at the same time it is\n% given y(t+1). For some applications such as decision making, it would\n% help to have predicted y(t+1) once y(t) is available, but before the\n% actual y(t+1) occurs. The network can be made to return its output a\n% timestep early by removing one delay so that its minimal tap delay is now\n% 0 instead of 1. The new network returns the same outputs as the original\n% network, but outputs are shifted left one timestep.\n\nnets = removedelay(net);\nnets.name = [net.name ' - Predict One Step Ahead'];\nview(nets)\n[xs,xis,ais,ts] = preparets(nets,{},{},T);\nys = nets(xs,xis,ais);\nstepAheadPerformance = perform(nets,ts,ys)\n\nMy question is: What is the real difference between them?\nCan one uses them equivalently? If yes, why? I mean, even tho the structure or how they are equipped, could be very very different, e.g. one is apple, the other is grape?\nAs far as I understand both can return new data if one codes them for that. For example, taking the closed net, one can predict 10 new values. Taking the net that removes one delay, one can predict one new value, but if one does this recursively 9 times, one can get the new 10 data. Is there a problem in using this last net in that way?\nOn another side, running both codes, as they are now (this changes depending on the code one works on), yields very different performances. Why?\nUpdate:\nI've checked this page https://www.mathworks.com/matlabcentral/answers/297187-neural-network-closed-loop-vs-open-loop, and in the answer by Greg Heath, we see\n[...]\n\nOPENLOOP: The desired output, AKA the delayed target, is used as an additional input. The OL net will produce output for the common time extent of the input and target.\nCLOSELOOP: The delayed target input is replaced by a direct delayed output connection. The CL net will produce output for the time extent of the input.\n\n[...]\n\"The desired output, AKA the delayed target, is used as an additional input.\" how is this?\n\"The OL net will produce output for the common time extent of the input and target.\" and this?\n\"The CL net will produce output for the time extent of the input.\" What does this mean?\n", "type": 1, "id": "32017", "date": "2021-10-12T00:04:52.660", "score": 3, "comment_count": 0, "tags": ["neural-networks", "comparison", "prediction", "feedforward-neural-networks", "matlab"], "title": "Closed networks vs Networks with a removed delay to predict new data", "answer_count": 1, "views": 73, "accepted_answer": null, "answers": {"32055": {"line": 21003, "body": "\n\n\n\nClosed Loop Network\nStep-Ahead Prediction Network\n\n\n\n\nThe function CLOSELOOP replaces the feedback input with a direct connection from the output layer.\nStep-Ahead Prediction Network also known as removedelay function helps to remove delay to neural network's response\n\n\nIn Closed loop networks, its own predictions become the feedback inputs.\ntargets with a delay were used as feedback input\n\n\nNetwork used to do multi-step prediction\nNetwork used to predict one step ahead\n\n\nHighly helpful to turn the network into parallel configuration\nCan't be used for parallel configuration\n\n\nOutput is not shifted one timestep\nthe output is shifted one timestep.\n\n\nClosed loop network continue to predict when external feedback is missing, by using internal feedback\nOpen-loop and Remove-delay use external feedback\n\n\nReal time Configuration\nNot real time configuration\n\n\nClosed loop network will produce output for the time extent of the input.\nwill produce output for the common time extent of the input and target.\n\n\n\n\n\n\n\n\nOpen-Loop Network\nIn open-loop networks, targets were used as feedback inputs. Open loops are primarily used when future outputs are not known. Below is how open loop network looks like\nNote:\n\nThe typical workflow is to fully create the network in open loop, and only when it has been trained (which includes validation and testing steps) it is transformed to closed loop for multistep-ahead prediction\n\n\nUPDATED Response to newly added questions\n\n\"The desired output, AKA the delayed target, is used as an additional input.\" how is this?\n\nThis is with reference to the Step-Ahead prediction network. Here, the delayed target is used as an additional input. Kindly refer to diagram in point #9. when the target is y(t+1) the additional input is the delayed target, i.e., y(t). Whereas in open-loop target is used as additional input this is the main difference between OL and step ahead prediction.\n\n\"The OL net will produce output for the common time extent of the input and target.\" and this?\n\nAs you can see, the open-loop network  will produce output y(t) as long as open-loop has two inputs x(t)and the targety(t)` for a certain time\n\n\"The CL net will produce output for the time extent of the input.\" What does this mean?\n\nIn closed loop based on the input x(t) the output y(t) will be produced for a certain time\nReference Link:\n\nOfficial Documentation\nDiscussion on Closed Loop(multi step ahead prediction)\nMulti-step ahead prediction using neural networks\nExample of Closed loop, open-loop and remove delay\nMulti-step Neural Network prediction\nDifference between Open loop, Closed loop\nAdditional difference information\n\n", "type": 2, "id": "32055", "date": "2021-10-14T18:34:45.947", "score": 0, "comment_count": 0, "parent_id": "32017"}}}
{"line": 20830, "body": "In the stochastic gradient descent algorithm, the weight update happens for every training sample.\nIn the mini-batch gradient descent algorithm, the weight update happens for every batch of training samples.\nIn the batch gradient descent algorithm, the weight update happens for all samples in the training dataset.\nI am confused with the procedure of training that happens in the mini-batch gradient descent algorithm. I am guessing one of the following two must be correct\n\nPassing each input individually at each layer and calculating the output. This happens for a number of training samples that are equal to batch size.\n\nPassing a batch of inputs at once at each layer and collecting the batch output at each layer.\n\n\nWhich of the above is true in general implementations of mini-batch gradient descent algorithms to train your neural networks?\n", "type": 1, "id": "31845", "date": "2021-09-27T07:57:02.227", "score": 0, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "gradient-descent", "implementation", "mini-batch-gradient-descent"], "title": "In mini-batch gradient descent, do we pass each input in the batch individually or all inputs at the same time through the layer?", "answer_count": 2, "views": 36, "accepted_answer": "31847", "answers": {"31847": {"line": 20832, "body": "In the usual scenario, case 2 occurs.\nIn the deep learning frameworks, Tensors have special dimension (usually corresponding to the 0 axis) which numerates the example in the batch.\nLook for example in the PyTorch documentation of Conv2d or\nTensorflow documentation of Conv2d. The same is true for any Layer - Linear, MultiheadAttention, RNN.\nAll samples from the batch are processed at once, as the integral entity. Most operations process each sample from the batch independently, without a combination of features from $i^{th}$ and $j^{th}$ sample. Linear layer, Convolution doesn't construct linear combinations of inputs, corresponding to different samples.\nHowever, there is an exception - the BatchNormalization Layer which subtracts the mean and divides by the standard deviation.\nHowever, one may want to work with large batches in order to have a less noisy and more precise estimate of the gradient at each step, but the memory usage increases linearly with the batch size, since one has to allocate batch size times the memory, that is required for propagating a single sample thorugh the network. In case, the computational resources do not allow for storing such large batch in a memory, one can pass a part of large batch or even single example, and only then aggregate the results.\nThis situation corresponds to the case 1.\nSuch functionality is implemented in the PyTorch-Lightning library .\n", "type": 2, "id": "31847", "date": "2021-09-27T09:55:10.773", "score": 1, "comment_count": 0, "parent_id": "31845"}, "31846": {"line": 20831, "body": "What happens in mini-batches is not very different from the way updates are made in batch gradient descent, only the number of samples is different. In mini-batch, you process all the data in the batch, and the update happens after that. It is detailed in this video after 6:11.\n", "type": 2, "id": "31846", "date": "2021-09-27T08:54:45.933", "score": 0, "comment_count": 2, "parent_id": "31845"}}}
{"line": 20565, "body": "I'm researching spatio-temporal forecasting utilising GCN as a side project, and I am wondering if I can extend it by using a graph with weighted edges instead of a simple adjacency matrix with 1's and 0's denoting connections between nodes.\nI've simply created a similarity measure and have replaced the 1's and 0's in the adjacency with it.\nFor example, let's take this adjacency matrix\n$$A=\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n$$\nIt would be replaced with the following weighted adjacency matrix\n$$\nA'=\n\\begin{bmatrix}\n0 & 0.8 & 0 \\\\\n0.8 & 0 & 0.3 \\\\\n0 & 0.3 & 0\n\\end{bmatrix}\n$$\nAs I am new to graph NN's, I am wondering whether my intuition checks out. If two nodes have similar time-series, then the weight of the edge between them should be approximately 1, right? If the convolution is performed based on my current weights, will this be incorporated into the learning?\n", "type": 1, "id": "31529", "date": "2021-09-03T16:55:24.417", "score": 1, "comment_count": 5, "tags": ["deep-learning", "geometric-deep-learning", "graph-neural-networks", "graphs"], "title": "Can I extend Graph Convolutional Networks to graphs with weighted edges?", "answer_count": 2, "views": 43, "accepted_answer": null, "answers": {"31563": {"line": 20590, "body": "Graph attention network(GAN) exactly perform the same thing you are referring to . In chebnet, graphsage we have a fixed adjacency matrix that is given to us. Now, in GAN the authors try to learn the adjacency matrix via self-attention mechanism.\nGraph Attention Network:\nLet, $K$ be the number of attention heads, $h^{l+1}_i$ is the feature vector of node $i$ at $l+1$ layer, $e^{l}_{ij}$ is the attention weight between two adjacence node $i$ and $j$ at layer $l$.\n\nThen the update rule for graph attention network is as follows:\n\\begin{align}\n    h^{l+1} &= \\text{Concat}_{K=1}^{K}(\\text{ELU}(\\sum_{j \\in \\mathcal{N}_i} \\underset{\\text{scalar}}{e_{ij}^{K,l}} \\underset{d \\times d}{W_1^{K,l}} \\underset{d \\times 1}{h_j^l})) \n\\end{align}\nwhere the $K$-th head attention weight is defined as:\n\\begin{align}\n\\underset{\\text{scalar}}{e^{K,l}_{ij}} &= \\text{Softmax}_{\\mathcal{N}_i}(\\hat{e}^{K,l}_{ij}) \\\\\n    \\hat{e}^{K,l}_{ij} &= \\text{LeakyRelu}(W_{2}^{K,l} \\text{Concat}(W_1^{K,l}h_i^l, W_1^{k,l}h_j^l))\n\\end{align}\nNotice that in GAN we are learning anisotropic filters(treats each direction differently, since attention weight is different for each direaction) which are more powerful than isotropic filters(treat all the directions). For this reason, GAN are more powerful than isotropic graph convolutional network(GCN).\nSpatio Temporal GCN(ST-GCN)\nIn stgcn, we first perform graph convolution(vanilla GCN or GAN) on the spatial domain then apply temporal convolution along the temporal direction. Here is an example of STGCN for human activity recognition here blurred skeleton indicate time axis.\n\nIn the aforementioned figure, color coding indicates attention weight.\n", "type": 2, "id": "31563", "date": "2021-09-05T20:48:30.193", "score": 0, "comment_count": 0, "parent_id": "31529"}, "31560": {"line": 20587, "body": "According to the definition of Graph Neural Networks taken from here GCN perfroms an operation of the form:\n$$\nf (H^{(l)} ,A) = \\sigma(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)})\n$$\nWhere $H^{(l)}$ is the input to GCN layer, $\\tilde{A} = A + I$ is the adjacency matrix with self loops added and $\\tilde{D}$ is a degree matrix, corresponding to the adjacency matrix $\\tilde{A}$ (on the diagonals there are sums over the columns of $\\tilde{A}$).\nThis definition is for matrix with $1$, if there is an edge between $i$ and $j$, and $0$ otherwise. For matrix of this form normalized Graph Laplacian $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ is\nguaranteed to be positive semidefinite matrix.\nOne can extend the definition for arbitrary values of $a_{ij} \\in A$. But there won't be guarantees, that graph Laplacian will be well-defined.\n", "type": 2, "id": "31560", "date": "2021-09-05T16:54:47.563", "score": 0, "comment_count": 0, "parent_id": "31529"}}}
{"line": 20798, "body": "Consider the following two excerpts from the research paper titled Densely Connected Convolutional Networks by Gao Huang et al.\n#1: From abstract\n\nRecent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the\noutput.\n\n#2: From discussion\n\nOne explanation for the improved accuracy of dense convolutional\nnetworks may be that individual layers receive additional supervision\nfrom the loss function through the shorter connections. One can\ninterpret DenseNets to perform a kind of \"deep supervision\".\n\nBoth excerpts mention the type of connections called shorter connections, especially to the layers that are close to the input and the output layers of the deep convolutional neural network. What does it mean by shorter connections here?\n", "type": 1, "id": "31806", "date": "2021-09-23T01:28:56.843", "score": 0, "comment_count": 0, "tags": ["convolutional-neural-networks"], "title": "What is meant by \"shorter connections\" in the case of deep convolutional neural networks?", "answer_count": 1, "views": 32, "accepted_answer": null, "answers": {"31807": {"line": 20799, "body": "Shorter connections are nothing but concatenating output tensors along the channel axis. Having shorter connections between layers, will help in gradient flow throughout the network easily and hence better training.\n", "type": 2, "id": "31807", "date": "2021-09-23T04:27:09.187", "score": 0, "comment_count": 0, "parent_id": "31806"}}}
{"line": 21047, "body": "I heard from many people about the paper titled Attention Is All You Need by Ashish Vaswani et al.\nWhat actually does the \"attention\" do in simple terms? Is it a function, property, or some other thing?\n", "type": 1, "id": "32110", "date": "2021-10-19T12:05:58.033", "score": 3, "comment_count": 0, "tags": ["neural-networks", "papers", "transformer", "attention"], "title": "In layman terms, what does \"attention\" do in a transformer?", "answer_count": 2, "views": 106, "accepted_answer": null, "answers": {"32112": {"line": 21049, "body": "Let's start by stressing out that in the literature unfortunately the term attention is still used widely without any precise consensus around the technical details, the only constant across papers is that attention should be used when a model is capable of learning, or focusing on local vs global patterns in the data we use for training. And with \"should be used\" I simply refer to the fact that everyone like to feel eligible to write \"Hey, we used attention!\" simply because of the hype generated by the introduction of transformers by Vaswani et al.\nSaid that, I think up to this point the best expression to describe attention is:\nA specific type of architecture\nWhat do I mean by this: Vaswani et al. introduced the expression attention in the paper you cite with a new whole machine learning architecture, namely the transformers. In the paper, attention is used to refer to a specific set of layers, similarly we call residual blocks or dense blocks specific type of layers combinations that were introduced for convolutional neural networks. For me the is no difference at all between attention and the two above mentioned examples. The confusion around the use of this expression in my opinion arose from the fact that Vaswani et al. put a lot of emphasis on the final purpose of the new proposed model, i.e. capturing local similarities within sentences in machine translation.\nOne last consideration why I think that architecture is the best label for attention is that it include also type of attentions that are completely different from the multi-head attention module introduced by Vaswani et al, like architectures that leverage attention maps. Mathematically, attention maps and the multi-head attention module share nothing but the name, still, because conceptually they seems to fulfill the same purpose, we call them both attention, with the consequence that to avoid confusion, one should always refer to a specific paper when talking about attention.\n\n\n", "type": 2, "id": "32112", "date": "2021-10-19T14:05:07.430", "score": 2, "comment_count": 3, "parent_id": "32110"}, "32126": {"line": 21061, "body": "The answer above is very concise but I will try to give an ELI5 example. I also agree with @nbro that attention does not exclusively mean transformer architecture.\n\nBefore attention\nWhat is the height of the youngest female child of the father of your mother's first cousin? That query is convoluted, depends on your good memory of your family tree's relationships. It looks like this:\n$Q_{RNN} = f(Mother(first\\_cousin(father(yongest\\_female(height))))\\ \\ (1)$\nWhat you see in (1) is how Recurrent Neural Networks function, and it is obvious that their performance is inversely correlated with the length of the input.\nAfter attention\nA more efficient way to query the height of that person is by giving it a name - Aligna. Now, whenever you need to get characteristics of that person you don't have to remember it's \"chained\" relationships backwards from the person that is closest to you. This new approach would look like this:\n$Q_{attentive_RNN} = f(Aligna(height))\\ \\ (2)$\nThe latter is what essentially attention pooling does. Attention establishes a direct link with all input steps (e.g. words in an input sentence) so that it can independently pay attention to the input (or set of inputs) that will generate a successful output/prediction.\n", "type": 2, "id": "32126", "date": "2021-10-20T12:45:24.267", "score": 0, "comment_count": 0, "parent_id": "32110"}}}
{"line": 20817, "body": "I am sorry but I have to explain my question using an example, I do not know how to ask it in proper scientific terms.\nLet's assume, I have trained a deep learning model on classifying hand gestures, but training and testing datasets' images are shot only in one lighting conditions and I achieved certain accuracy, let's assume 85%. As far as I understand, adding more data of the same hand gestures images but shot with different lightning should increase my model's \"generalization\" capabilities, right?\nSo the question is, if I take this model, trained in two lightning conditions, and test it only on the dataset of the first lightning conditions, would that increase it's accuracy (the 85%) or maybe this \"generalization\" would only mean that it can now also classify correctly images with different lightning, but not increase the accuracy on the first set?\n", "type": 1, "id": "31831", "date": "2021-09-25T06:27:15.570", "score": 3, "comment_count": 0, "tags": ["neural-networks", "deep-learning", "convolutional-neural-networks", "computer-vision", "generalization"], "title": "How general is generalization?", "answer_count": 2, "views": 64, "accepted_answer": null, "answers": {"31852": {"line": 20837, "body": "Generalization\nIn machine learning, generalization describes a model's ability to properly correct its algorithms to predict new data from the same distribution as the data used to train the model.\nBy providing additional training for your model (on data with varying lighting conditions), you are correct that you would be increasing the capabilities of your model.\nDoes better generalization equal better performance?\nConsider two models and assume they have the same number of coefficients/ layers & nodes:\n\nModel 1, which was trained solely on a single lighting condition\nModel 2, which was trained on multiple lighting conditions\n\nLet's say we have two test sets as well:\n\nTest Set 1 : data with the same lighting condition as the data used to train Model 1\nTest Set 2 : data with the multiple lighting conditions\n\nModel 1 would be expected to have an equal or better performance than Model 2 on Test Set 1, due to \"over\" fitting on that lighting condition, but as you noted, Model 1 would not perform as well on Test Set 2 as on Test Set 1. We also would not expect Model 2 to perform better on Test Set 1 than Model 1 due to the generalizability achieved during training.\nSimply put, you're probably sacrificing some lighting condition-specific accuracy for better accuracy across multiple lighting conditions.\nHowever\nBy allowing your Model 2 to increase the layers & nodes, or coefficients (including interactions), Model 2 may well be capable of performing just as well. All this depends on the size of training sets as well. For instance, if Model 1 is trained on 1,000 data points from the single lighting condition, and Model 2 is trained on 500 data points, Model 1 is generally expected to perform better on Test Set 1.\n", "type": 2, "id": "31852", "date": "2021-09-27T17:25:59.440", "score": 0, "comment_count": 0, "parent_id": "31831"}, "31861": {"line": 20845, "body": "I think there's a crucial point missed in the question, touched by jros answer but without further elaboration.\nIf you train a model on domain A: single lightning condition and test it on domain B: two lightning condition then you're not evaluating generalization but transfer learning capabilities. Or to phrase it differently you're evaluating how close domain A and B are for the model you trained.\nThe test set as you said is truly made of instances never seen by the model during training, but it should nevertheless be representative, i.e. correctly sampled, from the training domain, or from the same distribution as jros wrote. So the generalization of your model, trained on single lightning condition, should be evaluated on single lighting condition as well.\nA final remark about the rest that have been said:\n\neverything holds only under the assumptions that the initial training dataset is not only unbiased but also balanced. In a real case scenario changing the training distribution from something specific (single light condition) to another distribution (multiple light conditions) might well be lead to a worse model, simply cause the problem is now inherently harder to solve.\n\nSo the answer to your question (regarding both, true generalization on same distribution and what you describe, transfer learning) is actually just empirical.\n", "type": 2, "id": "31861", "date": "2021-09-28T08:37:00.000", "score": 1, "comment_count": 3, "parent_id": "31831"}}}
{"line": 19491, "body": "My dataset consists of about 40,000 200x200px grayscale images of centered blobs bathed in noise and occasional artifacts like stripes other blobs of different shapes and sizes, fuzzy speckles and so on in their neighborhood.\nThey are used in a binary classification problem, with emphasis on recall.\nI read that using FFT of image and FFT of the convolutional kernel and multiplying the two, produces a similar result as convolutions would but at a way lower resource expense. This is probably the most straightforward article I found if you need a more detailed description(https://medium.com/analytics-vidhya/fast-cnn-substitution-of-convolution-layers-with-fft-layers-a9ed3bfdc99a)\nWhat I want to do however is simply feed the FFT of images to the standard CNN. The reasoning being, maybe it would be easier for the network to catch on to features that it would miss or tend to weigh less. Or in other words, FFT as a feature engineering technique.\nWould this be an idea worth trying to pursue?\nIf so, any suggestion on which FFT components to extract (Amplitude/Phase, Real/Imaginary)?\n", "type": 1, "id": "28258", "date": "2021-06-14T21:04:39.863", "score": 0, "comment_count": 2, "tags": ["convolutional-neural-networks", "image-processing", "feature-engineering"], "title": "Feeding CNN FFT of an image, a dumb idea?", "answer_count": 1, "views": 241, "accepted_answer": null, "answers": {"28266": {"line": 19498, "body": " FFT is in essence linear transformation of the input image and can be represented by application of convolutional filter of the same size as image on the input.\nProvided, the convolutinoal neural network is deep enough with sufficient number of parameters and there are skip connections (in order to have a path of purely linear transformations on the input), FFT can be represented by the learned filters. If FFT of the image is relevant for the classification problem, NN most probably would learn to produce them in a certain way.\nFor image classification problems - when the goal is identify an instance of something, local information is crucial, and this problem is better solved in the spatial, not frequency domain.\nHowever, for your case it seems, like the semantic is rather trivial, and the goal is to get rid of some frequencies. Hence, working in the frequency domain is a sensible option. Possibly, you can combine the spatial and frequency representation in some way.\nI think, it would be simpler to work with the real and imaginary part, that with the complex abs and phase, since you need to account for periodicity of the phase in a certain way, and then in the end transform phase to $e^{i \\phi}$.\n", "type": 2, "id": "28266", "date": "2021-06-15T11:20:58.250", "score": 0, "comment_count": 0, "parent_id": "28258"}}}
{"line": 20916, "body": "Consider the following excerpt from the abstract of the research paper titled Squeeze-and-Excitation networks by Jie Hu et al.\n\nConvolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding.\n\nThe authors used the term \"spatial encoding\" and the excerpt implies that enhancing spatial encoding has the benefit of increasing the representational power of a convolutional neural network.\nWhat is meant by the term \"spatial encoding\" in this context related to the convolutional neural networks?\n", "type": 1, "id": "31952", "date": "2021-10-07T06:47:48.937", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "terminology", "papers"], "title": "What is meant by \"spatial encoding\" in the context of convolutional neural networks?", "answer_count": 1, "views": 23, "accepted_answer": null, "answers": {"32541": {"line": 21407, "body": "The Convolution Layer processes a certain part of the picture tensors and compresses it to a lower dimension. The spatial encoding adds the information of where the pixels were located in the image. Loosely speaking: It tells you the pixels I just processed were in the top left corner of the image. That way, the classification in the fully-connected layer can additionaly use this information. For more information on CNNs click here.\nMaybe it becomes clearer in text processing. Transformer models for NLP have so-called positional encoding which is the counterpart to spatial encoding in image processing. When the sentence is processed it, the positional encoding tells the index of the word in the whole sequence.\n", "type": 2, "id": "32541", "date": "2021-11-28T18:32:54.357", "score": 0, "comment_count": 0, "parent_id": "31952"}}}
{"line": 21442, "body": "My understanding of how CNN operates in image detection is through the use of kernels that slide through the image to detect features (edges and so on). So a single kernel could potentially be learning to detect an edge no matter where it is in the image. This is great for image recognition problems where an image of a dog shifted to the right or inverted is still an image of a dog. This article states \"the features the kernel learns must be general enough to come from any part of the image\". The article also states how using CNN for categorical data where the order in which data is organised is irrelevant can be \"disastrous\".\nHowever, there are instances where it is desirable for the algorithm to be location-aware in order to classify better. Take the case of using CNN to train a network that will predict card play in the game of bridge (a version of double-dummy where all cards are laid out open - perfect information, deterministic). At the beginning of the game the cards dealt to the four could look (very unrealistically) something like this.\n\nwhere Leader = the player playing the lead card in round 1, and the subsequent players organised as Leader.LeftHandOpponent, Leader.Partner and Leader.RightHandOpponent. Each player's cards are organised in four suits starting from the Trump_Suit and then the other suits in the original suit hierarchy. Cards go from highest value in the top 'A' to lowest value in the bottom '2'.\nHere is a transpose of the image above.\n\nThis layout provides a lot of visual cues in terms of how the gameplay will proceed and who will end up winning how many tricks if viewed it from the perspective of control cards distribution within each suit and hand strength. So, the answer to the question of will CNN actually be able to process this data to provide good predictions is a resounding Yes (at least to me).\nHowever, here is the problem - A regular CNN with a sliding kernel with a (4, 1) stride and no padding would make no distinction between the red boxes when in reality there is a massive difference between them.\nPossible Solution? - A filter consisting of non-sliding kernels/kernels that only slide in one direction (perhaps horizontally or vertically) however would theoretically only seek to learn location-aware features and that could potentially improve accuracy? Just shooting arrow in the sky.\nHas this been researched? Has anybody implemented this already? Could this work?\nP.S: CNN has been used on AlphaGo Zero was great success. Obviously in the game of Go, patterns located in the top of the board carry the same weight as those located in the bottom. The gameplay does not change if the board is flipped 180 degrees. This however is not the case in the game of contract bridge. I am looking at ideas of how this can be resolved.\n", "type": 1, "id": "32583", "date": "2021-12-01T11:24:58.037", "score": 0, "comment_count": 0, "tags": ["machine-learning", "deep-learning", "convolutional-neural-networks"], "title": "Non-sliding kernels for location-aware processing in Convolutional Neural Networks", "answer_count": 1, "views": 25, "accepted_answer": null, "answers": {"32584": {"line": 21443, "body": "As you observed, convolutions are \"shift/translation equivariant\". This is extremely useful and beneficial for image/video/audio processing where this \"symmetry\" exists in the underlying domain.\nThis is not the case in your settings. Each card from each suit carries a different meaning. You actually want to have a different (trainable) weight for each card for each player. A fully connected layer seems more suitable for this setting.\n", "type": 2, "id": "32584", "date": "2021-12-01T12:18:21.840", "score": 0, "comment_count": 0, "parent_id": "32583"}}}
{"line": 21392, "body": "I am building a CNN and am wondering if inputting derived or computed inputs are generally bad for the effectiveness of CNNs? Or just NNs in general?\nBy derived or computed values I mean data that is not \"raw\" and instead is computed based on the raw data. For example, in a very simple form, using time-series data as the \"raw\" data and computing a 30 day SMA as a \"derived/computed\" value, and as another input.\nIs this bad practice at boosting the effectiveness of the network? If it is not a bad practice, are there any tips on what kind of computed values someone should consider when adding new inputs?\nThe goal of my NN is for building predictions in time-series data.\n", "type": 1, "id": "32524", "date": "2021-11-26T14:52:14.983", "score": 1, "comment_count": 0, "tags": ["neural-networks", "machine-learning", "convolutional-neural-networks", "data-preprocessing", "feature-engineering"], "title": "Are derived or computed inputs bad for CNNs?", "answer_count": 1, "views": 32, "accepted_answer": null, "answers": {"32536": {"line": 21402, "body": "It seems to me that, you're basically asking whether feature engineering is bad or not. It's not necessarily bad, but the main advantage of deep neural networks stem from the fact that they do feature engineering for you. The earlier layers learn/extract useful features, and the last layer (usually a fully-connected one), just does some kind of regression on the extracted features.\nAll in all, feature engineering is not necessarily bad, but rather, the deep neural networks do it for you instead. So, they render feature engineering somewhat obsolete. However, if you've rather small amount of data, or using a shallow network for whatever reason, feature engineering can still benefit you a lot.\n", "type": 2, "id": "32536", "date": "2021-11-28T01:26:59.370", "score": 0, "comment_count": 0, "parent_id": "32524"}}}
{"line": 20755, "body": "I am a beginner with DL. I did some tutorials and I know the basics of TensorFlow. But I have a problem understanding how to construct more advanced NNs.\nLet's say I have 6 inputs and a list of 500 names from which you can pick any, but only 6 at the time. The output should be one value between $0.0$ and $1.0$.\nMy question is, how I can handle random order in inputs? In inputs 1-6 you have names ABCDEF and the output score is 0.7. I need the same output if input will be in order CEDBFA. How can I handle this? Should I make random shuffle on inputs during training, or should I make for every output value 500D binary vector-like $[0,0,1,0,1,...,0,1,0,0,0]$, where index position in the array is the corresponding token of name and then feed it in 500 inputs? Or there is some better way?\n", "type": 1, "id": "31753", "date": "2021-09-19T18:49:09.883", "score": 1, "comment_count": 0, "tags": ["neural-networks", "tensorflow", "keras", "input-layer"], "title": "How to handle random order of inputs and get same output?", "answer_count": 1, "views": 29, "accepted_answer": null, "answers": {"31769": {"line": 20769, "body": "Note: This is always going to be an estimate until you actually run the experiment. ML is not always predictable.\nIf order truly does not matter, then I think it will be better to design a network architecture that automatically ignores order, instead of using one that cares about order and then training it to ignore order. If nothing else, less training data will be needed since you don't need to train it on permutations of the input - similar to why CNNs are useful for image recognition.\nOne network architecture could be to have an \"input processing block\" (a group of layers) and an \"output processing block\". First apply the input processing block to each input. Then add together all the outputs of the input processing blocks. Because addition is insensitive to order, this step completely discards any information about the order. Finally, apply the output processing block to the sum of those and the output from that block is your final output.\n           output\n             ^\n             |\n           +--+\n           |NN| (different NN to the one below)\n           +--+\n             ^\n             |\n+---------------------------+\n| add all together          |\n+---------------------------+\n ^    ^    ^    ^    ^    ^\n |    |    |    |    |    |\n+--+ +--+ +--+ +--+ +--+ +--+\n|NN| |NN| |NN| |NN| |NN| |NN| (same NN 6 times)\n+--+ +--+ +--+ +--+ +--+ +--+\n ^    ^    ^    ^    ^    ^\n |    |    |    |    |    |\n C    E    D    B    F    A\n\nThis is just one idea. You are not limited to addition; you can use any commutative operation in the middle. Even something like an attention layer could be used.\n", "type": 2, "id": "31769", "date": "2021-09-20T12:56:21.843", "score": 0, "comment_count": 0, "parent_id": "31753"}}}
{"line": 20092, "body": "In the formula to calculate output shape of tensor after convolution operation\n$$\nW_2 = (W_1-F+2P)/S + 1,\n$$\nwhere:\n\n$W_2$ is the output shape of the tensor\n$W_1$ is the input shape\n$F$ is the filter size\n$P$ is the padding\n$S$ is the stride.\n\nWhy do we add $1$? It gets us to the correct answer, but how is this formula derived?\nSource: https://cs231n.github.io/convolutional-networks/#pool\n", "type": 1, "id": "29982", "date": "2021-08-02T21:49:26.757", "score": 1, "comment_count": 0, "tags": ["convolutional-neural-networks", "convolution", "convolutional-layers", "convolution-arithmetic"], "title": "Why do we add 1 in the formula to calculate the shape of the output of the convolution?", "answer_count": 1, "views": 62, "accepted_answer": "30018", "answers": {"30018": {"line": 20124, "body": "In a few words, we add $1$ to account for the initial position of the kernel.\nYou can easily see this if you let $s = 1$ (unit stride) and $p = 0$ (i.e. no padding), so your formula simplifies to\n\\begin{align}\nW_2 = (W_1 - F) + 1, \\label{1}\\tag{1}\n\\end{align}\nSo, in this simplified case and, for simplicity, assuming squared inputs and kernels, the width (or height) of the output of the convolutional layer is the number of steps that we slide the kernel horizontally (or vertically, respectively), for example, starting from the top left of the input, plus the initial position of the kernel. In this case, $(W_1 - F)$ is the number of times we slide the kernel horizontally (or vertically) and $+1$ is to account for the initial position of the kernel.\nHere's an animation of this simplified case for $W_1 = 4$ and $F = 3$ (the green matrix is the output, while the blue one is the input).\n\nSo, if we apply the formula \\ref{1}, we should get $W_2 = 2$, as we can see from the animation above, which produces a $2 \\times 2$ matrix. In fact, $(4 - 3) + 1 = 2 = W_2$. You can see from the animation that we slide the kernel only once in each of the axes, but the shape of the output matrix is $2 \\times 2$ because we compute the dot product between the kernel and the submatrix of the input that corresponds to the initial position of the kernel.\nA detailed explanation of this (section 2.4, relationship 6) and other simpler formulas to compute the size of the output of a convolutional layer can be found in the report A guide to convolution arithmetic for deep learning, which has also many images that illustrate the concepts (you can find animations here too).\n", "type": 2, "id": "30018", "date": "2021-08-04T13:03:30.183", "score": 0, "comment_count": 0, "parent_id": "29982"}}}
