data_args:
  seq_length: 1024
data_path: data
debug: false
description: 'Group=Test Finetuning on github python | Step=Finetune on the SO Data
  | Ablation=Model: CodeParrot Small'
device: 0
evaluation:
  num_generate_per_step: 200
  remove_input_ids: false
  seq_per_sample: 200
generation:
  do_sample: true
  max_length: 512
  temperature: 0.5
  top_k: 50
  top_p: 0.95
group: SO
hypothesis: ''
is_checkpoint: false
meta:
  ablation: ParrotSmall
  ablation_vals:
    Model: ParrotSmall
  card_name: PythonBaseline.LowLR
  step: PreTrain
metrics:
- exact-match
- bleu
model: gpt2
model_path: null
name: TestGPT2
num_proc: 4
numpy_seed: 2
objective: lm
preprocessors: []
project: so-code-gen
prompts: null
pytorch_seed: 3
save_best_model: false
seed: 1
task:
  buffer_size: 5000
  eval_splits: []
  name: hf_pretrain
  sequence_length: 1024
  train:
    columns_remove:
    - repo_name
    - path
    - copies
    - size
    - content
    - license
    - var_hash
    - doc_hash
    - line_mean
    - line_max
    - alpha_frac
    - autogenerated
    dataset: lvwerra/codeparrot-clean-train
    split: train
    subset: null
    text_key: content
  validation:
    dataset: lvwerra/codeparrot-clean-valid
    max_val_samples: 2500
    split: train
    subset: null
    text_key: content
tracking:
  entity: nyu-code-research
  log_model: true
  project: so-code-gen
  tags:
  - O2
  - Github
  watch: gradients
training:
  batch_size: 32
  dataloader_num_workers: 0
  ddp_find_unused_parameters: false
  disable_tqdm: true
  eval_accumulation_steps: 1
  eval_steps: 100
  evaluation_strategy: steps
  fp16: true
  fp16_backend: apex
  fp16_opt_level: O1
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  greater_is_better: false
  group_by_length: false
  half_precision_backend: apex
  learning_rate:  5.0e-05
  load_best_model_at_end: true
  logging_first_step: true
  logging_steps: 100
  lr_scheduler_type: cosine
  max_grad_norm: 1.0
  max_steps: 2500
  metric_for_best_model: eval_loss
  num_train_epochs: 1
  output_dir: checkpoints
  remove_unused_columns: false
  save_steps: 100
  save_strategy: steps
  save_total_limit: 10
  use_8bit_adam: false
  warmup_ratio: 0.01
  warmup_steps: 100
  weight_decay: 0.1
  xpu_backend: ccl
