data_args:
  seq_length: 1024
data_path: data
debug: false
description: 'Group=Baseline Evaluation of the models finetuned on SO with Only Title
  and Repeating + Only Code Answer + different prompting methods | Step=Finetune on
  the MBPP Data | Ablation=Model: CodeParrot Small, Prompts: Add at "TITLE:" before
  the actual Title'
device: 0
evaluation:
  num_generate_per_step: 200
  remove_input_ids: false
  seq_per_sample: 200
generation:
  do_sample: true
  max_length: 512
  num_return_sequences: 25
  temperature: 0.5
  top_k: 50
  top_p: 0.95
group: MBPP
hypothesis: 'Prompts: Slight Improvement B/C of "TITLE"'
is_checkpoint: false
meta:
  ablation: ParrotSmall.TitleMarker
  ablation_vals:
    Model: ParrotSmall
    Prompts: TitleMarker
  card_name: PromptingTitleCodeRepeat
  step: FineTune
metrics:
- exact-match
- bleu
model: EleutherAI/gpt-neo-125M
model_path: null
name: test_finetune
num_proc: 4
numpy_seed: 2
objective: lm
preprocessors: []
processor:
  name: stackoverflow
  params:
    allow_negative_best_answer: false
    allow_no_answer: false
    answer_sorting: accepted
    answers_per_sample: -1
    bad_answer_cutoff: -1
    date_format_str: '%Y-%m'
    good_answer_cutoff: 3
    highest_is_best: false
    no_answer_str: There is not an answer
    remove_modality: NL
    repeat_body_for_each_answer: false
    repeat_prompt_each_answer: true
    top_answer_cutoff: 12
    worst_is_best: false
    wrap_answer_character: None
project: so-code-gen
pytorch_seed: 3
raw_dump_path: data/dumps
save_best_model: false
seed: 1
task:
  dump_name: python_ParrotSmall.TitleMarker
  eval_splits:
  - test
  - validation
  metrics:
  - exact-match
  - bleu
  name: mbpp
  postprocessors:
  - split:
      split_phrase: '#Solution:'
  preprocessors:
  - add-prefix:
      prefix: 'You are an expert Python programmer, and here is your task: '
  raw_dump_name: python
  tensorized_name: python_ParrotSmall.TitleMarker
tracking:
  entity: nyu-code-research
  log_model: false
  project: so-code-gen
  tags:
  - O2
  - Prompting
  watch: gradients
training:
  batch_size: 1
  dataloader_num_workers: 0
  ddp_find_unused_parameters: false
  disable_tqdm: true
  eval_accumulation_steps: 1
  eval_steps: 50
  dataloader_drop_last: False
  evaluation_strategy: steps
  fp16: true
  fp16_backend: apex
  fp16_opt_level: O2
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  greater_is_better: false
  group_by_length: false
  half_precision_backend: apex
  learning_rate: 5.0e-06
  load_best_model_at_end: false
  logging_first_step: true
  logging_steps: 10
  lr_scheduler_type: linear
  max_grad_norm: 1.0
  metric_for_best_model: eval_loss
  num_train_epochs: 32
  output_dir: models
  remove_unused_columns: false
  save_steps: 50
  max_steps: 1000
  save_strategy: steps
  save_total_limit: 2
  use_8bit_adam: false
  warmup_ratio: 0.05
  weight_decay: 0.1
  xpu_backend: ccl
