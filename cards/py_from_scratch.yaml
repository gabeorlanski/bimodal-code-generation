starting_commands: templates/commands/sbatch_experiment.txt
experiments:
  PyFromScratch:
    description: "From Scratch Training Python Model"
    overrides:
      ++device: 0
      is_checkpoint: False
      objective: lm
      ++tracking.tags:
        - Github
      ++evaluation:
        num_generate_per_step: 200
        remove_input_ids: False
        seq_per_sample: 200

      ++task:
        train:
          dataset: lvwerra/codeparrot-clean-train
          text_key: content
          columns_remove:
            - repo_name
            - path
            - copies
            - size
            - content
            - license
            - var_hash
            - doc_hash
            - line_mean
            - line_max
            - alpha_frac
            - autogenerated
          split: train
        validation:
          dataset: lvwerra/codeparrot-clean-valid
          text_key: content
          split: train
          max_val_samples: 5000
    ablations:
      - Model:
          Neo125M:
            description: GPT Neo 125M
            overrides:
              ++model: 'EleutherAI/gpt-neo-125M'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
                ++training.gradient_checkpointing: True
                ++training.fp16_opt_level: "O0"
    steps:
      - name: PreTrain
        description: Finetune on the SO Data
        add_name: False
        base: pretrain
        group: PYTHON
        overrides:
          ++from_scratch: True
          ++task.buffer_size: 5000
          tracking:
            log_model: True
          is_checkpoint: False
          ++task.sequence_length: 1024
          training:
            dataloader_num_workers: 16
            fp16_opt_level: O0
            learning_rate: 5.0e-05
            warmup_steps: 750
            eval_steps: 500
            lr_scheduler_type: cosine

          task: hf_pretrain

    command:
      file: templates/commands/pretrain.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
        pretrain_time: "48:00:00"
        use_cds: True
      fields:
        - model_path