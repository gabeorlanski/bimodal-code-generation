starting_commands: command_templates/sbatch_experiment.txt
experiments:
  HumanEval:
    add_name: False
    overrides:
      batch_size: 200
      remove_input_ids: true
      ++generation:
        max_new_tokens: 256
      ++device: 0
      +splits:
        - validation
    ablations:
      - DumpName:
          CodeParrotSmall:
            model_path: best_models/MBPP.CodeParrotSmall
            ++generation:
              top_p: 0.95
          CodeParrotSmall.Replication:
            model_path: best_models/MBPP.CodeParrotSmall
            ++generation:
              top_p: 0.95
              top_k: 0
              temperature: 0.2
          CodeParrotSmall.HighTemp:
            model_path: best_models/MBPP.CodeParrotSmall
            ++generation:
              top_p: 0.95
              top_k: 50
              temperature: 0.8
          CodeParrotSmall.NoTopK:
            model_path: best_models/MBPP.CodeParrotSmall
            ++generation:
              top_p: 0.95
              top_k: 0
              temperature: 0.8
    steps:
      - name: HumanEval
        add_name: False
        base: eval_config
        group: Baseline
        overrides:
          is_checkpoint: false
          task: human_eval
          ++tracking:
            force_name: true
            ++model: 'lvwerra/codeparrot-small'
      - name: Eval
        add_name: False
        base: eval_config
        group: MBPP
        overrides:
          task: mbpp
    command:
      file: command_templates/code_evaluate.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
      fields:
        - model_path