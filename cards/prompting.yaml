starting_commands: templates/commands/sbatch_experiment.txt
experiments:
  PromptingTitleCodeRepeat:
    description: "Test SO finetuning with Only Title and Repeating + Only Code Answer 
    + different prompting methods"
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++tracking.tags:
        - O2
        - Prompting
      ++evaluation:
        num_generate_per_step: 200
        remove_input_ids: False
        seq_per_sample: 200
      ++task.dump_name: python_${..__META__.ablation.name}
      ++task.raw_dump_name: python
      ++task.tensorized_name: python_${..__META__.ablation.name}
      ++processor.params.repeat_prompt_each_answer: True
      ++processor.params.repeat_body_for_each_answer: False
      ++prompts:
        file: templates/so_prompts.yaml
      ++processor.params.remove_modality: "NL"
      ++processor.params.relative_quality: True
    ablations:
      - Model:
          Neo125M:
            description: GPT Neo 125M
            overrides:
              ++model: 'EleutherAI/gpt-neo-125M'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
          ParrotSmall:
            description: CodeParrot Small
            overrides:
              ++model: 'lvwerra/codeparrot-small'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
                ++training.gradient_checkpointing: True
      - Prompts:
#          TitleMarker:
#            description: Add at "TITLE:" before the actual Title
#            hypothesis: Slight Improvement B/C of "TITLE"
#            prompts.pipe:
#              - title
#          Baseline:
#            description: Only the Title
#            hypothesis: Baseline
#            prompts.pipe:
#              - base
#          QualityAfter:
#            description: The Title followed by a quality adjective based on the answers score
#            hypothesis: Slight Improvement B/C of "TITLE"
#            prompts.pipe:
#              - quality_after
#          AnswerYearMonth:
#            description: The year and month of the answer followed by the title
#            hypothesis: 'Should improve results as it handles the issue of deprecated code,
#            should also be better than the question date'
#            prompts.pipe:
#              - answer_date_before
#            ++processor.params.date_format_str: "%Y-%m"
#          QuestionYearMonth:
#            description: The year and month of the question followed by the title
#            hypothesis: 'Should improve results as it handles the issue of deprecated code,
#            should also be worse than the answer date'
#            prompts.pipe:
#              - question_date_before
#            ++processor.params.date_format_str: "%Y-%m"
          QualityBefore:
            description: The quality adjective based on the answers score followed by the title
            hypothesis: Slight Improvement B/C of The Relative Quality
            prompts.pipe:
              - quality_before
    steps:
      - name: PreTrain
        description: Finetune on the SO Data
        add_name: False
        base: pretrain
        group: SO
        overrides:
          ++task.buffer_size: 5000
          tracking:
            log_model: True
          is_checkpoint: False
          ++task.sequence_length: 1024
          task: so
    command:
      file: templates/commands/pretrain.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
        pretrain_time: "23:00:00"
        use_cds: True
      fields:
        - model_path