starting_commands: command_templates/sbatch_experiment.txt
experiments:
  ConditonEval:
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++task.dump_name: python_${..__META__.ablation.name}
      ++task.raw_dump_name: python
      ++task.tensorized_name: python_${..__META__.ablation.name}
      ++tracking.tags:
        - Torchrun
        - GradientCheckpointing
        - AdamW
        - O2
      ++batch_size: 200
      ++model: 'lvwerra/codeparrot-small'
      ++conditioning_tags:
        - python
        - dictionary
      ++prompt_template: data/prompt_templates/conditioning.txt
    steps:
      - name: HumanEval
        add_name: False
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: false
          ++generation:
            max_new_tokens: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models
          batch_size: 200
#    command:
#      file: command_templates/evaluate_model.txt
#      kwargs:
#        num_return_sequences: 200
#        task_name: MBPP
#        train_sbatch: train_single_gpu
#        pretrain_time: "27:59:59"
#        use_cds: True
#      fields:
#        - model_path