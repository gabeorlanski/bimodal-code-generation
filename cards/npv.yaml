starting_commands: templates/commands/sbatch_experiment.txt
experiments:
  Finetune:
    add_name: True
    description: "Fine tune the baseline models on NPV"
    overrides:
      ++device: 0
      objective: lm
      tracking:
        tags:
          - Baseline
      task: npv
      ++tracking.entity: nyu-code-research
      ++task.params.prompt: base_with_description
    ablations:
      - Model:
          ParrotSmall:
            description: CodeParrot Small
            ++model: 'lvwerra/codeparrot-small'
            ++training.batch_size: 8
          Neo125M:
            description: GPT Neo 125M
            ++model: 'EleutherAI/gpt-neo-125M'
            ++training.batch_size: 8
          Parrot:
            description: CodeParrot Normal
            ++model: 'lvwerra/codeparrot'
            ++training.batch_size: 2
            ++training.gradient_accumulation_steps: 4
          Neo1B:
            description: GPT Neo 1.3B
            ++model: 'EleutherAI/gpt-neo-1.3B'
            ++training.batch_size: 2
            ++training.gradient_accumulation_steps: 4
          Incoder1B:
            description: Incoder 1B
            ++model: 'facebook/incoder-1B'
            ++training.batch_size: 2
            ++training.gradient_accumulation_steps: 4
#          Incoder6B:
#            description: Incoder 6B
#            ++model: 'facebook/incoder-6B'
#            ++evaluation.num_generate_per_step: 4

    steps:
      - name: FineTune
        add_name: False
        description: Finetune on the NPV Data
        base: mbpp_finetune
        group: NPV
        overrides:
          task: npv
          ++is_checkpoint: false
          ++model_path: null
          ++training:
            evaluation_strategy: epoch
            save_strategy: epoch
            num_train_epochs: 10
            max_steps: -1
            logging_steps: 10
            eval_steps: 25
            warmup_steps: 25
            save_steps: 25
          tracking:
            log_model: False
            watch: null
      - name: Eval
        description: Eval on NPV
        add_name: True
        base: train_config.yaml
        group: NPV
        overrides:
          is_checkpoint: True
          model_path: outputs/npv/${..__META__.ablation.name}/best_model



    command:
      file: templates/commands/npv_finetune.txt
      kwargs:
        train_sbatch: train_single_gpu
        pretrain_time: "12:00:00"
        finetune_time: "4:00:00"
        use_cds: True
      fields:
        - model_path