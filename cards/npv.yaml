starting_commands: templates/commands/sbatch_experiment.txt
experiments:
  Finetune:
    add_name: True
    description: "Fine tune the baseline models on NPV"
    overrides:
      ++device: 0
      objective: lm
      tracking:
        tags:
          - Baseline
      task: npv
      ++tracking.entity: nyu-code-research
      ++task.params.prompt: nl_prompt
    ablations:
      - Model:
          ParrotSmall:
            description: CodeParrot Small
            ++model: 'lvwerra/codeparrot-small'
            ++training.batch_size: 8
            ++evaluation.num_generate_per_step: 48
          Neo125M:
            description: GPT Neo 125M
            ++model: 'EleutherAI/gpt-neo-125M'
            ++training.batch_size: 8
            ++evaluation.num_generate_per_step: 40
          Parrot:
            description: CodeParrot Normal
            ++model: 'lvwerra/codeparrot'
            ++training.batch_size: 2
            ++training.gradient_accumulation_steps: 4
            ++training.gradient_checkpointing: True
            ++evaluation.num_generate_per_step: 24
          Neo1B:
            description: GPT Neo 1.3B
            ++model: 'EleutherAI/gpt-neo-1.3B'
            ++training.batch_size: 4
            ++training.gradient_accumulation_steps: 2
            ++training.gradient_checkpointing: True
            ++evaluation.num_generate_per_step: 18
          Incoder1B:
            description: Incoder 1B
            ++model: 'facebook/incoder-1B'
            ++training.batch_size: 4
            ++training.gradient_accumulation_steps: 2
            ++training.gradient_checkpointing: True
            ++evaluation.num_generate_per_step: 16

    steps:
      - name: FineTune
        add_name: False
        description: Finetune on the NPV Data
        base: mbpp_finetune
        group: NPV
        overrides:
          task: npv
          ++is_checkpoint: false
          ++model_path: null
          ++training:
            evaluation_strategy: steps
            save_strategy: steps
            max_steps: 2500
            logging_steps: 5
            eval_steps: 25
            warmup_steps: 250
            save_steps: 25
            learning_rate: 1e-6
            lr_scheduler_type: cosine
            greater_is_better: True
            metric_for_best_model: True
          tracking:
            log_model: False
            watch: null
      - name: Eval
        description: Eval on NPV
        add_name: True
        base: train_config.yaml
        group: NPV
        overrides:
          is_checkpoint: True
          model_path: outputs/npv/${..__META__.ablation.name}/best_model



    command:
      file: templates/commands/npv_finetune.txt
      kwargs:
        train_sbatch: train_single_gpu
        pretrain_time: "12:00:00"
        finetune_time: "12:00:00"
        use_cds: True
      fields:
        - model_path