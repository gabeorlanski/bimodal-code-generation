starting_commands: command_templates/sbatch_experiment.txt
experiments:
  FinetuneSO.ParrotSmall:
    overrides:
      ++device: 0
      ++model: 'lvwerra/codeparrot-small'
      ++training:
        batch_size: 16
        gradient_accumulation_steps: 4
        learning_rate: 1e-4
        save_steps: 2500
        eval_steps: 500
        max_steps: 10000
        warmup_steps: 500
        logging_steps: 25
        lr_scheduler_type: linear
      task: so
      +processor: stackoverflow
      ++raw_dump_path: data/dumps
    ablations:
      - DumpName:
          Python:
            ++task.dump_name: python
            ++raw_dump_name: python
            ++tensorized_name: python
          PythonTitleRepeat:
            ++task.dump_name: python_title
            ++raw_dump_name: python
            ++tensorized_name: python_title
            ++processor.params.repeat_question_for_each_answer: 'title'
          PythonFullRepeat:
            ++task.dump_name: python_full
            ++raw_dump_name: python
            ++tensorized_name: python_full
            ++processor.params.repeat_question_for_each_answer: 'full'
          Random:
            ++task.dump_name: random
            ++raw_dump_name: random
            ++tensorized_name: random
          DB:
            ++task.dump_name: db
            ++raw_dump_name: db
            ++tensorized_name: db
    steps:
      - name: PreTrain
        add_name: False
        base: pretrain_with_tensorized
        group: SO
        overrides:
          ++tensorized_path: data/tensorized
          ++task.data_path: data/tensorized
          ++processor.params.clean: True
          objective: lm
          +tensorize_batch_size: 64
          tracking:
            log_model: true
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          objective: 'lm'
          task: mbpp
          ++is_checkpoint: True
          ++model_path: best_models/${..__META__.previous_step.save_name}
          training:
            learning_rate: 5e-5
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 200
          remove_input_ids: true
          ++generation:
            max_new_tokens: 256
          ++device: 0
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.previous_step.save_name}
    command:
      file: command_templates/deepspeed_pretrain.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
      fields:
        - model_path
