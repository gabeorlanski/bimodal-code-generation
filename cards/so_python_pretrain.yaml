starting_commands: command_templates/sbatch_experiment.txt
experiments:
  Neo.SO:
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++processor.params.repeat_question_for_each_answer: 'title'
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++task.dump_name: python_title
      ++task.raw_dump_name: python
      ++tensorized_name: python_title
    ablations:
      - Model:
          Small:
            ++model: 'EleutherAI/gpt-neo-125M'
    steps:
      - name: PreTrain
        add_name: False
        base: pretrain_with_tensorized
        group: SO
        overrides:
          ++task.buffer_size: 25
          ++tensorized_path: data/ds_info
          ++task.data_path: data/ds_info
          ++processor.params.clean: True
          +tensorize_batch_size: 64
          tracking:
            log_model: True
          is_checkpoint: False
          ++training:
            batch_size: 10
            gradient_accumulation_steps: 4
            learning_rate: 1e-4
            save_steps: 2500
            eval_steps: 500
            max_steps: 10000
            warmup_steps: 500
            logging_steps: 25
            lr_scheduler_type: linear
            deepspeed: ds_config.json
            save_total_limit: 5
          ++task.sequence_length: 1024
          task: so
      - name: HumanEval
        add_name: False
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_new_tokens: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.PreTrain.save_name}
          batch_size: 200
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          objective: 'lm'
          task: mbpp
          ++is_checkpoint: True
          ++model_path: best_models/${..__META__.PreTrain.save_name}
          ++training:
            batch_size: 32
            gradient_accumulation_steps: 1
            learning_rate: 5e-5
      - name: HEFineTune
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_new_tokens: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.FineTune.save_name}
          batch_size: 200
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 200
          remove_input_ids: true
          ++generation:
            max_new_tokens: 256
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.FineTune.save_name}
    command:
      file: command_templates/deepspeed_pretrain.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
      fields:
        - model_path

  FineTuneSO.ParrotSmall512:
    overrides:
      ++device: 0
      ++model: 'lvwerra/codeparrot-small'
      +processor: stackoverflow
      ++processor.params.repeat_question_for_each_answer: 'title'
      ++raw_dump_path: data/dumps
      is_checkpoint: true
    ablations:
      - DumpName:
          PythonTitleShuffle:
            ++task.dump_name: python_title
            ++raw_dump_name: python
            ++tensorized_name: python_title
    steps:
      - name: PreTrain
        add_name: False
        base: pretrain_with_tensorized
        group: SO
        overrides:
          ++buffer_size: 4
          ++tensorized_path: data/tensorized
          ++task.data_path: data/tensorized
          ++processor.params.clean: True
          objective: lm
          +tensorize_batch_size: 64
          tracking:
            log_model: True
          is_checkpoint: False
          ++training:
            batch_size: 32
            gradient_accumulation_steps: 4
            learning_rate: 1e-4
            save_steps: 2500
            eval_steps: 500
            max_steps: 10000
            warmup_steps: 500
            logging_steps: 25
            lr_scheduler_type: linear
            deepspeed: ds_512.json
            save_total_limit: 5
          ++task.sequence_length: 512
          ++data_args.seq_length: 512
          task: so

      - name: HumanEval
        add_name: False
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_new_tokens: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.PreTrain.save_name}
          batch_size: 200
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          objective: 'lm'
          task: mbpp
          ++is_checkpoint: True
          ++model_path: best_models/${..__META__.PreTrain.save_name}
          ++training:
            batch_size: 32
            gradient_accumulation_steps: 1
            learning_rate: 5e-5
      - name: HEFineTune
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_new_tokens: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.FineTune.save_name}
          batch_size: 200
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 200
          remove_input_ids: true
          ++generation:
            max_new_tokens: 256
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.FineTune.save_name}
#    command:
#      file: command_templates/deepspeed_pretrain.txt
#      kwargs:
#        num_return_sequences: 200
#        task_name: MBPP
#        train_sbatch: train_single_gpu
#      fields:
#        - model_path
