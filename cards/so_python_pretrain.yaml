starting_commands: command_templates/sbatch_experiment.txt
experiments:
  PretrainRaw.ParrotSmall:
    overrides:
      ++device: 0
      +disable_cache: True
      ++tracking:
        log_model: True
      ++model: 'lvwerra/codeparrot-small'
      ++num_proc: 8
      ++training:
        batch_size: 16
        gradient_accumulation_steps: 4
        deepspeed: ds_config.json
      ++task.data_name: raw_python_so
      ++task.sequence_length: 1024
    steps:
      - name: PreTrain
        base: pretrain_config
        group: SO
        overrides:
          objective: 'lm'
          ++training:
            save_steps: 2500
            eval_steps: 250
            logging_steps: 100
            max_steps: 10000
            warmup_steps: 500
          tracking:
            log_model: true
          task: tensorize
    command:
      file: command_templates/deepspeed_pretrain.txt
      fields:
        - model_path