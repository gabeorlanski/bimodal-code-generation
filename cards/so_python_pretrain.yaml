starting_commands: command_templates/sbatch_experiment.txt
experiments:
  Neo.SO:
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++processor.params.repeat_question_for_each_answer: 'none'
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++task.dump_name: python_norepeat
      ++task.raw_dump_name: python
      ++task.tensorized_name: python_norepeat
    ablations:
      - Model:
          Small:
            ++model: 'EleutherAI/gpt-neo-125M'
    steps:
      - name: PreTrain
        add_name: False
        base: pretrain_with_tensorized
        group: SO
        overrides:
          ++task.buffer_size: 25
          ++processor.params.clean: True
          ++tensorize_batch_size: 64
          tracking:
            log_model: True
          is_checkpoint: False
          ++training:
            batch_size: 8
            gradient_accumulation_steps: 8
            learning_rate: 1e-4
            save_steps: 1000
            eval_steps: 500
            max_steps: 10000
            warmup_steps: 500
            logging_steps: 25
            lr_scheduler_type: linear
            deepspeed: deepspeed_configs/neo.json
            save_total_limit: 10
            group_by_length: False
            dataloader_num_workers: 4
            half_precision_backend: 'apex'
          ++task.sequence_length: 1024
          task: so
      - name: HumanEval
        add_name: False
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_length: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.PreTrain.save_name}
          batch_size: 200
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: True
          ++model_path: best_models/${..__META__.PreTrain.save_name}
          ++training:
            batch_size: 32
            gradient_accumulation_steps: 1
            learning_rate: 5e-5
          tracking:
            log_model: False
      - name: HEFineTune
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_length: 512
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.FineTune.save_name}
          batch_size: 100
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 100
          remove_input_ids: true
          ++generation:
            max_length: 512
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.FineTune.save_name}
    command:
      file: command_templates/deepspeed_pretrain.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
      fields:
        - model_path
        -
  512Genlen.Parrot:
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++task.raw_dump_name: python
      ++model: lvwerra/codeparrot-small
    ablations:
      - ModelName:
          Python:
            ++task.dump_name: python_norepeat
            ++task.raw_dump_name: python
            ++task.tensorized_name: python_norepeat
            ++model_path: outputs/mbpp/FinetuneSO.ParrotSmall.Python.FineTune/models/checkpoint-35
          PythonFullRepeat:
            ++task.dump_name: python_fullrepeat
            ++task.raw_dump_name: python
            ++task.tensorized_name: python_fullrepeat
            ++model_path: outputs/mbpp/FinetuneSO.ParrotSmall.PythonFullRepeat.FineTune/models/checkpoint-35
          PythonNoNL:
            ++task.dump_name: python_nonl_title_repeat
            ++task.raw_dump_name: python
            ++task.tensorized_name: python_nonl_title_repeat
            ++model_path: outputs/mbpp/FinetuneSO.ParrotSmall.PythonNoNL.FineTune/models/checkpoint-35
    steps:
      - name: PreTrain
        add_name: False
        base: pretrain_with_tensorized
        group: SO
        overrides:
          ++task.buffer_size: 25
          ++processor.params.clean: True
          ++tensorize_batch_size: 64
          tracking:
            log_model: True
          is_checkpoint: False
          ++training:
            batch_size: 8
            gradient_accumulation_steps: 8
            learning_rate: 1e-4
            save_steps: 1000
            eval_steps: 500
            max_steps: 10000
            warmup_steps: 500
            logging_steps: 25
            lr_scheduler_type: linear
            deepspeed: deepspeed_configs/neo.json
            save_total_limit: 10
            group_by_length: False
            dataloader_num_workers: 4
            half_precision_backend: 'apex'
          ++task.sequence_length: 1024
          task: so
      - name: HumanEval
        add_name: False
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_length: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.PreTrain.save_name}
          batch_size: 200
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: True
          ++training:
            batch_size: 32
            gradient_accumulation_steps: 1
            learning_rate: 5e-5
          tracking:
            log_model: False
      - name: HEFineTune
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_length: 512
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.FineTune.save_name}
          batch_size: 100
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 100
          remove_input_ids: true
          ++generation:
            max_length: 512
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.FineTune.save_name}
    command:
      file: command_templates/mbpp_eval.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
      fields:
        - model_path