starting_commands: templates/commands/sbatch_experiment.txt
experiments:
  HighQuality:
    description: "Test Finetuning on a filtered down set of python questions that should be higher quality"
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++tracking.tags:
        - O2
        - Prompting
        - PythonHQ
      ++evaluation:
        num_generate_per_step: 200
        remove_input_ids: False
        seq_per_sample: 200
      ++task.dump_name: python_hq_${..__META__.ablation.name}
      ++task.raw_dump_name: python_hq
      ++task.tensorized_name: python_hq_${..__META__.ablation.name}
      ++processor.params.repeat_prompt_each_answer: True
      ++processor.params.repeat_body_for_each_answer: False
      ++prompts:
        file: templates/so_prompts.yaml
        disable_prompts: True
      ++processor.params.remove_modality: "NL"
    ablations:
      - Model:
#          Neo125M:
#            description: GPT Neo 125M
#            overrides:
#              ++model: 'EleutherAI/gpt-neo-125M'
#            step_overrides:
#              PreTrain:
#                ++training.batch_size: 32
#                ++training.gradient_accumulation_steps: 1
          ParrotSmall:
            description: CodeParrot Small
            overrides:
              ++model: 'lvwerra/codeparrot-small'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
                ++training.gradient_checkpointing: True
      - Prompts:
          Baseline:
            description: Only the Title
            hypothesis: Baseline
            prompts.pipe:
              - base
          QualityAfter:
            description: The Title followed by a quality adjective based on the answers score
            hypothesis: Slight Improvement B/C of the quality
            prompts.pipe:
              - quality_after
    steps:
      - name: PreTrain
        description: Finetune on the SO Data
        add_name: False
        base: pretrain
        group: SO
        overrides:
          ++task.buffer_size: 5000
          tracking:
            log_model: True
          is_checkpoint: False
          ++task.sequence_length: 1024
          task: so
      - name: FineTune
        add_name: False
        description: Finetune on the MBPP Data
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: True
          ++model_path: outputs/so/SO.HighQuality.${..__META__.ablation.Model}/best_model
          tracking:
            log_model: False
            watch: null
      - name: Eval
        description: Evaluate the model
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: true
          ++model_path: outputs/mbpp/MBPP.HighQuality.${..__META__.ablation.Model}/best_model
          tracking:
            log_model: False
            watch: null
    command:
      file: templates/commands/evaluate_model.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
        finetune_time: "12:00:00"
        use_cds: True
      fields:
        - model_path
  HighQuality.QualityPrompt:
    description: "Test Finetuning on a filtered down set of python questions that should be higher quality"
    overrides:
      ++device: 0
      +processor: stackoverflow
      ++raw_dump_path: data/dumps
      is_checkpoint: true
      objective: lm
      ++tracking.tags:
        - O2
        - Prompting
        - PythonHQ
      ++evaluation:
        num_generate_per_step: 200
        remove_input_ids: False
        seq_per_sample: 200
      ++task.dump_name: python_hq_${..__META__.ablation.name}
      ++task.raw_dump_name: python_hq
      ++task.tensorized_name: python_hq_${..__META__.ablation.name}
      ++processor.params.repeat_prompt_each_answer: True
      ++processor.params.repeat_body_for_each_answer: False
      ++prompts:
        file: templates/so_prompts.yaml
      ++processor.params.remove_modality: "NL"
    ablations:
      - Model:
#          Neo125M:
#            description: GPT Neo 125M
#            overrides:
#              ++model: 'EleutherAI/gpt-neo-125M'
#            step_overrides:
#              PreTrain:
#                ++training.batch_size: 32
#                ++training.gradient_accumulation_steps: 1
          ParrotSmall:
            description: CodeParrot Small
            overrides:
              ++model: 'lvwerra/codeparrot-small'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
                ++training.gradient_checkpointing: True
      - Prompts:
          QualityAfter:
            description: The Title followed by a quality adjective based on the answers score
            hypothesis: Slight Improvement B/C of the quality
            prompts.pipe:
              - quality_after

      - QualityAdjective:
          Best:
            description: The 'BEST' adjective, should in theory do the best
            ++prompts.params:
              quality: BEST
          Bad:
            description: The 'BAD' adjective, should in theory do the worst
            ++prompts.params:
              quality: BAD
          Worst:
            description: The 'WORST' adjective, should have no effect b/c it was not used in training
            ++prompts.params:
              quality: WORST
          Ok:
            description: The 'OK' adjective, should have minimal effect b/c is neutral
            ++prompts.params:
              quality: OK
          Good:
            description: The 'GOOD' adjective, should have slight performance improvement
            ++prompts.params:
              quality: GOOD
    steps:
      - name: PreTrain
        description: Finetune on the SO Data
        add_name: False
        base: pretrain
        group: SO
        overrides:
          ++task.buffer_size: 5000
          tracking:
            log_model: True
          is_checkpoint: False
          ++task.sequence_length: 1024
          task: so
      - name: FineTune
        add_name: False
        description: Finetune on the MBPP Data
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: True
          ++model_path: outputs/so/HighQuality.ParrotSmall.QualityAfter/checkpoints/checkpoint-5000
          tracking:
            log_model: False
            watch: null
      - name: Eval
        description: Evaluate the model
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: true
          ++model_path: outputs/mbpp/HighQuality.QualityPrompt.${..__META__.ablation.name}/best_model
          tracking:
            log_model: False
            watch: null
    command:
      file: templates/commands/mbpp_eval.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
        finetune_time: "12:00:00"
        use_cds: True
      fields:
        - model_path