starting_commands: templates/commands/sbatch_experiment.txt
experiments:
  Baselines.100MBPP.WSolution:
    add_name: True
    description: "Evaluate the baseline models with the updated MBPP loop"
    overrides:
      ++device: 0
      is_checkpoint: true
      objective: lm
      ++evaluation:
        num_generate_per_step: 200
        remove_input_ids: True
        seq_per_sample: 200
      ++raw_dump_path: data/dumps
      ++task.dump_name: python_${..__META__.ablation.name}
      ++task.raw_dump_name: python
      ++task.tensorized_name: python_${..__META__.ablation.name}
      tracking:
        tags:
          - Baseline
    ablations:
      - Model:
          Neo125M:
            description: GPT Neo 125M
            overrides:
              ++model: 'EleutherAI/gpt-neo-125M'
              ++training.gradient_checkpointing: True
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
          ParrotSmall:
            description: CodeParrot Small
            overrides:
              ++model: 'lvwerra/codeparrot-small'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
                ++training.gradient_checkpointing: True
    steps:
      - name: PreTrain
        description: Finetune on the SO Data
        add_name: False
        base: pretrain
        group: SO
        overrides:
          ++task.buffer_size: 5000
          tracking:
            log_model: True
          is_checkpoint: False
          ++task.sequence_length: 1024
          task: so
      - name: HumanEval
        add_name: False
        description: Evaluate the model
        base: mbpp_finetune
        group: HUMAN_EVAL
        overrides:
          task: human_eval
          ++is_checkpoint: true
          ++model_path: outputs/so/${..__META__.ablation.name}/best_model
          tracking:
            log_model: False
            watch: null
      - name: FineTune
        add_name: False
        description: Finetune on the MBPP Data
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: false
          ++model_path: null
          tracking:
            log_model: False
            watch: null
      - name: Eval
        description: Evaluate the model
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: true
          ++model_path: outputs/MBPP/${..__META__.ablation.name}/best_model
          tracking:
            log_model: False
            watch: null
          ++prompts:
            file: templates/mbpp_prompts.yaml
            pipe:
              - basic_with_solution
    command:
      file: templates/commands/mbpp_eval.txt
      kwargs:
        train_sbatch: train_single_gpu
        finetune_time: "12:00:00"
        pretrain_time: "12:00:00"
        use_cds: True
      fields:
        - model_path

  MBPPNoReturnCarriage.WSolution:
    description: "Evaluate the baseline models with the updated MBPP loop"
    overrides:
      ++device: 0
      is_checkpoint: true
      objective: lm
      ++evaluation:
        num_generate_per_step: 200
        remove_input_ids: True
        seq_per_sample: 200
      ++task.params.remove_carriage_return: True
      tracking:
        tags:
          - Baseline

      ++prompts:
        file: templates/mbpp_prompts.yaml
        pipe:
          - basic_with_solution
    ablations:
      - Model:
          Neo125M:
            description: GPT Neo 125M
            overrides:
              ++model: 'EleutherAI/gpt-neo-125M'
              ++training.gradient_checkpointing: True
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
          ParrotSmall:
            description: CodeParrot Small
            overrides:
              ++model: 'lvwerra/codeparrot-small'
            step_overrides:
              PreTrain:
                ++training.batch_size: 32
                ++training.gradient_accumulation_steps: 1
                ++training.gradient_checkpointing: True
    steps:
      - name: FineTune
        add_name: False
        description: Finetune on the MBPP Data
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: false
          ++model_path: null
          tracking:
            log_model: False
            watch: null
      - name: Eval
        description: Evaluate the model
        base: mbpp_finetune
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: true
          ++model_path: outputs/MBPP/${..__META__.ablation.Model}/best_model
          tracking:
            log_model: False
            watch: null

          ++prompts:
            file: templates/mbpp_prompts.yaml
            pipe:
              - basic_with_solution
    command:
      file: templates/commands/mbpp_eval.txt
      kwargs:
        train_sbatch: train_single_gpu
        finetune_time: "12:00:00"
        pretrain_time: "12:00:00"
        use_cds: True
      fields:
        - model_path