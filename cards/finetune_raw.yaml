starting_commands: command_templates/sbatch_experiment.txt
experiments:
  Pretrain.ParrotSmall:
    overrides:
      ++device: 0
      +disable_cache: True
      ++tracking:
        log_model: false
      ++model: 'lvwerra/codeparrot-small'
      ++num_proc: 4
      ++training:
        batch_size: 32
        gradient_accumulation_steps: 1

    ablations:
      - DumpName:
          Python:
            ++dump_name: python
            ++processor.params.clean: false
          RandomClean:
            ++dump_name: random_clean
            ++processor.params.clean: True
          PythonClean:
            ++dump_name: python_clean
            ++processor.params.clean: True
          Random:
            ++dump_name: random
            ++processor.params.clean: False
      - CheckPoint:
          Chk10K:
            ++checkpoint: checkpoint-10000
          Chk2.5K:
            ++checkpoint: checkpoint-2500
          Chk5K:
            ++checkpoint: checkpoint-5000
          Chk7.5K:
            ++checkpoint: checkpoint-7500

    steps:
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          objective: 'lm'
          task: mbpp
          ++is_checkpoint: True
          ++model_path: pretrain_outputs/SO.Pretrain.ParrotSmall.${..__META__.ablation.ablation_values.DumpName}/checkpoints/${..__META__.ablation.overrides.++checkpoint}
          training:
            learning_rate: 5e-5
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 200
          remove_input_ids: true
          ++generation:
            max_new_tokens: 256
          ++device: 0
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.previous_step.save_name}
    command:
      file: command_templates/finetune.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
      fields:
        - model_path