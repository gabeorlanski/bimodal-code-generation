starting_commands: command_templates/sbatch_experiment.txt
experiments:
  CodeT5.SO:
      overrides:
        ++device: 0
        ++model: 'EleutherAI/gpt-neo-125M'
        +processor: stackoverflow
        ++processor.params.repeat_question_for_each_answer: 'full'
        ++raw_dump_path: data/dumps
        is_checkpoint: true
        ++task.dump_name: python_title
        ++raw_dump_name: python
        ++tensorized_name: python_title
        ++model: Salesforce/codet5-small
        objective: seq2seq
      steps:
        - name: PreTrain
          add_name: False
          base: pretrain_with_tensorized
          group: SO
          overrides:
            ++buffer_size: 4
            ++tensorized_path: data/tensorized
            ++task.data_path: data/tensorized
            ++processor.params.clean: True
            +tensorize_batch_size: 64
            tracking:
              log_model: True
            is_checkpoint: False
            ++training:
              batch_size: 10
              gradient_accumulation_steps: 8
              learning_rate: 1e-4
              save_steps: 2500
              eval_steps: 500
              max_steps: 10000
              warmup_steps: 500
              logging_steps: 25
              lr_scheduler_type: linear
              deepspeed: ds_config.json
              save_total_limit: 5
            ++task.sequence_length: 1024
            task: so
        - name: HumanEval
          add_name: False
          base: human_eval
          group: HUMAN_EVAL
          overrides:
            is_checkpoint: true
            ++generation:
              max_new_tokens: 256
              do_sample: true
              temperature: 0.5
              top_p: 0.95
              top_k: 50
            model_path: best_models/${..__META__.PreTrain.save_name}
            batch_size: 200
        - name: FineTune
          base: greene_config
          group: MBPP
          overrides:
            task: mbpp
            ++is_checkpoint: True
            ++model_path: best_models/${..__META__.PreTrain.save_name}
            ++training:
              batch_size: 32
              gradient_accumulation_steps: 1
              learning_rate: 5e-5
        - name: HEFineTune
          base: human_eval
          group: HUMAN_EVAL
          overrides:
            is_checkpoint: true
            ++generation:
              max_new_tokens: 256
              do_sample: true
              temperature: 0.5
              top_p: 0.95
              top_k: 50
            model_path: best_models/${..__META__.FineTune.save_name}
            batch_size: 200
        - name: Eval
          base: eval_config
          group: MBPP
          overrides:
            batch_size: 200
            remove_input_ids: true
            ++generation:
              max_new_tokens: 256
            task: mbpp
            remove_input_ids: false
            ++model_path: best_models/${..__META__.FineTune.save_name}
      command:
        file: command_templates/deepspeed_pretrain.txt
        kwargs:
          num_return_sequences: 200
          task_name: MBPP
          train_sbatch: train_single_gpu
        fields:
          - model_path