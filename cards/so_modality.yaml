starting_commands: command_templates/sbatch_experiment.txt
experiments:
  ParrotSmall.SO:
    overrides:
      ++device: 0
      ++model: 'lvwerra/codeparrot-small'
      +processor: stackoverflow
      ++task.raw_dump_path: data/dumps
      ++task.raw_dump_name: python
      objective: lm
      ++processor.params.remove_modality: NL
    ablations:
      - DumpName:
          PyOnlyCodeNoRepeat:
            ++task.tensorized_name: python_code_norepeat
            ++processor.params.repeat_question_for_each_answer: 'none'
          PyOnlyCodeFullRepeat:
            ++task.tensorized_name: python_code_fullrepeat
            ++processor.params.repeat_question_for_each_answer: 'full'
          PyOnlyCodeFullRepeatWTitle:
            ++task.tensorized_name: python_code_fullrepeat_title
            ++processor.params.repeat_question_for_each_answer: 'full'
            ++processor.params.force_include_title: True
          PyOnlyCodeFullRepeatWBody:
            ++task.tensorized_name: python_code_fullrepeat_body
            ++processor.params.repeat_question_for_each_answer: 'full'
            ++processor.params.force_include_question: True
          PyOnlyCodeFullRepeatWQuestion:
            ++task.tensorized_name: python_code_fullrepeat_question
            ++processor.params.repeat_question_for_each_answer: 'full'
            ++processor.params.force_include_question: True
            ++processor.params.force_include_title: True
    steps:
      - name: PreTrain
        add_name: False
        base: pretrain_with_tensorized
        group: SO
        overrides:
          ++task.buffer_size: 25
          ++processor.params.clean: True
          +tensorize_batch_size: 64
          tracking:
            log_model: True
          is_checkpoint: False
          ++training:
            batch_size: 16
            gradient_accumulation_steps: 4
            learning_rate: 1e-4
            save_steps: 1000
            eval_steps: 500
            max_steps: 10000
            warmup_steps: 500
            logging_steps: 25
            lr_scheduler_type: linear
            deepspeed: ds_config.json
            save_total_limit: 10
            group_by_length: False
            dataloader_num_workers: 4
          ++task.sequence_length: 1024
          task: so
      - name: HumanEval
        add_name: False
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_length: 256
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.PreTrain.save_name}
          batch_size: 200
      - name: FineTune
        base: greene_config
        group: MBPP
        overrides:
          task: mbpp
          ++is_checkpoint: True
          ++model_path: best_models/${..__META__.PreTrain.save_name}
          ++training:
            batch_size: 32
            gradient_accumulation_steps: 1
            learning_rate: 5e-5
      - name: HEFineTune
        base: human_eval
        group: HUMAN_EVAL
        overrides:
          is_checkpoint: true
          ++generation:
            max_length: 512
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          model_path: best_models/${..__META__.FineTune.save_name}
          batch_size: 100
      - name: Eval
        base: eval_config
        group: MBPP
        overrides:
          batch_size: 100
          remove_input_ids: true
          ++generation:
            max_length: 512
            do_sample: true
            temperature: 0.5
            top_p: 0.95
            top_k: 50
          task: mbpp
          remove_input_ids: true
          ++model_path: best_models/${..__META__.FineTune.save_name}
    command:
      file: command_templates/deepspeed_pretrain.txt
      kwargs:
        num_return_sequences: 200
        task_name: MBPP
        train_sbatch: train_single_gpu
      fields:
        - model_path